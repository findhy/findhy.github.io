
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Findhy's Blog</title>
  <meta name="author" content="Findhy">

  
  <meta name="description" content="首先解释什么是数据倾斜问题，举一个例子，图书馆有A0、A1、A2&hellip;A9共10个书架，A0书架有1000本书，而A1到A9书架有100本书，现在要统计图书的总数量，这时我们会发现A1到A9很快就统计完了，而A0书架需要统计很长时间，显然A0成为整个统计过程的瓶颈。 &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://findhy.com">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="Findhy's Blog" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="./javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href='http://fonts.googleapis.com/css?family=Noto+Serif:400,700' rel='stylesheet' type='text/css'>
<link href='http://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>

<script type="text/javascript">

function addBlankTargetForLinks () {

  $('a[href^="http"]').each(function(){

      $(this).attr('target', '_blank');

  });

}

$(document).bind('DOMNodeInserted', function(event) {

  addBlankTargetForLinks();

});

</script>

  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-52121968-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">Findhy's Blog</a></h1>
  
    <h2>Art is long, Life is short.</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  
  
</ul>

<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
  <li><a href="/about">About</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2015/01/16/hadoop-data-tilt/">Hadoop数据倾斜问题总结</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2015-01-16T09:41:26+08:00" pubdate data-updated="true">Jan 16<span>th</span>, 2015</time>
        
           | <a href="/blog/2015/01/16/hadoop-data-tilt/#disqus_thread"
             data-disqus-identifier="http://findhy.com/blog/2015/01/16/hadoop-data-tilt/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>首先解释什么是数据倾斜问题，举一个例子，图书馆有A0、A1、A2&hellip;A9共10个书架，A0书架有1000本书，而A1到A9书架有100本书，现在要统计图书的总数量，这时我们会发现A1到A9很快就统计完了，而A0书架需要统计很长时间，显然A0成为整个统计过程的瓶颈。<br/>
Hadoop最根本的思想就是分而治之，数据倾斜会导致分布式的计算效率依赖单个节点，这与它的初衷是背道而驰的，但是Hadoop又没有从根本层面解决这个问题，所以需要我们在业务设计和开发时来解决这个问题，最好的解决方法是设计时避免让这个问题出现。如果无法避免，那么解决数据倾斜问题和核心思想只有一个：让map输出的key均匀分布到各个reduce中去（mapjoin例外，直接在内存中解决，牺牲内存空间提高效率，只适用于较小的输入数据）</p>

<p>产生数据倾斜的原因有哪些呢？<br/>
1.数据本身的key分布不均匀</p>

<p>2.join操作过程产生过多空值或0</p>

<p>未完周末待续</p>

<p>参考：<a href="http://www.alidata.org/archives/2109">http://www.alidata.org/archives/2109</a></p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/12/06/hbase-index-mysql/">HBase通过Mysql建立二级索引</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-12-06T16:28:39+08:00" pubdate data-updated="true">Dec 6<span>th</span>, 2014</time>
        
           | <a href="/blog/2014/12/06/hbase-index-mysql/#disqus_thread"
             data-disqus-identifier="http://findhy.com/blog/2014/12/06/hbase-index-mysql/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>HBase查询数据有三种模式：Rowkey查询、Rowkey范围scan和全表扫描，HBase的Rowkey查询效率很高，毫秒级的，所以我们应该尽量对HBase使用Rowkey查询，而避免范围和全表扫描，思路有很多种，一种是Rowkey的巧妙设计，例如Rowkey是userId#timestamp，这样就可以支持用户和时间戳的查询了，这种方式最简单但也比较局限，例如无法支持较复杂的查询条件，还有Rowkey设计与region数据分布的问题，我们总是希望数据在多个region上均匀分布，又希望经常被一起查的数据在同一个region上，所以全靠Rowkey设计来解决还是比较局限的，本文介绍通过Mysql来建立HBase二级索引的方式，来实现HBase高效率的分页查询、复杂条件组合查询。</p>

<h2>思路</h2>

<ul>
<li>在Mysql中设计HBase的索引表，Rowkey作为主键，其它查询条件作为其它字段，由于数据量很大，Mysql索引表需要做分表</li>
<li>通过Python编码实现定时将HBase的Rowkey扫描到Mysql中，这里注意一个点，每次扫描完将最后的Rowkey记录在redis中，下次扫描从这个Rowkey开始</li>
<li>前端分页查询先查询索引表，这样就能够实现和Mysql一样的分页效果，总行数还有直接跳转到具体页，HBase是实现不了直接跳转到具体页的</li>
<li>从Mysql中根据查询条件查询到具体的Rowkey，再用这些Rowkey到HBase中批量查询，每一个都是Rowkey直接定位，批量查询效率也是毫秒左右返回，整体查询效率是在秒级响应的</li>
</ul>


</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2014/12/06/hbase-index-mysql/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/11/28/hbase-page/">HBase分页</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-11-28T09:45:23+08:00" pubdate data-updated="true">Nov 28<span>th</span>, 2014</time>
        
           | <a href="/blog/2014/11/28/hbase-page/#disqus_thread"
             data-disqus-identifier="http://findhy.com/blog/2014/11/28/hbase-page/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>HBase分页的难点在于两点：获取总行数困难、没有行标号。根据HBase的特点获取数据只有三种模式：RowKey唯一定位、Rowkey范围扫描和全表扫描。那么分页可以采用Rowkey的范围扫描，每次扫描一定范围内的数据并且通过PageFilter过滤来限定扫描的行数，如果我们每页10条，那么每次就扫描11条，记住第11条的Rowkey，当点击下一页的时候，将startRowkey设置为第11条的Rowkey，就实现了分页的效果，下面是相关的核心代码，具体项目可以参考<a href="https://github.com/findhy/hbase-page">github-HBase-page</a></p>

<pre><code>public class PageHBase {

    private Integer currentPageNo=1;//当前页码
    private Integer pageSize=5;//每页显示行数
    private Integer totalCount;//总行数
    private Integer totalPage;//总页数
    private Integer direction;//下一页：1 上一页2
    private Boolean hasNext=false;//是否有下一页
    private String nextPageRowkey;//下一页起始rowkey
    private List&lt;Map&lt;String, String&gt;&gt; resultList;//结果集List
    private Map&lt;String,String&gt; paramMap=new HashMap&lt;String,String&gt;();//分页查询参数
    private Map&lt;Integer,String&gt; pageStartRowMap=new HashMap&lt;Integer,String&gt;();//每页对应的startRow，key为currentPageNo，value为Rowkey
    private Scan scan=new Scan();

    public Scan getScan(String startRowkey,String endRowkey){
        scan.setCaching(100);
        if(direction==1&amp;&amp;hasNext){
            scan.setStartRow(Bytes.toBytes(startRowkey));
            scan.setStopRow(Bytes.toBytes(endRowkey));
        }else{
            if(pageStartRowMap.get(currentPageNo)!=null){
                scan.setStartRow(Bytes.toBytes(pageStartRowMap.get(currentPageNo)));
                scan.setStopRow(Bytes.toBytes(endRowkey));
            }else{
                scan.setStartRow(Bytes.toBytes(startRowkey));
                scan.setStopRow(Bytes.toBytes(endRowkey));
            }
        }
        this.hasNext=false;
        this.nextPageRowkey=null;
        return scan;
    }
}


public class AdunionHbaseService {

    private static final Logger log = LoggerFactory
            .getLogger(AdunionHbaseService.class);
    public static final String CLICK_COLUMN_NAME="c";
    public static final String CLICK_COLUMN_KEY="v";
    public static final String POSTBACK_COLUMN_NAME="p";
    public static final String POSTBACK_COLUMN_KEY="v";
    public static final String ADUNION_TABLE_NAME="adunion_active";
    private Configuration configuration;
    private String clientPort;
    private String retriesNumber;
    private String zookeeperQuorum;

    public AdunionHbaseService(String zookeeeper,String port,String retries){
        try {
            configuration = HBaseConfiguration.create();
            this.setClientPort(port);
            this.setRetriesNumber(retries);
            this.setZookeeperQuorum(zookeeeper);
            configuration.set("hbase.zookeeper.property.clientPort", this.getClientPort());
            configuration.set("hbase.client.retries.number", this.getRetriesNumber());
            configuration.set("hbase.zookeeper.quorum",this.getZookeeperQuorum());
        } catch (Exception e) {
            log.error(e.getMessage());
        }
    }

    public PageHBase getPageHBaseData(String tableName, String columnName,String columnKey,String businessId,String publisherId,String offerId,
            Date startDate,Date endDate, PageHBase pager) {
        HConnection con = null;
        HTable table = null;
        ResultScanner rs=null;
        List&lt;Map&lt;String,String&gt;&gt; resultList=new ArrayList&lt;Map&lt;String,String&gt;&gt;();
        try {
            if(startDate==null||endDate==null) return pager;

            con = HConnectionManager.createConnection(configuration);
            table=(HTable)con.getTable(tableName);

            StringBuffer startRow=new StringBuffer();
            StringBuffer endRow=new StringBuffer();
            startRow.append(startDate.getTime());
            endRow.append(endDate.getTime());
            if(pager.getNextPageRowkey()==null) pager.setNextPageRowkey(startRow.toString());
            Scan scan=pager.getScan(pager.getNextPageRowkey(),endRow.toString());

            List&lt;Filter&gt; filters=new ArrayList&lt;Filter&gt;();

            if(StringUtils.isNotBlank(publisherId)){
                SingleColumnValueFilter filter=new SingleColumnValueFilter(
                        Bytes.toBytes(columnName),
                        Bytes.toBytes("p"),
                        CompareFilter.CompareOp.EQUAL,
                        Bytes.toBytes(publisherId));
                filters.add(filter);
            }
            if(StringUtils.isNotBlank(offerId)){
                SingleColumnValueFilter filter=new SingleColumnValueFilter(
                        Bytes.toBytes(columnName),
                        Bytes.toBytes("o"),
                        CompareFilter.CompareOp.EQUAL,
                        Bytes.toBytes(offerId));
                filters.add(filter);
            }
            if(StringUtils.isNotBlank(businessId)){
                SingleColumnValueFilter filter=new SingleColumnValueFilter(
                        Bytes.toBytes(columnName),
                        Bytes.toBytes("b"),
                        CompareFilter.CompareOp.EQUAL,
                        Bytes.toBytes(businessId));
                filters.add(filter);
            }
            Filter pageFilter=new PageFilter(pager.getPageSize()+1);
            Filter familyFilter=new FamilyFilter(CompareFilter.CompareOp.EQUAL,
                    new BinaryComparator(Bytes.toBytes(columnName)));
            Filter qualifierFilter=new QualifierFilter(CompareFilter.CompareOp.EQUAL,
                    new BinaryComparator(Bytes.toBytes(columnKey)));
            filters.add(familyFilter);
            filters.add(qualifierFilter);
            filters.add(pageFilter);
            FilterList filterList=new FilterList(filters);
            scan.setFilter(filterList);
            rs = table.getScanner(scan);
            int totalRow=0;
            if(rs!=null){
                for(Result result : rs){
                    totalRow++;
                    if(totalRow==1){
                        pager.getPageStartRowMap().put(pager.getCurrentPageNo(),Bytes.toString(result.getRow()));
    pager.setTotalPage(pager.getPageStartRowMap().size());
    }
                    if(totalRow&gt;pager.getPageSize()){
                        pager.setNextPageRowkey(new String(result.getRow()));
                        pager.setHasNext(true);
                    }else{
                        Map&lt;String,String&gt; map = new HashMap&lt;String,String&gt;();
                        for(KeyValue keyValue:result.raw()){
                            String family=new String(keyValue.getFamily());
                            byte[] value=keyValue.getValue();
                            //map.put("rowkey", new String(result.getRow()));
                            Map&lt;Object,Object&gt; jsonMap=JsonUtil.getMapFromJsonObjStr(new String(value));
                            for(Object obj:jsonMap.keySet()){
                                map.put(obj.toString(), jsonMap.get(obj).toString());
                            }
                        }
                        resultList.add(map);
                    }
                }
            }
            pager.setResultList(resultList);
        }catch(IOException ioe){
            log.error(ioe.getMessage());
        } catch (Exception e) {
            log.error(e.getMessage());
        }finally{
            try {
                rs.close();
            } catch (Exception e) {
                log.error(e.getMessage());
            }
            try {
                table.close();
            } catch (Exception e) {
                log.error(e.getMessage());
            }
            try {
                con.close();
            } catch (Exception e) {
                log.error(e.getMessage());
            }
        }
        return pager;
    }
}
</code></pre>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/11/13/namenode-low-on-available-disk-space/">Hadoop集群NameNode运行中进入安全模式的问题：NameNode Low on Available Disk Space</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-11-13T15:23:26+08:00" pubdate data-updated="true">Nov 13<span>th</span>, 2014</time>
        
           | <a href="/blog/2014/11/13/namenode-low-on-available-disk-space/#disqus_thread"
             data-disqus-identifier="http://findhy.com/blog/2014/11/13/namenode-low-on-available-disk-space/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>问题描述：Hadoop集群在运行过程，早上来的时候发现NameNode自动进入安全模式，导致系统运行的MR任务失败。</p>

<p>最开始怀疑是HDFS的块复制没有达到最低的要求，通过hadoop fsck -blocks查看，Minimally replicated blocks为100%，说明blocks都达到了复制要求，最后查看了NameNode的日志发现几行警告信息：</p>

<pre><code>2014-11-12 21:31:38,478 WARN org.apache.hadoop.hdfs.server.namenode.NameNodeResourceChecker (org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeResourceMonitor@1f1dd4b6): Space available o
n volume 'null' is 13119488, which is below the configured reserved amount 104857600
2014-11-12 21:31:38,478 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem (org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeResourceMonitor@1f1dd4b6): NameNode low on available di
sk space. Entering safe mode.
2014-11-12 21:31:38,478 INFO org.apache.hadoop.hdfs.StateChange (org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeResourceMonitor@1f1dd4b6): STATE* Safe mode is ON. 
Resources are low on NN. Please add or free up more resources then turn off safe mode manually. NOTE:  If you turn off safe mode before adding resources, the NN will immediately return to safe mode
. Use "hdfs dfsadmin -safemode leave" to turn safe mode off.
</code></pre>

<p>通过日志分析发现这样的错误：<code>NameNode low on available disk space. Entering safe mode</code></p>

<p>在网上查到hadoop-jira上面有这样的记录：<a href="https://issues.apache.org/jira/browse/HDFS-4425">https://issues.apache.org/jira/browse/HDFS-4425</a></p>

<p>另外这里还有详细的解释：<a href="https://support.pivotal.io/hc/en-us/articles/201455807-Namenode-logs-reports-Space-available-on-volume-null-is-below-threshold-and-enters-safe-mode">https://support.pivotal.io/hc/en-us/articles/201455807-Namenode-logs-reports-Space-available-on-volume-null-is-below-threshold-and-enters-safe-mode</a></p>

<p>原因是Hadoop的源码里面有一个类 <code>NameNodeResourceChecker</code> 负责检查NameNode的磁盘空间，如果磁盘空间低于100M则进入到安全模式，导致系统不可用，<strong>那么问题来了</strong>，是什么原因导致磁盘空间暴涨了呢？最后发现是HBase的DEBUG日志没有关闭导致日志文件非常大，最终磁盘空间占满，NameNode不可用。</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/07/29/hadoop-streaming-python/">Hadoop Streaming编程(Python)</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-07-29T17:17:59+08:00" pubdate data-updated="true">Jul 29<span>th</span>, 2014</time>
        
           | <a href="/blog/2014/07/29/hadoop-streaming-python/#disqus_thread"
             data-disqus-identifier="http://findhy.com/blog/2014/07/29/hadoop-streaming-python/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>Hadoop Streaming是Hadoop提供的一个工具，可以允许开发者以非Java的编程语言开发Mapper和Reducer，而且在开发过程中，我们只需要专注于数据的输入和输出处理，其它的事情都交给这个工具来做。本例是用Python实现一个简单的Wordcount例子，环境为：</p>

<ul>
<li>Hadoop:CDH4.3.2</li>
<li>Python:2.6.6</li>
<li>OS:Centos 6.5</li>
<li>Hadoop Streaming:hadoop-2.0.0-cdh4.3.2/share/hadoop/tools/lib/hadoop-streaming-2.0.0-cdh4.3.2.jar</li>
</ul>


</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2014/07/29/hadoop-streaming-python/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/07/25/hadoop-cloud/">基于云的Hadoop架构</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-07-25T12:18:55+08:00" pubdate data-updated="true">Jul 25<span>th</span>, 2014</time>
        
           | <a href="/blog/2014/07/25/hadoop-cloud/#disqus_thread"
             data-disqus-identifier="http://findhy.com/blog/2014/07/25/hadoop-cloud/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>今天看了Netflix分享的基于AWS的大数据平台Hadoop架构，感觉这是未来大数据平台的发展模式，将整个数据仓库部署在云端，开发者不需要考虑集群的资源扩展，也不需要接触复杂的底层命令，通过一个简单RESTFul接口就可以提交我们的MapReduce任务，还可以实时看到任务和运行状态。</p>

</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2014/07/25/hadoop-cloud/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/07/24/s3cmd-multiple-accounts/">S3cmd 多账户配置</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-07-24T15:16:17+08:00" pubdate data-updated="true">Jul 24<span>th</span>, 2014</time>
        
           | <a href="/blog/2014/07/24/s3cmd-multiple-accounts/#disqus_thread"
             data-disqus-identifier="http://findhy.com/blog/2014/07/24/s3cmd-multiple-accounts/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>关于s3cmd的安装配置参考这篇文章：<a href="http://findhy.com/blog/2014/06/05/s3cmd-config/">s3cmd Configure</a>，本文介绍s3cmd多账户配置。</p>

</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2014/07/24/s3cmd-multiple-accounts/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/07/22/introduction-to-item-based-recommendation-with-hadoop/">Introduction to Item-Based Recommendations With Hadoop</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-07-22T14:30:07+08:00" pubdate data-updated="true">Jul 22<span>nd</span>, 2014</time>
        
           | <a href="/blog/2014/07/22/introduction-to-item-based-recommendation-with-hadoop/#disqus_thread"
             data-disqus-identifier="http://findhy.com/blog/2014/07/22/introduction-to-item-based-recommendation-with-hadoop/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>本文是Mahout官网上的Item-Based CF on Hadoop的例子，原文在这里<a href="https://mahout.apache.org/users/recommender/intro-itembased-hadoop.html">Introduction to Item-Based Recommendations with Hadoop</a>。</p>

</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2014/07/22/introduction-to-item-based-recommendation-with-hadoop/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/07/21/mahout-tutorial/">Mahout Tutorial</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-07-21T17:47:04+08:00" pubdate data-updated="true">Jul 21<span>st</span>, 2014</time>
        
           | <a href="/blog/2014/07/21/mahout-tutorial/#disqus_thread"
             data-disqus-identifier="http://findhy.com/blog/2014/07/21/mahout-tutorial/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p><a href="http://mahout.apache.org/">Mahout</a> 是一个开源的Java实现的可伸缩的机器学习库，它实现了常见的聚类和推荐等机器学习算法，并且可运行在 Hadoop 分布式平台处理上PB的数据，Mahout 社区和文档非常丰富，是目前搭建推荐系统或者其它算法平台的首选方案。</p>

</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2014/07/21/mahout-tutorial/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/07/18/app-store-business-and-recommend/">手机应用商店的商业形态以及个性化推荐</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-07-18T11:53:49+08:00" pubdate data-updated="true">Jul 18<span>th</span>, 2014</time>
        
           | <a href="/blog/2014/07/18/app-store-business-and-recommend/#disqus_thread"
             data-disqus-identifier="http://findhy.com/blog/2014/07/18/app-store-business-and-recommend/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>最近同时在参与两件事：手机应用商店的商业化之路以及应用的个性化推荐，一点想法记录下来，手机应用商店市场已经泛滥，既有官方出品（Google Play），又有巨头入场（应用宝、360、91），还有少年得志生机勃勃的豌豆荚，作为应用商店怎么在竞争如此激烈的市场中抢占一席之地呢？</p>

</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2014/07/18/app-store-business-and-recommend/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  <div class="pagination">
    
      <a class="prev" href="/blog/page/2/">&larr; Older</a>
    
    <a href="/blog/archives">Blog Archives</a>
    
  </div>
</div>
<aside class="sidebar">
   
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:findhy.com" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
  
    <section>
  <h1>Categories</h1>
  <ul id="categories">
    <li class='category'><a href='/blog/categories/ad/'>AD (1)</a></li>
<li class='category'><a href='/blog/categories/aws/'>AWS (3)</a></li>
<li class='category'><a href='/blog/categories/bigdata/'>Bigdata (1)</a></li>
<li class='category'><a href='/blog/categories/hbase/'>HBase (2)</a></li>
<li class='category'><a href='/blog/categories/hadoop/'>Hadoop (5)</a></li>
<li class='category'><a href='/blog/categories/io/'>IO (2)</a></li>
<li class='category'><a href='/blog/categories/ios/'>IOS (1)</a></li>
<li class='category'><a href='/blog/categories/kafka/'>Kafka (3)</a></li>
<li class='category'><a href='/blog/categories/mahout/'>Mahout (2)</a></li>
<li class='category'><a href='/blog/categories/nginx/'>Nginx (1)</a></li>
<li class='category'><a href='/blog/categories/opendata/'>Opendata (1)</a></li>
<li class='category'><a href='/blog/categories/others/'>Others (4)</a></li>
<li class='category'><a href='/blog/categories/rpc/'>RPC (1)</a></li>
<li class='category'><a href='/blog/categories/reading/'>Reading (1)</a></li>
<li class='category'><a href='/blog/categories/spark/'>Spark (1)</a></li>
<li class='category'><a href='/blog/categories/storm/'>Storm (4)</a></li>
<li class='category'><a href='/blog/categories/titan/'>Titan (8)</a></li>
<li class='category'><a href='/blog/categories/websocket/'>WebSocket (1)</a></li>
<li class='category'><a href='/blog/categories/yarn/'>YARN (1)</a></li>
<li class='category'><a href='/blog/categories/zookeeper/'>Zookeeper (1)</a></li>

  </ul>
</section><section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2015/01/16/hadoop-data-tilt/">Hadoop数据倾斜问题总结</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/12/06/hbase-index-mysql/">HBase通过Mysql建立二级索引</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/11/28/hbase-page/">HBase分页</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/11/13/namenode-low-on-available-disk-space/">Hadoop集群NameNode运行中进入安全模式的问题：NameNode Low on Available Disk Space</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/07/29/hadoop-streaming-python/">Hadoop Streaming编程(Python)</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/07/25/hadoop-cloud/">基于云的Hadoop架构</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/07/24/s3cmd-multiple-accounts/">S3cmd 多账户配置</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/07/22/introduction-to-item-based-recommendation-with-hadoop/">Introduction to Item-Based Recommendations With Hadoop</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/07/21/mahout-tutorial/">Mahout Tutorial</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/07/18/app-store-business-and-recommend/">手机应用商店的商业形态以及个性化推荐</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/07/09/zookeeper-manager/">Zookeeper运维管理工具介绍</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/06/28/spark-on-yarn/">Spark1.0在YARN上部署过程</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/06/23/titan-visualization-with-vivagraphjs/">Titan Visualization With VivaGraphJS</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/06/23/storage-performance-locate/">存储性能瓶颈的成因、定位与排查</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/06/21/graph-database-visualization/">图数据库(graph Database)可视化</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/06/19/titan-tutorial/">Titan Tutorial</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/06/19/titan-classic-get-started/">Titan经典入门PPT</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/06/19/how-to-put-slideshare-on-octopress-blog/">怎么在Octopress Blog中嵌入slideshare</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/06/18/tinkerpop/">TinkerPop</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/06/17/graph-database-data-structure/">Graph Database</a>
      </li>
    
  </ul>
</section>

<section>
  <h1>GitHub Repos</h1>
  <ul id="gh_repos">
    <li class="loading">Status updating&#8230;</li>
  </ul>
  
  <a href="https://github.com/findhy">@findhy</a> on GitHub
  
  <script type="text/javascript">
    $(document).ready(function(){
        if (!window.jXHR){
            var jxhr = document.createElement('script');
            jxhr.type = 'text/javascript';
            jxhr.src = '/javascripts/libs/jXHR.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(jxhr, s);
        }

        github.showRepos({
            user: 'findhy',
            count: 5,
            skip_forks: true,
            target: '#gh_repos'
        });
    });
  </script>
  <script src="/javascripts/github.js" type="text/javascript"> </script>
</section>





  
</aside>


    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2015 - Findhy -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'findhy';
      
        
        var disqus_script = 'count.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = '//platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>



<script>
  $(document).ready(function() {  
  var stickyNavTop = $('nav').offset().top;  
    
  var stickyNav = function(){  
  var scrollTop = $(window).scrollTop();  
         
  if (scrollTop > stickyNavTop) {   
      $('nav').addClass('sticky');  
  } else {  
      $('nav').removeClass('sticky');   
  }  
  };  
    
  stickyNav();  
    
  $(window).scroll(function() {  
      stickyNav();  
  });  
  });  
</script>


</body>
</html>
