<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Hadoop | Findhy's Blog]]></title>
  <link href="http://findhy.com/blog/categories/hadoop/atom.xml" rel="self"/>
  <link href="http://findhy.com/"/>
  <updated>2014-06-17T13:06:42+08:00</updated>
  <id>http://findhy.com/</id>
  <author>
    <name><![CDATA[Findhy]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Hadoop 2.2.0-cdh5.0.0-beta-2 Install on Centos]]></title>
    <link href="http://findhy.com/blog/2014/05/14/cdh5-hadoop-2-2-0-install/"/>
    <updated>2014-05-14T23:08:37+08:00</updated>
    <id>http://findhy.com/blog/2014/05/14/cdh5-hadoop-2-2-0-install</id>
    <content type="html"><![CDATA[<p>环境说明</p>

<pre><code>master 10.0.1.252  
slave1 10.0.1.252  
slave2 10.0.1.252  
</code></pre>

<p>软件版本</p>

<pre><code>Hadoop 2.2.0-cdh5.0.0-beta-2  
JDK 1.7.0_45
</code></pre>

<!--more-->


<h2>开始安装</h2>

<hr />

<h3>1.创建用户</h3>

<pre><code>useradd hadoop
</code></pre>

<h3>2.修改密码</h3>

<pre><code>passwd hadoop
</code></pre>

<h3>3.修改HOSTS文件</h3>

<pre><code>10.0.1.252 master  
10.0.1.253 slave1  
10.0.1.254 slave2  
</code></pre>

<h3>4.节点互信配置</h3>

<p>在每个节点执行</p>

<pre><code>ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa
cat ~/.ssh/id_dsa.pub &gt;~/.ssh/authorized_keys
</code></pre>

<p>更改权限</p>

<pre><code>[hadoop@master .ssh]$ chmod 600 authorized_keys
[hadoop@master ~]$ chmod 700 .ssh/
</code></pre>

<p>在master节点操作<br/>
拷贝其它节点的公钥到authorized_keys文件中</p>

<pre><code>[hadoop@master .ssh]$ ssh hadoop@slave1 cat ~/.ssh/authorized_keys &gt;&gt; authorized_keys
[hadoop@master .ssh]$ ssh hadoop@slave2 cat ~/.ssh/authorized_keys &gt;&gt; authorized_keys
</code></pre>

<p>然后将公钥拷贝到其它节点</p>

<pre><code>[hadoop@master .ssh]$ scp authorized_keys hadoop@slave1:~/.ssh/
[hadoop@master .ssh]$ scp authorized_keys hadoop@slave2:~/.ssh/
</code></pre>

<p>测试一下，不需要密码就可以访问</p>

<pre><code>ssh master  
Ssh slave1  
Ssh slave2  
</code></pre>

<p>首次会让输入yes，后面就可以直接登录了</p>

<h3>5.JDK安装</h3>

<p>用java –version检查是否安装了JDK，如果没有安装，则参照下面的连接安装：
<a href="https://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH5/latest/CDH5-Installation-Guide/cdh5ig_oracle_jdk_installation.html#topic_29_1">https://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH5/latest/CDH5-Installation-Guide/cdh5ig_oracle_jdk_installation.html#topic_29_1</a></p>

<h3>6.载CDH安装包</h3>

<p>下载解压：<a href="http://archive-primary.cloudera.com/cdh5/cdh/5/hadoop-2.2.0-cdh5.0.0-beta-2.tar.gz">http://archive-primary.cloudera.com/cdh5/cdh/5/hadoop-2.2.0-cdh5.0.0-beta-2.tar.gz</a></p>

<pre><code>tar –zxvf hadoop-2.2.0-cdh5.0.0-beta-2.tar.gz
</code></pre>

<h3>7.修改hadoop-env.sh</h3>

<p>修改${HADOOP_HOME}/etc/hadoop/hadoop-env.sh</p>

<pre><code>export JAVA_HOME=/usr/java/jdk1.7.0_45
</code></pre>

<h3>8.修改mapred-site.xml</h3>

<p>在${HADOOP_HOME}/etc/hadoop/目录中，将mapred-site.xml.templat重命名成mapred-site.xml：</p>

<pre><code>mv mapred-site.xml.template mapred-site.xml
</code></pre>

<p>并添加以下内容：</p>

<pre><code>&lt;property&gt;
&lt;name&gt;mapreduce.framework.name&lt;/name&gt;
&lt;value&gt;yarn&lt;/value&gt;
&lt;/property&gt; 
</code></pre>

<h3>9.修改core-site.xml</h3>

<p>修改${HADOOP_HOME}/etc/hadoop/core-site.xml</p>

<pre><code>&lt;property&gt;
&lt;name&gt;fs.default.name&lt;/name&gt;
&lt;value&gt;hdfs://master:8020&lt;/value&gt;
&lt;final&gt;true&lt;/final&gt;
&lt;/property&gt;
</code></pre>

<h3>10.修改yarn-site.xml</h3>

<p>修改${HADOOP_HOME}/etc/hadoop/yarn-site.xml：</p>

<pre><code>&lt;property&gt;
&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
&lt;value&gt;mapreduce_shuffle&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt;
&lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;
&lt;/property&gt;
</code></pre>

<h3>11.修改hdfs-site.xml</h3>

<p>修改${HADOOP_HOME}/etc/hadoop/hdfs-site.xml</p>

<pre><code>&lt;property&gt;
&lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;
&lt;value&gt;/home/hadoop/dfs/yarn/name&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
&lt;value&gt;/home/hadoop/dfs/yarn/data&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;dfs.replication&lt;/name&gt;
&lt;value&gt;1&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;dfs.permissions&lt;/name&gt;
&lt;value&gt;false&lt;/value&gt;
&lt;/property&gt;
</code></pre>

<h3>12.修改slaves文件</h3>

<p>在slaves文件中添加你的slave节点：</p>

<pre><code>slave1
slave2
</code></pre>

<h3>13.修改masters文件</h3>

<p>在masters文件中添加你的master节点：</p>

<pre><code>master
</code></pre>

<h3>14.将安装包复制到其它节点</h3>

<pre><code>scp -r /home/hadoop/hadoop-2.2.0-cdh5.0.0-beta-2 hadoop@slave1:/home/hadoop/hadoop-2.2.0-cdh5.0.0-beta-2
scp -r /home/hadoop/hadoop-2.2.0-cdh5.0.0-beta-2 hadoop@slave2:/home/hadoop/hadoop-2.2.0-cdh5.0.0-beta-2
</code></pre>

<p><em>上面配置文件对应的目录在每个节点都需要提前创建好</em></p>

<h3>15.初始化HDFS</h3>

<pre><code>./bin/hadoop namenode –format
</code></pre>

<h3>16.启动HDFS</h3>

<pre><code>./sbin/start-dfs.sh
</code></pre>

<h3>17.启动YARN</h3>

<pre><code>./sbin/start-yarn.sh
</code></pre>

<h3>18.测试集群</h3>

<p>自己创建一个文件put到HDFS的/test/in目录下面
执行：</p>

<pre><code>./bin/hadoop jar share/hadoop/mapreduce2/hadoop-mapreduce-examples-2.2.0-cdh5.0.0-beta-2.jar wordcount /test/in/ /test/out/wordcountr
</code></pre>

<h3>19.YARN管理地址</h3>

<p><a href="http://master:8089/cluster/cluster">http://master:8089/cluster/cluster</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Oracle DBA如何转型到Hadoop平台]]></title>
    <link href="http://findhy.com/blog/2014/03/20/the-hadoop-faq-for-oracle-dbas/"/>
    <updated>2014-03-20T15:02:44+08:00</updated>
    <id>http://findhy.com/blog/2014/03/20/the-hadoop-faq-for-oracle-dbas</id>
    <content type="html"><![CDATA[<p>Oracle DBA如何转型到Hadoop平台，这篇博客是Cloudera的工程师Gwen Shapira的回答，因为他以前就是从事Oracle DBA的。原文在<a href="http://blog.cloudera.com/blog/2014/01/the-hadoop-faq-for-oracle-dbas/">这里</a>。对于国内的一些DBA想转型Hadoop的可以参考一下。</p>

<!--more-->


<p></p>

<blockquote><p>Q：<strong>如果转型或者学习Hadoop平台的技术，Oracle DBA的经验还有用吗？</strong></p></blockquote>

<p>A：在我合作的很多客户当中，我们和DBA团队一起合作将他们的数据仓库从Teradata或者Netezza迁移到Hadoop平台中，他们和我一起写Sqoop脚本、Oozie流程、Hive ETL脚本和Impala报表，几个月之后，我走了，原来的DBA团队已经掌握了Hadoop的技能了。<br/>
我现在在Cloudera的解决方案架构师团队也会录用前DBA人员来作为解决方案顾问或系统工程师，我们认为他们的DBA经验非常有价值。</p>

<blockquote><p>Q：<strong>公司会录用一个没有Hadoop工作经验的DBA吗，如果会，看重的是什么呢？</strong></p></blockquote>

<p>A：我坚信DBAs完全有能力成为优秀的Hadoop专家 &ndash; 但这不代表所有的DBAs。下面是一些我觉得必备的技能：</p>

<ul>
<li><strong>适应命令行操作</strong>：只会鼠标操作点击的DBAs和ETL工程师不合适</li>
<li><strong>Linux经验</strong>：Hadoop是运行在Linux上的，所以你需要非常适应Linux的文件系统、工具还有命令行，你还要理解CPU调度和IO等相关概念。</li>
<li><strong>网络方面的知识</strong>：<a href="http://www.technology-training.co.uk/understandingtheiso7layermodel_10.php">ISO7层网络架构</a>，ssh原理，主机名称解析（/etc/hosts）,还有交换机。</li>
<li><strong>良好SQL能力</strong>：你需要熟练使用SQL语句，有数据仓库分区和并行处理经验、ETL经验和性能优化经验的优先。</li>
<li><strong>编程能力</strong>：不一定是Java，但是你要会写批处理脚本，不管是用Perl、Python还是其他语言，要会用伪代码来解决一些简单的问题，如果你不会编程，那就有问题了。</li>
<li><strong>解决问题能力</strong>：Hadoop没有Oracle那么成熟，所以你必须要学会用Google或者其他方式来搜索和解决问题，这个很重要。</li>
<li><strong>更高级的职位，我还关注系统和架构方面的能力</strong>：比如你做过飞行调度系统或者其他相似的东西。</li>
<li><strong>因为我们需要面对客户，所以沟通能力也很重要</strong>：如何倾听？如何向客户解释一个负责的技术问题？当我质疑你的时候该如何反应？</li>
</ul>


<p>或许这些太多了，但是我觉得这些你转型Hadoop平台必备的技能。</p>

<blockquote><p>Q：<strong>怎么开始学习Hadoop？</strong></p></blockquote>

<p>A：首先你需要在自己的电脑上搭建一个Hadoop集群环境，这个网上的资料很多。然后你可以导入一些数据到Hadoop中来分析，这里有一个例子是用Flume导入Twitter数据然后用Hive来分析：</p>

<ul>
<li><a href="http://blog.cloudera.com/blog/2012/09/analyzing-twitter-data-with-hadoop/">http://blog.cloudera.com/blog/2012/09/analyzing-twitter-data-with-hadoop/</a></li>
<li><a href="http://blog.cloudera.com/blog/2012/10/analyzing-twitter-data-with-hadoop-part-2-gathering-data-with-flume/">http://blog.cloudera.com/blog/2012/10/analyzing-twitter-data-with-hadoop-part-2-gathering-data-with-flume/</a></li>
</ul>


<p>还有就是买一些Hadoop方面的书籍，比如「Hadoop权威指南」、「Hadoop实战」等等。
推荐看下<a href="http://dongxicheng.org/mapreduce-nextgen/hadoope-every-day/">这里</a>，是一些Hadoop方面的学习资源。</p>

<blockquote><p>Q：<strong>我需要学习Java吗？</strong></p></blockquote>

<p>A：你不需要成为一个特别专业的Java开发人员，但是你需要能够看懂Java源代码，因为Hadoop就是Java写的，还有Hive UDFS也需要Java来编写，相信我，不是很难，比SQL简单多了。</p>
]]></content>
  </entry>
  
</feed>
