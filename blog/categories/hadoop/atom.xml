<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Hadoop | Findhy's Blog]]></title>
  <link href="http://findhy.github.io/blog/categories/hadoop/atom.xml" rel="self"/>
  <link href="http://findhy.github.io/"/>
  <updated>2017-07-21T15:42:48+08:00</updated>
  <id>http://findhy.github.io/</id>
  <author>
    <name><![CDATA[Findhy]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Hadoop数据倾斜问题总结]]></title>
    <link href="http://findhy.github.io/blog/2015/01/16/hadoop-data-tilt/"/>
    <updated>2015-01-16T09:41:26+08:00</updated>
    <id>http://findhy.github.io/blog/2015/01/16/hadoop-data-tilt</id>
    <content type="html"><![CDATA[<p>首先解释什么是数据倾斜问题，举一个例子，图书馆有A0、A1、A2&hellip;A9共10个书架，A0书架有1000本书，而A1到A9书架有100本书，现在要统计图书的总数量，这时我们会发现A1到A9很快就统计完了，而A0书架需要统计很长时间，显然A0成为整个统计过程的瓶颈。</p>

<p>Hadoop最根本的思想就是分而治之，数据倾斜会导致分布式的计算效率依赖单个节点，这与它的初衷是背道而驰的，但是Hadoop又没有从根本层面解决这个问题，所以需要我们在业务设计和开发时来解决这个问题，最好的解决方法是设计时避免让这个问题出现。如果无法避免，那么解决数据倾斜问题和核心思想只有一个：让map输出的key均匀分布到各个reduce中去（mapjoin例外，直接在内存中解决，牺牲内存空间提高效率，只适用于较小的输入数据）</p>

<p>产生数据倾斜的具体原因以及解决方案可以参考阿里的这篇博客：<a href="http://www.alidata.org/archives/2109">http://www.alidata.org/archives/2109</a></p>

<p>本文主要想总结一下各种处理方案的思路：</p>

<!--more-->


<h3>1、Distributed Cache</h3>

<p>Distributed Cache是Hadoop提供的文件缓存工具，会自动将需要缓存的文件分发到各个DataNode上去，对于key分布不均匀的数据并且数据量不大，可以使用Distributed Cache将文件缓存到各个节点上，这样在做map的时候直接从内存中读取数据不需要到reduce阶段处理从而提高性能。</p>

<p>创建job时可以添加需要cache的文件</p>

<pre><code>Job job = new Job();
...
job.addCacheFile(new Path(cacheFilename).toUri());
</code></pre>

<p>在mapper端使用</p>

<pre><code>Path[] localPaths = context.getLocalCacheFiles();
...
</code></pre>

<h3>2、Map Join</h3>

<p>如果是Hive，就是用Map Join，底层用的就是Distributed Cache，</p>

<pre><code>SELECT /*+ MAPJOIN(b) */ a.key, a.value
FROM a JOIN b ON a.key = b.key
</code></pre>

<p>在hive0.7之后，可以不用显示的指定MAPJOIN关键词，hive会根据参数配置来决定是否需要用到mapjoin</p>

<pre><code>hive.auto.convert.join=true//默认打开mapjoin优化
hive.mapjoin.smalltable.filesize=25000000//文件达到多大时开启mapjoin优化，阀值控制
</code></pre>

<h3>3、转换map输出的key值</h3>

<p>如果我们无法在设计时避免，也无法在map时处理，那只能在最后一刻来弥补，比如map输出的key大量为空，那么可以用一个随机数来代替空值，这样数据就是均摊到多个reduce上了</p>

<h3>4、步骤拆解</h3>

<p>将一步操作拆解为多步，将倾斜的数据单独拿出来处理，最后再与正常数据的结果合并就可以了，具体实现可以看这篇博客：<a href="http://www.alidata.org/archives/2109">http://www.alidata.org/archives/2109</a>  最后的总结</p>

<h3>5、参数调整</h3>

<pre><code>hive.map.aggr = true//map端部分聚合，相当于Combiner
hive.groupby.skewindata=true//有数据倾斜的时候自动进行负载均衡
</code></pre>

<p>如果大家以后遇到数据倾斜的问题，可以尝试从这几个方面来想办法优化。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop集群NameNode运行中进入安全模式的问题：NameNode Low on Available Disk Space]]></title>
    <link href="http://findhy.github.io/blog/2014/11/13/namenode-low-on-available-disk-space/"/>
    <updated>2014-11-13T15:23:26+08:00</updated>
    <id>http://findhy.github.io/blog/2014/11/13/namenode-low-on-available-disk-space</id>
    <content type="html"><![CDATA[<p>问题描述：Hadoop集群在运行过程，早上来的时候发现NameNode自动进入安全模式，导致系统运行的MR任务失败。</p>

<p>最开始怀疑是HDFS的块复制没有达到最低的要求，通过hadoop fsck -blocks查看，Minimally replicated blocks为100%，说明blocks都达到了复制要求，最后查看了NameNode的日志发现几行警告信息：</p>

<pre><code>2014-11-12 21:31:38,478 WARN org.apache.hadoop.hdfs.server.namenode.NameNodeResourceChecker (org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeResourceMonitor@1f1dd4b6): Space available o
n volume 'null' is 13119488, which is below the configured reserved amount 104857600
2014-11-12 21:31:38,478 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem (org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeResourceMonitor@1f1dd4b6): NameNode low on available di
sk space. Entering safe mode.
2014-11-12 21:31:38,478 INFO org.apache.hadoop.hdfs.StateChange (org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeResourceMonitor@1f1dd4b6): STATE* Safe mode is ON. 
Resources are low on NN. Please add or free up more resources then turn off safe mode manually. NOTE:  If you turn off safe mode before adding resources, the NN will immediately return to safe mode
. Use "hdfs dfsadmin -safemode leave" to turn safe mode off.
</code></pre>

<p>通过日志分析发现这样的错误：<code>NameNode low on available disk space. Entering safe mode</code></p>

<p>在网上查到hadoop-jira上面有这样的记录：<a href="https://issues.apache.org/jira/browse/HDFS-4425">https://issues.apache.org/jira/browse/HDFS-4425</a></p>

<p>另外这里还有详细的解释：<a href="https://support.pivotal.io/hc/en-us/articles/201455807-Namenode-logs-reports-Space-available-on-volume-null-is-below-threshold-and-enters-safe-mode">https://support.pivotal.io/hc/en-us/articles/201455807-Namenode-logs-reports-Space-available-on-volume-null-is-below-threshold-and-enters-safe-mode</a></p>

<p>原因是Hadoop的源码里面有一个类 <code>NameNodeResourceChecker</code> 负责检查NameNode的磁盘空间，如果磁盘空间低于100M则进入到安全模式，导致系统不可用，<strong>那么问题来了</strong>，是什么原因导致磁盘空间暴涨了呢？最后发现是HBase的DEBUG日志没有关闭导致日志文件非常大，最终磁盘空间占满，NameNode不可用。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop Streaming编程(Python)]]></title>
    <link href="http://findhy.github.io/blog/2014/07/29/hadoop-streaming-python/"/>
    <updated>2014-07-29T17:17:59+08:00</updated>
    <id>http://findhy.github.io/blog/2014/07/29/hadoop-streaming-python</id>
    <content type="html"><![CDATA[<p>Hadoop Streaming是Hadoop提供的一个工具，可以允许开发者以非Java的编程语言开发Mapper和Reducer，而且在开发过程中，我们只需要专注于数据的输入和输出处理，其它的事情都交给这个工具来做。本例是用Python实现一个简单的Wordcount例子，环境为：</p>

<ul>
<li>Hadoop:CDH4.3.2</li>
<li>Python:2.6.6</li>
<li>OS:Centos 6.5</li>
<li>Hadoop Streaming:hadoop-2.0.0-cdh4.3.2/share/hadoop/tools/lib/hadoop-streaming-2.0.0-cdh4.3.2.jar</li>
</ul>


<!--more-->


<h3>代码实现</h3>

<p>Python实现wordcount的Mapper和Reducer，比较简单直接看代码，<em>mapper.py</em>：</p>

<pre><code>#!/usr/bin/python

import sys
for line in sys.stdin:
    line = line.strip()
    words = line.split()
    for word in words:
        print '%s\t%s' % (word,1)
</code></pre>

<p><em>reducer.py</em>:</p>

<pre><code>#!/usr/bin/python

from operator import itemgetter
import sys

current_word = None
current_count = 0
word = None

# input comes from STDIN
for line in sys.stdin:
    # remove leading and trailing whitespace
    line = line.strip()

    # parse the input we got from mapper.py
    word, count = line.split('\t', 1)

    # convert count (currently a string) to int
    try:
        count = int(count)
    except ValueError:
        # count was not a number, so silently
        # ignore/discard this line
        continue

    # this IF-switch only works because Hadoop sorts map output
    # by key (here: word) before it is passed to the reducer
    if current_word == word:
        current_count += count
    else:
        if current_word:
            # write result to STDOUT
            print '%s\t%s' % (current_word, current_count)
        current_count = count
        current_word = word

# do not forget to output the last word if needed!
if current_word == word:
    print '%s\t%s' % (current_word, current_count)
</code></pre>

<p>这里需要注意第一行是：<em>#!/usr/bin/python</em>，而不是其它例子中的<em>#!/usr/bin/env python</em></p>

<h3>数据准备</h3>

<p>下载下面链接的数据，拷贝到HDFS中</p>

<ul>
<li><a href="http://www.gutenberg.org/ebooks/20417">The Outline of Science, Vol. 1 (of 4) by J. Arthur Thomson</a></li>
<li><a href="http://www.gutenberg.org/ebooks/5000">The Notebooks of Leonardo Da Vinci</a></li>
<li><a href="http://www.gutenberg.org/ebooks/4300">Ulysses by James Joyce</a></li>
</ul>


<h3>任务提交</h3>

<p>这块试了很多次，很多命令参数MR1和MR2不同的，比如必须这么写才可以：<em>-mapper &ldquo;python mapper.py&rdquo;</em>，下面是完整的执行脚本，经过测试没有问题：</p>

<pre><code>hadoop jar /home/hadoop/hadoop-2.0.0-cdh4.3.2/share/hadoop/tools/lib/hadoop-streaming-2.0.0-cdh4.3.2.jar -input /test/data/* -output /test/output  -file ./*.py -mapper "python mapper.py" -reducer  "python reducer.py" -numReduceTasks 2
</code></pre>

<p>完整代码参考这里项目：<a href="https://github.com/findhy/python-example/tree/master/hadoop/wordcount">https://github.com/findhy/python-example/tree/master/hadoop/wordcount</a></p>

<h3>参考</h3>

<p><a href="http://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/">http://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop 2.2.0-cdh5.0.0-beta-2 Install on Centos]]></title>
    <link href="http://findhy.github.io/blog/2014/05/14/cdh5-hadoop-2-2-0-install/"/>
    <updated>2014-05-14T23:08:37+08:00</updated>
    <id>http://findhy.github.io/blog/2014/05/14/cdh5-hadoop-2-2-0-install</id>
    <content type="html"><![CDATA[<p>环境说明</p>

<pre><code>master 10.0.1.252  
slave1 10.0.1.252  
slave2 10.0.1.252  
</code></pre>

<p>软件版本</p>

<pre><code>Hadoop 2.2.0-cdh5.0.0-beta-2  
JDK 1.7.0_45
</code></pre>

<!--more-->


<h2>开始安装</h2>

<hr />

<h3>1.创建用户</h3>

<pre><code>useradd hadoop
</code></pre>

<h3>2.修改密码</h3>

<pre><code>passwd hadoop
</code></pre>

<h3>3.修改HOSTS文件</h3>

<pre><code>10.0.1.252 master  
10.0.1.253 slave1  
10.0.1.254 slave2  
</code></pre>

<h3>4.节点互信配置</h3>

<p>在每个节点执行</p>

<pre><code>ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa
cat ~/.ssh/id_dsa.pub &gt;~/.ssh/authorized_keys
</code></pre>

<p>更改权限</p>

<pre><code>[hadoop@master .ssh]$ chmod 600 authorized_keys
[hadoop@master ~]$ chmod 700 .ssh/
</code></pre>

<p>在master节点操作<br/>
拷贝其它节点的公钥到authorized_keys文件中</p>

<pre><code>[hadoop@master .ssh]$ ssh hadoop@slave1 cat ~/.ssh/authorized_keys &gt;&gt; authorized_keys
[hadoop@master .ssh]$ ssh hadoop@slave2 cat ~/.ssh/authorized_keys &gt;&gt; authorized_keys
</code></pre>

<p>然后将公钥拷贝到其它节点</p>

<pre><code>[hadoop@master .ssh]$ scp authorized_keys hadoop@slave1:~/.ssh/
[hadoop@master .ssh]$ scp authorized_keys hadoop@slave2:~/.ssh/
</code></pre>

<p>测试一下，不需要密码就可以访问</p>

<pre><code>ssh master  
Ssh slave1  
Ssh slave2  
</code></pre>

<p>首次会让输入yes，后面就可以直接登录了</p>

<h3>5.JDK安装</h3>

<p>用java –version检查是否安装了JDK，如果没有安装，则参照下面的连接安装：
<a href="https://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH5/latest/CDH5-Installation-Guide/cdh5ig_oracle_jdk_installation.html#topic_29_1">https://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH5/latest/CDH5-Installation-Guide/cdh5ig_oracle_jdk_installation.html#topic_29_1</a></p>

<h3>6.载CDH安装包</h3>

<p>下载解压：<a href="http://archive-primary.cloudera.com/cdh5/cdh/5/hadoop-2.2.0-cdh5.0.0-beta-2.tar.gz">http://archive-primary.cloudera.com/cdh5/cdh/5/hadoop-2.2.0-cdh5.0.0-beta-2.tar.gz</a></p>

<pre><code>tar –zxvf hadoop-2.2.0-cdh5.0.0-beta-2.tar.gz
</code></pre>

<h3>7.修改hadoop-env.sh</h3>

<p>修改${HADOOP_HOME}/etc/hadoop/hadoop-env.sh</p>

<pre><code>export JAVA_HOME=/usr/java/jdk1.7.0_45
</code></pre>

<h3>8.修改mapred-site.xml</h3>

<p>在${HADOOP_HOME}/etc/hadoop/目录中，将mapred-site.xml.templat重命名成mapred-site.xml：</p>

<pre><code>mv mapred-site.xml.template mapred-site.xml
</code></pre>

<p>并添加以下内容：</p>

<pre><code>&lt;property&gt;
&lt;name&gt;mapreduce.framework.name&lt;/name&gt;
&lt;value&gt;yarn&lt;/value&gt;
&lt;/property&gt; 
</code></pre>

<h3>9.修改core-site.xml</h3>

<p>修改${HADOOP_HOME}/etc/hadoop/core-site.xml</p>

<pre><code>&lt;property&gt;
&lt;name&gt;fs.default.name&lt;/name&gt;
&lt;value&gt;hdfs://master:8020&lt;/value&gt;
&lt;final&gt;true&lt;/final&gt;
&lt;/property&gt;
</code></pre>

<h3>10.修改yarn-site.xml</h3>

<p>修改${HADOOP_HOME}/etc/hadoop/yarn-site.xml：</p>

<pre><code>&lt;property&gt;
&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
&lt;value&gt;mapreduce_shuffle&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt;
&lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;
&lt;/property&gt;
</code></pre>

<h3>11.修改hdfs-site.xml</h3>

<p>修改${HADOOP_HOME}/etc/hadoop/hdfs-site.xml</p>

<pre><code>&lt;property&gt;
&lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;
&lt;value&gt;/home/hadoop/dfs/yarn/name&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
&lt;value&gt;/home/hadoop/dfs/yarn/data&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;dfs.replication&lt;/name&gt;
&lt;value&gt;1&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;dfs.permissions&lt;/name&gt;
&lt;value&gt;false&lt;/value&gt;
&lt;/property&gt;
</code></pre>

<h3>12.修改slaves文件</h3>

<p>在slaves文件中添加你的slave节点：</p>

<pre><code>slave1
slave2
</code></pre>

<h3>13.修改masters文件</h3>

<p>在masters文件中添加你的master节点：</p>

<pre><code>master
</code></pre>

<h3>14.将安装包复制到其它节点</h3>

<pre><code>scp -r /home/hadoop/hadoop-2.2.0-cdh5.0.0-beta-2 hadoop@slave1:/home/hadoop/hadoop-2.2.0-cdh5.0.0-beta-2
scp -r /home/hadoop/hadoop-2.2.0-cdh5.0.0-beta-2 hadoop@slave2:/home/hadoop/hadoop-2.2.0-cdh5.0.0-beta-2
</code></pre>

<p><em>上面配置文件对应的目录在每个节点都需要提前创建好</em></p>

<h3>15.初始化HDFS</h3>

<pre><code>./bin/hadoop namenode –format
</code></pre>

<h3>16.启动HDFS</h3>

<pre><code>./sbin/start-dfs.sh
</code></pre>

<h3>17.启动YARN</h3>

<pre><code>./sbin/start-yarn.sh
</code></pre>

<h3>18.测试集群</h3>

<p>自己创建一个文件put到HDFS的/test/in目录下面
执行：</p>

<pre><code>./bin/hadoop jar share/hadoop/mapreduce2/hadoop-mapreduce-examples-2.2.0-cdh5.0.0-beta-2.jar wordcount /test/in/ /test/out/wordcountr
</code></pre>

<h3>19.YARN管理地址</h3>

<p><a href="http://master:8089/cluster/cluster">http://master:8089/cluster/cluster</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Oracle DBA如何转型到Hadoop平台]]></title>
    <link href="http://findhy.github.io/blog/2014/03/20/the-hadoop-faq-for-oracle-dbas/"/>
    <updated>2014-03-20T15:02:44+08:00</updated>
    <id>http://findhy.github.io/blog/2014/03/20/the-hadoop-faq-for-oracle-dbas</id>
    <content type="html"><![CDATA[<p>Oracle DBA如何转型到Hadoop平台，这篇博客是Cloudera的工程师Gwen Shapira的回答，因为他以前就是从事Oracle DBA的。原文在<a href="http://blog.cloudera.com/blog/2014/01/the-hadoop-faq-for-oracle-dbas/">这里</a>。对于国内的一些DBA想转型Hadoop的可以参考一下。</p>

<!--more-->


<p></p>

<blockquote><p>Q：<strong>如果转型或者学习Hadoop平台的技术，Oracle DBA的经验还有用吗？</strong></p></blockquote>

<p>A：在我合作的很多客户当中，我们和DBA团队一起合作将他们的数据仓库从Teradata或者Netezza迁移到Hadoop平台中，他们和我一起写Sqoop脚本、Oozie流程、Hive ETL脚本和Impala报表，几个月之后，我走了，原来的DBA团队已经掌握了Hadoop的技能了。<br/>
我现在在Cloudera的解决方案架构师团队也会录用前DBA人员来作为解决方案顾问或系统工程师，我们认为他们的DBA经验非常有价值。</p>

<blockquote><p>Q：<strong>公司会录用一个没有Hadoop工作经验的DBA吗，如果会，看重的是什么呢？</strong></p></blockquote>

<p>A：我坚信DBAs完全有能力成为优秀的Hadoop专家 &ndash; 但这不代表所有的DBAs。下面是一些我觉得必备的技能：</p>

<ul>
<li><strong>适应命令行操作</strong>：只会鼠标操作点击的DBAs和ETL工程师不合适</li>
<li><strong>Linux经验</strong>：Hadoop是运行在Linux上的，所以你需要非常适应Linux的文件系统、工具还有命令行，你还要理解CPU调度和IO等相关概念。</li>
<li><strong>网络方面的知识</strong>：<a href="http://www.technology-training.co.uk/understandingtheiso7layermodel_10.php">ISO7层网络架构</a>，ssh原理，主机名称解析（/etc/hosts）,还有交换机。</li>
<li><strong>良好SQL能力</strong>：你需要熟练使用SQL语句，有数据仓库分区和并行处理经验、ETL经验和性能优化经验的优先。</li>
<li><strong>编程能力</strong>：不一定是Java，但是你要会写批处理脚本，不管是用Perl、Python还是其他语言，要会用伪代码来解决一些简单的问题，如果你不会编程，那就有问题了。</li>
<li><strong>解决问题能力</strong>：Hadoop没有Oracle那么成熟，所以你必须要学会用Google或者其他方式来搜索和解决问题，这个很重要。</li>
<li><strong>更高级的职位，我还关注系统和架构方面的能力</strong>：比如你做过飞行调度系统或者其他相似的东西。</li>
<li><strong>因为我们需要面对客户，所以沟通能力也很重要</strong>：如何倾听？如何向客户解释一个负责的技术问题？当我质疑你的时候该如何反应？</li>
</ul>


<p>或许这些太多了，但是我觉得这些你转型Hadoop平台必备的技能。</p>

<blockquote><p>Q：<strong>怎么开始学习Hadoop？</strong></p></blockquote>

<p>A：首先你需要在自己的电脑上搭建一个Hadoop集群环境，这个网上的资料很多。然后你可以导入一些数据到Hadoop中来分析，这里有一个例子是用Flume导入Twitter数据然后用Hive来分析：</p>

<ul>
<li><a href="http://blog.cloudera.com/blog/2012/09/analyzing-twitter-data-with-hadoop/">http://blog.cloudera.com/blog/2012/09/analyzing-twitter-data-with-hadoop/</a></li>
<li><a href="http://blog.cloudera.com/blog/2012/10/analyzing-twitter-data-with-hadoop-part-2-gathering-data-with-flume/">http://blog.cloudera.com/blog/2012/10/analyzing-twitter-data-with-hadoop-part-2-gathering-data-with-flume/</a></li>
</ul>


<p>还有就是买一些Hadoop方面的书籍，比如「Hadoop权威指南」、「Hadoop实战」等等。
推荐看下<a href="http://dongxicheng.org/mapreduce-nextgen/hadoope-every-day/">这里</a>，是一些Hadoop方面的学习资源。</p>

<blockquote><p>Q：<strong>我需要学习Java吗？</strong></p></blockquote>

<p>A：你不需要成为一个特别专业的Java开发人员，但是你需要能够看懂Java源代码，因为Hadoop就是Java写的，还有Hive UDFS也需要Java来编写，相信我，不是很难，比SQL简单多了。</p>
]]></content>
  </entry>
  
</feed>
