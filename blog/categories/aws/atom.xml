<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: AWS | Findhy's Blog]]></title>
  <link href="http://findhy.com/blog/categories/aws/atom.xml" rel="self"/>
  <link href="http://findhy.com/"/>
  <updated>2014-11-28T10:23:52+08:00</updated>
  <id>http://findhy.com/</id>
  <author>
    <name><![CDATA[Findhy]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[基于云的Hadoop架构]]></title>
    <link href="http://findhy.com/blog/2014/07/25/hadoop-cloud/"/>
    <updated>2014-07-25T12:18:55+08:00</updated>
    <id>http://findhy.com/blog/2014/07/25/hadoop-cloud</id>
    <content type="html"><![CDATA[<p>今天看了Netflix分享的基于AWS的大数据平台Hadoop架构，感觉这是未来大数据平台的发展模式，将整个数据仓库部署在云端，开发者不需要考虑集群的资源扩展，也不需要接触复杂的底层命令，通过一个简单RESTFul接口就可以提交我们的MapReduce任务，还可以实时看到任务和运行状态。</p>

<!--more-->


<h3>Netflix基于AWS的Hadoop架构介绍</h3>

<p><img src="/images/hadoopcloud1.png"></p>

<p>Netflix的Hadoop云架构是完全基于AWS来搭建，底层存储采用S3，Hadoop组件采用AWS上原生的EMR，PAAS层是自己开发的Genie(目前已经开源)，开发者或者数据分析师通过Genie提供的RESTFul接口来提交和管理任务，由于AWS云带来的弹性扩展的好处，整个集群可以任意横向和纵向扩展，而开发者只需要专注在算法和业务上即可。</p>

<p>Netflix官方博客介绍：<br/>
<a href="http://techblog.netflix.com/2013/01/hadoop-platform-as-service-in-cloud.html">http://techblog.netflix.com/2013/01/hadoop-platform-as-service-in-cloud.html</a></p>

<p>Infoq中文介绍：<br/>
<a href="http://www.infoq.com/cn/news/2013/02/netflix-hadoop-PaaS">http://www.infoq.com/cn/news/2013/02/netflix-hadoop-PaaS</a></p>

<p>Genie介绍PPT：<br/>
<iframe src="http://www.slideshare.net/slideshow/embed_code/24063535 " width="595" height="446" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC;border-width:1px 1px 0;margin-bottom:5px" allowfullscreen></iframe> </p>

<p>Genie已经开源：<br/>
<a href="https://github.com/Netflix/genie">https://github.com/Netflix/genie</a></p>

<p>Hadoop on OpenStack介绍PPT： <br/>
<iframe src="http://www.slideshare.net/slideshow/embed_code/18948566 " width="595" height="446" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC;border-width:1px 1px 0;margin-bottom:5px" allowfullscreen></iframe> </p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[S3cmd 多账户配置]]></title>
    <link href="http://findhy.com/blog/2014/07/24/s3cmd-multiple-accounts/"/>
    <updated>2014-07-24T15:16:17+08:00</updated>
    <id>http://findhy.com/blog/2014/07/24/s3cmd-multiple-accounts</id>
    <content type="html"><![CDATA[<p>关于s3cmd的安装配置参考这篇文章：<a href="http://findhy.com/blog/2014/06/05/s3cmd-config/">s3cmd Configure</a>，本文介绍s3cmd多账户配置。</p>

<!--more-->


<h4>复制配置文件</h4>

<p>s3cmd安装完成之后，在home/user/目录下会生成一个 .s3cfg 配置文件，在复制一个出来</p>

<pre><code>cp .s3cfg .s3ad
</code></pre>

<h4>修改配置文件</h4>

<pre><code>vi .s3ad
将access_key和secret_key修改成对应账户的值
</code></pre>

<h4>测试</h4>

<p>使用 -c 指令指定配置文件</p>

<pre><code>s3cmd -c ~/.s3ad ls s3://log-ad/adserver/ 
</code></pre>

<h4>改进</h4>

<p>会不会觉得每次用 -c 指定配置文件太麻烦了，用 Bash 别名，我们用一个别名来替代 <em>s3cmd -c ~/.s3ad</em></p>

<pre><code>vi ~/.bashrc  
添加
alias s3ad='s3cmd -c ~/.s3ad'  

执行下面命令，让配置文件生效
source .bashrc  

测试
s3ad ls s3://log-ad/adserver/

OK!这样就不用每次指定配置文件了
</code></pre>

<h4>参考</h4>

<p><a href="https://blog.techopsguru.com/2011/12/s3-bucket-copying-with-multiple-accounts.html  ">https://blog.techopsguru.com/2011/12/s3-bucket-copying-with-multiple-accounts.html  </a>
<a href="http://mikesisk.tumblr.com/post/8703449578/s3cmd-and-multiple-accounts  ">http://mikesisk.tumblr.com/post/8703449578/s3cmd-and-multiple-accounts  </a>
<a href="http://mikesisk.com/2011/08/09/s3cmd-with-multiple-aws-accounts/">http://mikesisk.com/2011/08/09/s3cmd-with-multiple-aws-accounts/</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[S3cmd Configure]]></title>
    <link href="http://findhy.com/blog/2014/06/05/s3cmd-config/"/>
    <updated>2014-06-05T18:52:03+08:00</updated>
    <id>http://findhy.com/blog/2014/06/05/s3cmd-config</id>
    <content type="html"><![CDATA[<p>s3cmd是AWS S3的命令行工具，可以用来下载、上传、同步文件，还可以配置权限。</p>

<!--more-->


<h3>一、配置</h3>

<h4>1.安装python-pip</h4>

<pre><code>yum install python-pip
</code></pre>

<p>如果提示No package python-pip available，先安装<a href="http://dl.fedoraproject.org/pub/epel/6/i386/epel-release-6-8.noarch.rpm">epel-release-6-8.noarch.rpm</a></p>

<pre><code>rpm -ivh epel-release-6-8.noarch.rpm
</code></pre>

<h4>2.安装s3cmd</h4>

<pre><code>pip install s3cmd
</code></pre>

<h4>3.配置s3cmd</h4>

<pre><code>s3cmd --configure
</code></pre>

<p>分别输入Access Key和Secret Key</p>

<pre><code>Access Key:
Secret Key:
</code></pre>

<p>回车，如果提示：</p>

<pre><code>Encryption password is used to protect your files from reading
by unauthorized persons while in transfer to S3
Encryption password:
</code></pre>

<p>输入当前登录的操作系统用户</p>

<h4>4.HTTPS协议选择no</h4>

<pre><code>Use HTTPS protocol [No]: no
</code></pre>

<h4>5.代理不填直接回车</h4>

<pre><code>HTTP Proxy server name: 
</code></pre>

<h4>6.最后就是测试保存</h4>

<pre><code>Test access with supplied credentials? [Y/n] y
Save settings? [y/N] y
</code></pre>

<h4>7.测试</h4>

<pre><code>s3cmd ls
</code></pre>

<h4>8.下载整个目录</h4>

<pre><code>s3cmd get --recursive dir_name
</code></pre>

<h3>二、使用</h3>

<h4>1.查看所有的Buckets</h4>

<pre><code>s3cmd ls
</code></pre>

<h4>2.创建Buckets</h4>

<pre><code>s3cmd mb s3://my-bucket-name
</code></pre>

<h4>3.删除空Buckets</h4>

<pre><code>s3cmd rb s3://my-bucket-name
</code></pre>

<h4>4.上传</h4>

<pre><code>上传单个文件：s3cmd put file.txt s3://my-bucket-name/file.txt
批量上传：s3cmd put ./* s3://my-bucket-name/
上传整个目录：s3cmd put -r dir1 s3://my-bucket-name/
上传目录下所有文件：s3cmd put -r dir1/ s3://my-bucket-name/
</code></pre>

<h4>5.下载</h4>

<pre><code>下载单个文件：s3cmd get s3://my-bucket-name/file.txt file.txt
下载整个目录下的文件：s3cmd get -r  s3://my-bucket-name/bucket_dir ./
</code></pre>

<h4>6.删除文件</h4>

<pre><code>s3cmd del s3://my-bucket-name/file.txt
</code></pre>

<h4>7.查看空间大小</h4>

<pre><code>s3cmd du -H s3://my-bucket-name
</code></pre>

<h4>8.同步</h4>

<pre><code>同步当前目录下所有文件：s3cmd sync  ./  s3://my-bucket-name/
只列出不同步：s3cmd sync  --dry-run ./  s3://my-bucket-name/
同步且删除本地不存在的文件：s3cmd sync  --delete-removed ./  s3://my-bucket-name/
不检验跳过本地已经存在的文件：s3cmd sync  --skip-existing ./  s3://my-bucket-name/
排除包含文件规则：s3cmd sync --dry-run --exclude '*.txt' --include 'dir2/*' ./ 
更多参考这里：http://s3tools.org/s3cmd-sync
</code></pre>
]]></content>
  </entry>
  
</feed>
