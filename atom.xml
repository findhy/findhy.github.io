<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Findhy's Blog]]></title>
  <link href="http://findhy.github.io/atom.xml" rel="self"/>
  <link href="http://findhy.github.io/"/>
  <updated>2017-07-21T15:42:48+08:00</updated>
  <id>http://findhy.github.io/</id>
  <author>
    <name><![CDATA[Findhy]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Hello Baidu]]></title>
    <link href="http://findhy.github.io/blog/2016/02/15/hello-baidu/"/>
    <updated>2016-02-15T12:03:55+08:00</updated>
    <id>http://findhy.github.io/blog/2016/02/15/hello-baidu</id>
    <content type="html"><![CDATA[<p class='post-footer'>
            original link:
            <a href='http://findhy.github.io/blog/2016/02/15/hello-baidu/'>http://findhy.github.io/blog/2016/02/15/hello-baidu/</a><br/>
            written by <a href='http://findhy.github.io'>Findhy</a>
            &nbsp;posted at <a href='http://findhy.github.io'>http://findhy.github.io</a>
            </p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop数据倾斜问题总结]]></title>
    <link href="http://findhy.github.io/blog/2015/01/16/hadoop-data-tilt/"/>
    <updated>2015-01-16T09:41:26+08:00</updated>
    <id>http://findhy.github.io/blog/2015/01/16/hadoop-data-tilt</id>
    <content type="html"><![CDATA[<p>首先解释什么是数据倾斜问题，举一个例子，图书馆有A0、A1、A2&hellip;A9共10个书架，A0书架有1000本书，而A1到A9书架有100本书，现在要统计图书的总数量，这时我们会发现A1到A9很快就统计完了，而A0书架需要统计很长时间，显然A0成为整个统计过程的瓶颈。</p>

<p>Hadoop最根本的思想就是分而治之，数据倾斜会导致分布式的计算效率依赖单个节点，这与它的初衷是背道而驰的，但是Hadoop又没有从根本层面解决这个问题，所以需要我们在业务设计和开发时来解决这个问题，最好的解决方法是设计时避免让这个问题出现。如果无法避免，那么解决数据倾斜问题和核心思想只有一个：让map输出的key均匀分布到各个reduce中去（mapjoin例外，直接在内存中解决，牺牲内存空间提高效率，只适用于较小的输入数据）</p>

<p>产生数据倾斜的具体原因以及解决方案可以参考阿里的这篇博客：<a href="http://www.alidata.org/archives/2109">http://www.alidata.org/archives/2109</a></p>

<p>本文主要想总结一下各种处理方案的思路：</p>

<!--more-->


<h3>1、Distributed Cache</h3>

<p>Distributed Cache是Hadoop提供的文件缓存工具，会自动将需要缓存的文件分发到各个DataNode上去，对于key分布不均匀的数据并且数据量不大，可以使用Distributed Cache将文件缓存到各个节点上，这样在做map的时候直接从内存中读取数据不需要到reduce阶段处理从而提高性能。</p>

<p>创建job时可以添加需要cache的文件</p>

<pre><code>Job job = new Job();
...
job.addCacheFile(new Path(cacheFilename).toUri());
</code></pre>

<p>在mapper端使用</p>

<pre><code>Path[] localPaths = context.getLocalCacheFiles();
...
</code></pre>

<h3>2、Map Join</h3>

<p>如果是Hive，就是用Map Join，底层用的就是Distributed Cache，</p>

<pre><code>SELECT /*+ MAPJOIN(b) */ a.key, a.value
FROM a JOIN b ON a.key = b.key
</code></pre>

<p>在hive0.7之后，可以不用显示的指定MAPJOIN关键词，hive会根据参数配置来决定是否需要用到mapjoin</p>

<pre><code>hive.auto.convert.join=true//默认打开mapjoin优化
hive.mapjoin.smalltable.filesize=25000000//文件达到多大时开启mapjoin优化，阀值控制
</code></pre>

<h3>3、转换map输出的key值</h3>

<p>如果我们无法在设计时避免，也无法在map时处理，那只能在最后一刻来弥补，比如map输出的key大量为空，那么可以用一个随机数来代替空值，这样数据就是均摊到多个reduce上了</p>

<h3>4、步骤拆解</h3>

<p>将一步操作拆解为多步，将倾斜的数据单独拿出来处理，最后再与正常数据的结果合并就可以了，具体实现可以看这篇博客：<a href="http://www.alidata.org/archives/2109">http://www.alidata.org/archives/2109</a>  最后的总结</p>

<h3>5、参数调整</h3>

<pre><code>hive.map.aggr = true//map端部分聚合，相当于Combiner
hive.groupby.skewindata=true//有数据倾斜的时候自动进行负载均衡
</code></pre>

<p>如果大家以后遇到数据倾斜的问题，可以尝试从这几个方面来想办法优化。</p>

<p class='post-footer'>
            original link:
            <a href='http://findhy.github.io/blog/2015/01/16/hadoop-data-tilt/'>http://findhy.github.io/blog/2015/01/16/hadoop-data-tilt/</a><br/>
            written by <a href='http://findhy.github.io'>Findhy</a>
            &nbsp;posted at <a href='http://findhy.github.io'>http://findhy.github.io</a>
            </p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HBase通过Mysql建立二级索引]]></title>
    <link href="http://findhy.github.io/blog/2014/12/06/hbase-index-mysql/"/>
    <updated>2014-12-06T16:28:39+08:00</updated>
    <id>http://findhy.github.io/blog/2014/12/06/hbase-index-mysql</id>
    <content type="html"><![CDATA[<p>HBase查询数据有三种模式：Rowkey查询、Rowkey范围scan和全表扫描，HBase的Rowkey查询效率很高，毫秒级的，所以我们应该尽量对HBase使用Rowkey查询，而避免范围和全表扫描，思路有很多种，一种是Rowkey的巧妙设计，例如Rowkey是userId#timestamp，这样就可以支持用户和时间戳的查询了，这种方式最简单但也比较局限，例如无法支持较复杂的查询条件，还有Rowkey设计与region数据分布的问题，我们总是希望数据在多个region上均匀分布，又希望经常被一起查的数据在同一个region上，所以全靠Rowkey设计来解决还是比较局限的，本文介绍通过Mysql来建立HBase二级索引的方式，来实现HBase高效率的分页查询、复杂条件组合查询。</p>

<h2>思路</h2>

<ul>
<li>在Mysql中设计HBase的索引表，Rowkey作为主键，其它查询条件作为其它字段，由于数据量很大，Mysql索引表需要做分表</li>
<li>通过Python编码实现定时将HBase的Rowkey扫描到Mysql中，这里注意一个点，每次扫描完将最后的Rowkey记录在redis中，下次扫描从这个Rowkey开始</li>
<li>前端分页查询先查询索引表，这样就能够实现和Mysql一样的分页效果，总行数还有直接跳转到具体页，HBase是实现不了直接跳转到具体页的</li>
<li>从Mysql中根据查询条件查询到具体的Rowkey，再用这些Rowkey到HBase中批量查询，每一个都是Rowkey直接定位，批量查询效率也是毫秒左右返回，整体查询效率是在秒级响应的</li>
</ul>


<!--more-->


<h2>Mysql索引表设计</h2>

<p>索引表设计尽量小，只存Rowkey、时间还有必要的查询字段，Rowkey为主键，并且索引表一定要分区，因为HBase的数据量很大，而Mysql只能撑住单表几百万的数据量，考虑到我们目前的数据量不是很大，做了按月分区：</p>

<pre><code>DROP TABLE IF EXISTS INDEX_HBASE_CLICK_201412;
CREATE TABLE INDEX_HBASE_CLICK_201412
(
   ROW_KEY VARCHAR(100) NOT NULL PRIMARY KEY,
   CLICK_TIME DATETIME NOT NULL COMMENT 'CLICK_TIME',
   PUBLISHER_ID INT(11) COMMENT '媒体ID',
   OFF_ID INT(11) COMMENT '订单ID',
   BUSINESS_ID INT(11) COMMENT '商务ID',
   DATABASE_TIME DATETIME COMMENT '入mysql时间'
) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT='HBASE_CLICK索引表 ';
</code></pre>

<h2>扫描Rowkey入Mysql</h2>

<p>这部分是通过Python来实现的，无法提供完整代码，思路就是定时每隔30秒去扫描HBase，然后记住最后一个Rowkey，下次扫描的时候从这个Rowkey开始。其中HbaseDb是通过thrift2来访问HBase。</p>

<pre><code>from adunion.db.mysql_db import Mysql
from adunion.db.hbase_db2 import HbaseDb
import time
from adunion.redis.redis_count import redisCount
from core.logger import logUtil
from adunion.msg.msg_obj import msgObj

class AdunionIndex(object):
    hbaseDb=None
    work_status=False
    def __init__(self):
        self.hbaseDb=HbaseDb()
    def start_work(self):
        if self.work_status:
            msgObj.addRedisCount("work_status is True!!!")
            return
        self.work_status=True
        start_time=time.time()
        msgObj.addHbaseIndexLog("AdunionIndex.AdunionIndex.start_work:begin")
        last_row_key=redisCount.getHbaseLastKey()
        new_last_row_key=self.hbaseScan(last_row_key)
        redisCount.setHbaseLastKey(new_last_row_key)
        self.work_status=False
        used=time.time()-int(start_time)
        msgObj.addHbaseIndexLog("AdunionIndex.AdunionIndex.start_work:end,used:"+str(used))
    def hbaseScan(self,last_row_key):
        try:
            self.hbaseDb.begin()
            hbase_index_list_click={}
            hbase_index_list_postback={}
            row_family_qualifier_list=[]
            row_family_qualifier_list.append(("c","b"))
            row_family_qualifier_list.append(("c","o"))
            row_family_qualifier_list.append(("c","p"))
            row_family_qualifier_list.append(("p","b"))
            row_family_qualifier_list.append(("p","o"))
            row_family_qualifier_list.append(("p","p"))
            scannerId=self.hbaseDb.openScanner("adunion_log",last_row_key,row_family_qualifier_list)
            result = self.hbaseDb.getScannerRows(scannerId, 10)
            new_last_row_key=None
            if not result:
                msgObj.addHbaseIndexLog("AdunionIndex.AdunionIndex.hbaseScan:result is null!!!")
                return last_row_key
            last_res=result.pop()
            new_last_row_key=last_res.row
            for res in result:
                row_key=res.row
                time_str=row_key[row_key.index('_')+1:row_key.index('_')+11]
                mon_key=time.strftime('%Y%m', time.localtime(float(time_str)))
                if not hbase_index_list_click.has_key(mon_key):
                    hbase_index_list_click[mon_key]={}
                if not hbase_index_list_postback.has_key(mon_key):
                    hbase_index_list_postback[mon_key]={}
                for col in res.columnValues:
                    if col.family=="c":
                        if not hbase_index_list_click[mon_key].has_key(row_key):
                            hbase_index_list_click[mon_key][row_key]=[]
                        hbase_index_list_click[mon_key][row_key].append(col.qualifier+"_"+col.value)
                        hbase_index_list_click[mon_key][row_key].append("timestamp"+"_"+str(col.timestamp))
                    elif col.family=="p":
                        if not hbase_index_list_postback[mon_key].has_key(row_key):
                            hbase_index_list_postback[mon_key][row_key]=[]
                        hbase_index_list_postback[mon_key][row_key].append(col.qualifier+"_"+col.value)
                        hbase_index_list_postback[mon_key][row_key].append("timestamp"+"_"+str(col.timestamp))
            self.hbaseIndexToMysqlClick(hbase_index_list_click)
            self.hbaseIndexToMysqlPostback(hbase_index_list_postback)
            return new_last_row_key
        except Exception,e:
            logUtil.log("AdunionIndex.hbase_scan"+",error:"+str(e))
            import traceback
            msgObj.addErrorLog("AdunionIndex.hbase_scan error:"+str(traceback.format_exc()))
            logUtil.log(str(traceback.format_exc()))
        finally:
            self.hbaseDb.end()
    def hbaseIndexToMysqlReal(self,hbase_index_param_list,sql):
        mysql=Mysql()
        mysql.begin()
        try:
            row_size=len(hbase_index_param_list) 
            prop_size=5
            mysql.insertManyWithDup(sql,"@@".join(hbase_index_param_list),row_size,prop_size)       
            msgObj.addHbaseIndexLog("success insert into mysql,len:"+str(row_size)) 
        except Exception,e:
            import traceback
            logUtil.log(traceback.format_exc())
            msgObj.addErrorLog("AdunionIndex.hbaseIndexToMysqlReal error:"+str(traceback.format_exc()))
        finally:
            mysql.end();
    def hbaseIndexToMysqlPostback(self,hbase_index_list):
        sql="INSERT INTO INDEX_HBASE_POSTBACK_#mon#(ROW_KEY,POSTBACK_TIME,PUBLISHER_ID,OFF_ID,BUSINESS_ID,DATABASE_TIME) VALUES ('%0%','%1%',%2%,%3%,%4%,NOW()) on duplicate key update PUBLISHER_ID=PUBLISHER_ID"
        for mon_key in hbase_index_list.keys():
            sql=sql.replace('#mon#', mon_key)
            new_hbase_index_list=self.processParamList(hbase_index_list[mon_key])
            self.hbaseIndexToMysqlReal(new_hbase_index_list,sql)
    def hbaseIndexToMysqlClick(self,hbase_index_list):
        sql="INSERT INTO INDEX_HBASE_CLICK_#mon#(ROW_KEY,CLICK_TIME,PUBLISHER_ID,OFF_ID,BUSINESS_ID,DATABASE_TIME) VALUES('%0%','%1%',%2%,%3%,%4%,NOW()) on duplicate key update PUBLISHER_ID=PUBLISHER_ID"
        for mon_key in hbase_index_list.keys():
            sql=sql.replace('#mon#', mon_key)
            new_hbase_index_list=self.processParamList(hbase_index_list[mon_key])
            self.hbaseIndexToMysqlReal(new_hbase_index_list,sql)
    def processParamList(self,hbase_index_list):
        new_hbase_index_list=[]
        for row_key in hbase_index_list.keys():
            col_list=hbase_index_list[row_key]
            publisher_id=None
            business_id=None
            offer_id=None
            data_time=None
            for col in col_list:
                qualifier=col.split('_')[0]
                qualifier_value=col.split('_')[1]
                if qualifier=='p':
                    publisher_id=qualifier_value
                elif qualifier=='b':
                    business_id=qualifier_value
                elif qualifier=='o':
                    offer_id=qualifier_value
                elif qualifier=='timestamp':
                    data_time=qualifier_value
            data_time_str=time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(float(data_time)/1000))
            new_hbase_index_list.append(",".join([str(row_key),str(data_time_str),str(publisher_id),str(offer_id),str(business_id)]))
        return new_hbase_index_list
adunionIndex=AdunionIndex()
</code></pre>

<h2>Java实现查询</h2>

<p>对应业务层来说，他只需要查询Mysql的索引表就可以了，查出来Rowkey的list，再用这些Rowkey来批量查询HBase就可以了：</p>

<pre><code>public List&lt;Map&lt;String,String&gt;&gt; getRowByRowKeyFromHBase(List&lt;String&gt; row_key_list,String dataType){
        HConnection con = null;
        HTable table = null;
        List&lt;Map&lt;String,String&gt;&gt; resultList=new ArrayList&lt;Map&lt;String,String&gt;&gt;();
        try {
            con = HConnectionManager.createConnection(configuration);
            table=(HTable)con.getTable(ADUNION_TABLE_NAME);
            List&lt;Get&gt; gets=new ArrayList&lt;Get&gt;();
            String family=null;
            if(dataType!=null&amp;&amp;dataType.equals(DATA_TYPE_CLICK)){
                family="c";
            }else if(dataType!=null&amp;&amp;dataType.equals(DATA_TYPE_POSTBACK)){
                family="p";
            }
            for(int i=0;i&lt;row_key_list.size();i++){
                Get get=new Get(Bytes.toBytes(row_key_list.get(i)));
                get.addColumn(Bytes.toBytes(family), Bytes.toBytes("v"));
                gets.add(get);
            }
            Result[] result=table.get(gets);
            for(Result res:result){
                Map&lt;String,String&gt; map = new HashMap&lt;String,String&gt;();
                for(KeyValue keyValue:res.raw()){
                    String familyCol=new String(keyValue.getFamily());
                    byte[] value=keyValue.getValue();
                    //map.put("rowkey", new String(result.getRow()));
                    Map&lt;Object,Object&gt; jsonMap=JsonUtil.getMapFromJsonObjStr(new String(value));
                    for(Object obj:jsonMap.keySet()){
                        map.put(obj.toString(), jsonMap.get(obj).toString());
                    }
                }
                resultList.add(map);
            }
        }catch(IOException ioe){
            log.error(ioe.getMessage());
            ioe.printStackTrace();
        } catch (Exception e) {
            log.error(e.getMessage());
            e.printStackTrace();
        }finally{
            try {
                table.close();
            } catch (Exception e) {
                log.error(e.getMessage());
            }
            try {
                con.close();
            } catch (Exception e) {
                log.error(e.getMessage());
            }
        }
        return resultList;
    }&lt;p class='post-footer'&gt;
        original link:
        &lt;a href='http://findhy.github.io/blog/2014/12/06/hbase-index-mysql/'&gt;http://findhy.github.io/blog/2014/12/06/hbase-index-mysql/&lt;/a&gt;&lt;br/&gt;
        written by &lt;a href='http://findhy.github.io'&gt;Findhy&lt;/a&gt;
        &amp;nbsp;posted at &lt;a href='http://findhy.github.io'&gt;http://findhy.github.io&lt;/a&gt;
        &lt;/p&gt;
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HBase分页]]></title>
    <link href="http://findhy.github.io/blog/2014/11/28/hbase-page/"/>
    <updated>2014-11-28T09:45:23+08:00</updated>
    <id>http://findhy.github.io/blog/2014/11/28/hbase-page</id>
    <content type="html"><![CDATA[<p>HBase分页的难点在于两点：获取总行数困难、没有行标号。根据HBase的特点获取数据只有三种模式：RowKey唯一定位、Rowkey范围扫描和全表扫描。那么分页可以采用Rowkey的范围扫描，每次扫描一定范围内的数据并且通过PageFilter过滤来限定扫描的行数，如果我们每页10条，那么每次就扫描11条，记住第11条的Rowkey，当点击下一页的时候，将startRowkey设置为第11条的Rowkey，就实现了分页的效果，下面是相关的核心代码，具体项目可以参考<a href="https://github.com/findhy/hbase-page">github-HBase-page</a></p>

<pre><code>public class PageHBase {

    private Integer currentPageNo=1;//当前页码
    private Integer pageSize=5;//每页显示行数
    private Integer totalCount;//总行数
    private Integer totalPage;//总页数
    private Integer direction;//下一页：1 上一页2
    private Boolean hasNext=false;//是否有下一页
    private String nextPageRowkey;//下一页起始rowkey
    private List&lt;Map&lt;String, String&gt;&gt; resultList;//结果集List
    private Map&lt;String,String&gt; paramMap=new HashMap&lt;String,String&gt;();//分页查询参数
    private Map&lt;Integer,String&gt; pageStartRowMap=new HashMap&lt;Integer,String&gt;();//每页对应的startRow，key为currentPageNo，value为Rowkey
    private Scan scan=new Scan();

    public Scan getScan(String startRowkey,String endRowkey){
        scan.setCaching(100);
        if(direction==1&amp;&amp;hasNext){
            scan.setStartRow(Bytes.toBytes(startRowkey));
            scan.setStopRow(Bytes.toBytes(endRowkey));
        }else{
            if(pageStartRowMap.get(currentPageNo)!=null){
                scan.setStartRow(Bytes.toBytes(pageStartRowMap.get(currentPageNo)));
                scan.setStopRow(Bytes.toBytes(endRowkey));
            }else{
                scan.setStartRow(Bytes.toBytes(startRowkey));
                scan.setStopRow(Bytes.toBytes(endRowkey));
            }
        }
        this.hasNext=false;
        this.nextPageRowkey=null;
        return scan;
    }
}


public class AdunionHbaseService {

    private static final Logger log = LoggerFactory
            .getLogger(AdunionHbaseService.class);
    public static final String CLICK_COLUMN_NAME="c";
    public static final String CLICK_COLUMN_KEY="v";
    public static final String POSTBACK_COLUMN_NAME="p";
    public static final String POSTBACK_COLUMN_KEY="v";
    public static final String ADUNION_TABLE_NAME="adunion_active";
    private Configuration configuration;
    private String clientPort;
    private String retriesNumber;
    private String zookeeperQuorum;

    public AdunionHbaseService(String zookeeeper,String port,String retries){
        try {
            configuration = HBaseConfiguration.create();
            this.setClientPort(port);
            this.setRetriesNumber(retries);
            this.setZookeeperQuorum(zookeeeper);
            configuration.set("hbase.zookeeper.property.clientPort", this.getClientPort());
            configuration.set("hbase.client.retries.number", this.getRetriesNumber());
            configuration.set("hbase.zookeeper.quorum",this.getZookeeperQuorum());
        } catch (Exception e) {
            log.error(e.getMessage());
        }
    }

    public PageHBase getPageHBaseData(String tableName, String columnName,String columnKey,String businessId,String publisherId,String offerId,
            Date startDate,Date endDate, PageHBase pager) {
        HConnection con = null;
        HTable table = null;
        ResultScanner rs=null;
        List&lt;Map&lt;String,String&gt;&gt; resultList=new ArrayList&lt;Map&lt;String,String&gt;&gt;();
        try {
            if(startDate==null||endDate==null) return pager;

            con = HConnectionManager.createConnection(configuration);
            table=(HTable)con.getTable(tableName);

            StringBuffer startRow=new StringBuffer();
            StringBuffer endRow=new StringBuffer();
            startRow.append(startDate.getTime());
            endRow.append(endDate.getTime());
            if(pager.getNextPageRowkey()==null) pager.setNextPageRowkey(startRow.toString());
            Scan scan=pager.getScan(pager.getNextPageRowkey(),endRow.toString());

            List&lt;Filter&gt; filters=new ArrayList&lt;Filter&gt;();

            if(StringUtils.isNotBlank(publisherId)){
                SingleColumnValueFilter filter=new SingleColumnValueFilter(
                        Bytes.toBytes(columnName),
                        Bytes.toBytes("p"),
                        CompareFilter.CompareOp.EQUAL,
                        Bytes.toBytes(publisherId));
                filters.add(filter);
            }
            if(StringUtils.isNotBlank(offerId)){
                SingleColumnValueFilter filter=new SingleColumnValueFilter(
                        Bytes.toBytes(columnName),
                        Bytes.toBytes("o"),
                        CompareFilter.CompareOp.EQUAL,
                        Bytes.toBytes(offerId));
                filters.add(filter);
            }
            if(StringUtils.isNotBlank(businessId)){
                SingleColumnValueFilter filter=new SingleColumnValueFilter(
                        Bytes.toBytes(columnName),
                        Bytes.toBytes("b"),
                        CompareFilter.CompareOp.EQUAL,
                        Bytes.toBytes(businessId));
                filters.add(filter);
            }
            Filter pageFilter=new PageFilter(pager.getPageSize()+1);
            Filter familyFilter=new FamilyFilter(CompareFilter.CompareOp.EQUAL,
                    new BinaryComparator(Bytes.toBytes(columnName)));
            Filter qualifierFilter=new QualifierFilter(CompareFilter.CompareOp.EQUAL,
                    new BinaryComparator(Bytes.toBytes(columnKey)));
            filters.add(familyFilter);
            filters.add(qualifierFilter);
            filters.add(pageFilter);
            FilterList filterList=new FilterList(filters);
            scan.setFilter(filterList);
            rs = table.getScanner(scan);
            int totalRow=0;
            if(rs!=null){
                for(Result result : rs){
                    totalRow++;
                    if(totalRow==1){
                        pager.getPageStartRowMap().put(pager.getCurrentPageNo(),Bytes.toString(result.getRow()));
    pager.setTotalPage(pager.getPageStartRowMap().size());
    }
                    if(totalRow&gt;pager.getPageSize()){
                        pager.setNextPageRowkey(new String(result.getRow()));
                        pager.setHasNext(true);
                    }else{
                        Map&lt;String,String&gt; map = new HashMap&lt;String,String&gt;();
                        for(KeyValue keyValue:result.raw()){
                            String family=new String(keyValue.getFamily());
                            byte[] value=keyValue.getValue();
                            //map.put("rowkey", new String(result.getRow()));
                            Map&lt;Object,Object&gt; jsonMap=JsonUtil.getMapFromJsonObjStr(new String(value));
                            for(Object obj:jsonMap.keySet()){
                                map.put(obj.toString(), jsonMap.get(obj).toString());
                            }
                        }
                        resultList.add(map);
                    }
                }
            }
            pager.setResultList(resultList);
        }catch(IOException ioe){
            log.error(ioe.getMessage());
        } catch (Exception e) {
            log.error(e.getMessage());
        }finally{
            try {
                rs.close();
            } catch (Exception e) {
                log.error(e.getMessage());
            }
            try {
                table.close();
            } catch (Exception e) {
                log.error(e.getMessage());
            }
            try {
                con.close();
            } catch (Exception e) {
                log.error(e.getMessage());
            }
        }
        return pager;
    }
}&lt;p class='post-footer'&gt;
        original link:
        &lt;a href='http://findhy.github.io/blog/2014/11/28/hbase-page/'&gt;http://findhy.github.io/blog/2014/11/28/hbase-page/&lt;/a&gt;&lt;br/&gt;
        written by &lt;a href='http://findhy.github.io'&gt;Findhy&lt;/a&gt;
        &amp;nbsp;posted at &lt;a href='http://findhy.github.io'&gt;http://findhy.github.io&lt;/a&gt;
        &lt;/p&gt;
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop集群NameNode运行中进入安全模式的问题：NameNode Low on Available Disk Space]]></title>
    <link href="http://findhy.github.io/blog/2014/11/13/namenode-low-on-available-disk-space/"/>
    <updated>2014-11-13T15:23:26+08:00</updated>
    <id>http://findhy.github.io/blog/2014/11/13/namenode-low-on-available-disk-space</id>
    <content type="html"><![CDATA[<p>问题描述：Hadoop集群在运行过程，早上来的时候发现NameNode自动进入安全模式，导致系统运行的MR任务失败。</p>

<p>最开始怀疑是HDFS的块复制没有达到最低的要求，通过hadoop fsck -blocks查看，Minimally replicated blocks为100%，说明blocks都达到了复制要求，最后查看了NameNode的日志发现几行警告信息：</p>

<pre><code>2014-11-12 21:31:38,478 WARN org.apache.hadoop.hdfs.server.namenode.NameNodeResourceChecker (org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeResourceMonitor@1f1dd4b6): Space available o
n volume 'null' is 13119488, which is below the configured reserved amount 104857600
2014-11-12 21:31:38,478 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem (org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeResourceMonitor@1f1dd4b6): NameNode low on available di
sk space. Entering safe mode.
2014-11-12 21:31:38,478 INFO org.apache.hadoop.hdfs.StateChange (org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeResourceMonitor@1f1dd4b6): STATE* Safe mode is ON. 
Resources are low on NN. Please add or free up more resources then turn off safe mode manually. NOTE:  If you turn off safe mode before adding resources, the NN will immediately return to safe mode
. Use "hdfs dfsadmin -safemode leave" to turn safe mode off.
</code></pre>

<p>通过日志分析发现这样的错误：<code>NameNode low on available disk space. Entering safe mode</code></p>

<p>在网上查到hadoop-jira上面有这样的记录：<a href="https://issues.apache.org/jira/browse/HDFS-4425">https://issues.apache.org/jira/browse/HDFS-4425</a></p>

<p>另外这里还有详细的解释：<a href="https://support.pivotal.io/hc/en-us/articles/201455807-Namenode-logs-reports-Space-available-on-volume-null-is-below-threshold-and-enters-safe-mode">https://support.pivotal.io/hc/en-us/articles/201455807-Namenode-logs-reports-Space-available-on-volume-null-is-below-threshold-and-enters-safe-mode</a></p>

<p>原因是Hadoop的源码里面有一个类 <code>NameNodeResourceChecker</code> 负责检查NameNode的磁盘空间，如果磁盘空间低于100M则进入到安全模式，导致系统不可用，<strong>那么问题来了</strong>，是什么原因导致磁盘空间暴涨了呢？最后发现是HBase的DEBUG日志没有关闭导致日志文件非常大，最终磁盘空间占满，NameNode不可用。</p>

<p class='post-footer'>
            original link:
            <a href='http://findhy.github.io/blog/2014/11/13/namenode-low-on-available-disk-space/'>http://findhy.github.io/blog/2014/11/13/namenode-low-on-available-disk-space/</a><br/>
            written by <a href='http://findhy.github.io'>Findhy</a>
            &nbsp;posted at <a href='http://findhy.github.io'>http://findhy.github.io</a>
            </p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop Streaming编程(Python)]]></title>
    <link href="http://findhy.github.io/blog/2014/07/29/hadoop-streaming-python/"/>
    <updated>2014-07-29T17:17:59+08:00</updated>
    <id>http://findhy.github.io/blog/2014/07/29/hadoop-streaming-python</id>
    <content type="html"><![CDATA[<p>Hadoop Streaming是Hadoop提供的一个工具，可以允许开发者以非Java的编程语言开发Mapper和Reducer，而且在开发过程中，我们只需要专注于数据的输入和输出处理，其它的事情都交给这个工具来做。本例是用Python实现一个简单的Wordcount例子，环境为：</p>

<ul>
<li>Hadoop:CDH4.3.2</li>
<li>Python:2.6.6</li>
<li>OS:Centos 6.5</li>
<li>Hadoop Streaming:hadoop-2.0.0-cdh4.3.2/share/hadoop/tools/lib/hadoop-streaming-2.0.0-cdh4.3.2.jar</li>
</ul>


<!--more-->


<h3>代码实现</h3>

<p>Python实现wordcount的Mapper和Reducer，比较简单直接看代码，<em>mapper.py</em>：</p>

<pre><code>#!/usr/bin/python

import sys
for line in sys.stdin:
    line = line.strip()
    words = line.split()
    for word in words:
        print '%s\t%s' % (word,1)
</code></pre>

<p><em>reducer.py</em>:</p>

<pre><code>#!/usr/bin/python

from operator import itemgetter
import sys

current_word = None
current_count = 0
word = None

# input comes from STDIN
for line in sys.stdin:
    # remove leading and trailing whitespace
    line = line.strip()

    # parse the input we got from mapper.py
    word, count = line.split('\t', 1)

    # convert count (currently a string) to int
    try:
        count = int(count)
    except ValueError:
        # count was not a number, so silently
        # ignore/discard this line
        continue

    # this IF-switch only works because Hadoop sorts map output
    # by key (here: word) before it is passed to the reducer
    if current_word == word:
        current_count += count
    else:
        if current_word:
            # write result to STDOUT
            print '%s\t%s' % (current_word, current_count)
        current_count = count
        current_word = word

# do not forget to output the last word if needed!
if current_word == word:
    print '%s\t%s' % (current_word, current_count)
</code></pre>

<p>这里需要注意第一行是：<em>#!/usr/bin/python</em>，而不是其它例子中的<em>#!/usr/bin/env python</em></p>

<h3>数据准备</h3>

<p>下载下面链接的数据，拷贝到HDFS中</p>

<ul>
<li><a href="http://www.gutenberg.org/ebooks/20417">The Outline of Science, Vol. 1 (of 4) by J. Arthur Thomson</a></li>
<li><a href="http://www.gutenberg.org/ebooks/5000">The Notebooks of Leonardo Da Vinci</a></li>
<li><a href="http://www.gutenberg.org/ebooks/4300">Ulysses by James Joyce</a></li>
</ul>


<h3>任务提交</h3>

<p>这块试了很多次，很多命令参数MR1和MR2不同的，比如必须这么写才可以：<em>-mapper &ldquo;python mapper.py&rdquo;</em>，下面是完整的执行脚本，经过测试没有问题：</p>

<pre><code>hadoop jar /home/hadoop/hadoop-2.0.0-cdh4.3.2/share/hadoop/tools/lib/hadoop-streaming-2.0.0-cdh4.3.2.jar -input /test/data/* -output /test/output  -file ./*.py -mapper "python mapper.py" -reducer  "python reducer.py" -numReduceTasks 2
</code></pre>

<p>完整代码参考这里项目：<a href="https://github.com/findhy/python-example/tree/master/hadoop/wordcount">https://github.com/findhy/python-example/tree/master/hadoop/wordcount</a></p>

<h3>参考</h3>

<p><a href="http://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/">http://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/</a></p>

<p class='post-footer'>
            original link:
            <a href='http://findhy.github.io/blog/2014/07/29/hadoop-streaming-python/'>http://findhy.github.io/blog/2014/07/29/hadoop-streaming-python/</a><br/>
            written by <a href='http://findhy.github.io'>Findhy</a>
            &nbsp;posted at <a href='http://findhy.github.io'>http://findhy.github.io</a>
            </p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[基于云的Hadoop架构]]></title>
    <link href="http://findhy.github.io/blog/2014/07/25/hadoop-cloud/"/>
    <updated>2014-07-25T12:18:55+08:00</updated>
    <id>http://findhy.github.io/blog/2014/07/25/hadoop-cloud</id>
    <content type="html"><![CDATA[<p>今天看了Netflix分享的基于AWS的大数据平台Hadoop架构，感觉这是未来大数据平台的发展模式，将整个数据仓库部署在云端，开发者不需要考虑集群的资源扩展，也不需要接触复杂的底层命令，通过一个简单RESTFul接口就可以提交我们的MapReduce任务，还可以实时看到任务和运行状态。</p>

<!--more-->


<h3>Netflix基于AWS的Hadoop架构介绍</h3>

<p><img src="http://findhy.github.io/images/hadoopcloud1.png"></p>

<p>Netflix的Hadoop云架构是完全基于AWS来搭建，底层存储采用S3，Hadoop组件采用AWS上原生的EMR，PAAS层是自己开发的Genie(目前已经开源)，开发者或者数据分析师通过Genie提供的RESTFul接口来提交和管理任务，由于AWS云带来的弹性扩展的好处，整个集群可以任意横向和纵向扩展，而开发者只需要专注在算法和业务上即可。</p>

<p>Netflix官方博客介绍：<br/>
<a href="http://techblog.netflix.com/2013/01/hadoop-platform-as-service-in-cloud.html">http://techblog.netflix.com/2013/01/hadoop-platform-as-service-in-cloud.html</a></p>

<p>Infoq中文介绍：<br/>
<a href="http://www.infoq.com/cn/news/2013/02/netflix-hadoop-PaaS">http://www.infoq.com/cn/news/2013/02/netflix-hadoop-PaaS</a></p>

<p>Genie介绍PPT：</p>

<iframe src="http://www.slideshare.net/slideshow/embed_code/24063535 " width="595" height="446" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC;border-width:1px 1px 0;margin-bottom:5px" allowfullscreen></iframe>


<p></p>

<p>Genie已经开源：<br/>
<a href="https://github.com/Netflix/genie">https://github.com/Netflix/genie</a></p>

<p>Hadoop on OpenStack介绍PPT：</p>

<iframe src="http://www.slideshare.net/slideshow/embed_code/18948566 " width="595" height="446" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC;border-width:1px 1px 0;margin-bottom:5px" allowfullscreen></iframe>


<p></p>

<p class='post-footer'>
            original link:
            <a href='http://findhy.github.io/blog/2014/07/25/hadoop-cloud/'>http://findhy.github.io/blog/2014/07/25/hadoop-cloud/</a><br/>
            written by <a href='http://findhy.github.io'>Findhy</a>
            &nbsp;posted at <a href='http://findhy.github.io'>http://findhy.github.io</a>
            </p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[S3cmd 多账户配置]]></title>
    <link href="http://findhy.github.io/blog/2014/07/24/s3cmd-multiple-accounts/"/>
    <updated>2014-07-24T15:16:17+08:00</updated>
    <id>http://findhy.github.io/blog/2014/07/24/s3cmd-multiple-accounts</id>
    <content type="html"><![CDATA[<p>关于s3cmd的安装配置参考这篇文章：<a href="http://findhy.com/blog/2014/06/05/s3cmd-config/">s3cmd Configure</a>，本文介绍s3cmd多账户配置。</p>

<!--more-->


<h4>复制配置文件</h4>

<p>s3cmd安装完成之后，在home/user/目录下会生成一个 .s3cfg 配置文件，在复制一个出来</p>

<pre><code>cp .s3cfg .s3ad
</code></pre>

<h4>修改配置文件</h4>

<pre><code>vi .s3ad
将access_key和secret_key修改成对应账户的值
</code></pre>

<h4>测试</h4>

<p>使用 -c 指令指定配置文件</p>

<pre><code>s3cmd -c ~/.s3ad ls s3://log-ad/adserver/ 
</code></pre>

<h4>改进</h4>

<p>会不会觉得每次用 -c 指定配置文件太麻烦了，用 Bash 别名，我们用一个别名来替代 <em>s3cmd -c ~/.s3ad</em></p>

<pre><code>vi ~/.bashrc  
添加
alias s3ad='s3cmd -c ~/.s3ad'  

执行下面命令，让配置文件生效
source .bashrc  

测试
s3ad ls s3://log-ad/adserver/

OK!这样就不用每次指定配置文件了
</code></pre>

<h4>参考</h4>

<p><a href="https://blog.techopsguru.com/2011/12/s3-bucket-copying-with-multiple-accounts.html  ">https://blog.techopsguru.com/2011/12/s3-bucket-copying-with-multiple-accounts.html  </a>
<a href="http://mikesisk.tumblr.com/post/8703449578/s3cmd-and-multiple-accounts  ">http://mikesisk.tumblr.com/post/8703449578/s3cmd-and-multiple-accounts  </a>
<a href="http://mikesisk.com/2011/08/09/s3cmd-with-multiple-aws-accounts/">http://mikesisk.com/2011/08/09/s3cmd-with-multiple-aws-accounts/</a></p>

<p class='post-footer'>
            original link:
            <a href='http://findhy.github.io/blog/2014/07/24/s3cmd-multiple-accounts/'>http://findhy.github.io/blog/2014/07/24/s3cmd-multiple-accounts/</a><br/>
            written by <a href='http://findhy.github.io'>Findhy</a>
            &nbsp;posted at <a href='http://findhy.github.io'>http://findhy.github.io</a>
            </p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Introduction to Item-Based Recommendations With Hadoop]]></title>
    <link href="http://findhy.github.io/blog/2014/07/22/introduction-to-item-based-recommendation-with-hadoop/"/>
    <updated>2014-07-22T14:30:07+08:00</updated>
    <id>http://findhy.github.io/blog/2014/07/22/introduction-to-item-based-recommendation-with-hadoop</id>
    <content type="html"><![CDATA[<p>本文是Mahout官网上的Item-Based CF on Hadoop的例子，原文在这里<a href="https://mahout.apache.org/users/recommender/intro-itembased-hadoop.html">Introduction to Item-Based Recommendations with Hadoop</a>。</p>

<!--more-->


<h3>前言</h3>

<p>Item-Based CF是协调过滤推荐算法的一个分支，是基于物品相似推荐，基于物品的 CF 的原理和基于用户的 CF 类似，只是在计算邻居时采用物品本身，而不是从用户的角度，即基于用户对物品的偏好找到相似的物品，然后根据用户的历史偏好，推荐相似的物品给他。从计算的角度看，就是将所有用户对某个物品的偏好作为一个向量来计算物品之间的相似度，得到物品的相似物品后，根据用户历史的偏好预测当前用户还没有表示偏好的 物品，计算得到一个排序的物品列表作为推荐。下图给出了一个例子，对于物品 A，根据所有用户的历史偏好，喜欢物品 A 的用户都喜欢物品 C，得出物品 A 和物品 C 比较相似，而用户 C 喜欢物品 A，那么可以推断出用户 C 可能也喜欢物品 C。  <br/>
<img src="http://findhy.github.io/images/mahout3.png"></p>

<h3>示例</h3>

<h4>1、准备数据</h4>

<p>数据要求有三个字段：<em>userID, itemID and preference</em>，<em>userID</em>是用户ID，<em>itemID</em>是商品ID或者其它可推荐的物品ID，<em>preference</em>是用户对物品的偏好，可以是购买记录也可以是用户对物品的评分，如果<em>preference</em>没有值，Mahout会默认填1.0，比如我们用户购买商品的数据来进行推荐，这个偏好大家都是一样的，下面是准备的数据，没有填偏好字段：</p>

<pre><code>1,101
1,102
1,103
2,101
2,102
2,103
2,104
3,101
3,104
3,105
3,107
4,101
4,103
4,104
4,106
5,101
5,102
5,103
5,104
5,105
5,106
</code></pre>

<p>生成<em>input.txt</em>，将文件上传到HDFS中</p>

<pre><code>hadoop fs -put input.txt /user/hadoop/mahout/
</code></pre>

<h4>2、执行</h4>

<pre><code>mahout recommenditembased -s SIMILARITY_LOGLIKELIHOOD -i /user/hadoop/mahout/input.txt -o /user/hadoop/mahout/output --numRecommendations 25
</code></pre>

<h4>3、查看结果</h4>

<p><img src="http://findhy.github.io/images/mahout2.png"></p>

<p class='post-footer'>
            original link:
            <a href='http://findhy.github.io/blog/2014/07/22/introduction-to-item-based-recommendation-with-hadoop/'>http://findhy.github.io/blog/2014/07/22/introduction-to-item-based-recommendation-with-hadoop/</a><br/>
            written by <a href='http://findhy.github.io'>Findhy</a>
            &nbsp;posted at <a href='http://findhy.github.io'>http://findhy.github.io</a>
            </p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Mahout Tutorial]]></title>
    <link href="http://findhy.github.io/blog/2014/07/21/mahout-tutorial/"/>
    <updated>2014-07-21T17:47:04+08:00</updated>
    <id>http://findhy.github.io/blog/2014/07/21/mahout-tutorial</id>
    <content type="html"><![CDATA[<p><a href="http://mahout.apache.org/">Mahout</a> 是一个开源的Java实现的可伸缩的机器学习库，它实现了常见的聚类和推荐等机器学习算法，并且可运行在 Hadoop 分布式平台处理上PB的数据，Mahout 社区和文档非常丰富，是目前搭建推荐系统或者其它算法平台的首选方案。</p>

<!--more-->


<h3>目录</h3>

<ul>
<li>一、前言

<ul>
<li>1、Mahout 介绍</li>
<li>2、Mahout 最新动态 &ndash; Goodbye MapReduce</li>
<li>3、算法介绍</li>
</ul>
</li>
<li>二、Mahout 架构介绍</li>
<li>三、Mahout Startup</li>
</ul>


<h3>一、前言</h3>

<h4>1、Mahout介绍</h4>

<p>Mahout 是 Apache Software Foundation (ASF) 开发的一个全新的开源项目，其主要目标是创建一些可伸缩的机器学习算法，供开发人员在 Apache 在许可下免费使用，Mahout项目是由 Apache Lucene社区中对机器学习感兴趣的一些成员发起的，他们希望建立一个可靠、文档翔实、可伸缩的项目，在其中实现一些常见的用于聚类和分类的机器学习算法，该社区最初基于 Ngetal. 的文章 “Map-Reduce for Machine Learning on Multicore”，但此后在发展中又并入了更多广泛的机器学习方法，包括Collaborative Filtering（CF），Dimensionality Reduction，Topic Models等。此外，通过使用 Apache Hadoop 库，Mahout 可以有效地扩展到云中。在Mahout的Recommendation类算法中，主要有User-Based CF，Item-Based CF，ALS，ALS on Implicit Feedback，Weighted MF，SVD++，Parallel SGD等。</p>

<h4>2、Mahout 最新动态 &ndash; Goodbye MapReduce</h4>

<p>在今年4月25日，Mahout官方发布了一则新闻：</p>

<pre><code>25 April 2014 - Goodbye MapReduce
The Mahout community decided to move its codebase onto modern data processing systems that offer a richer programming model and more efficient execution than Hadoop MapReduce. Mahout will therefore reject new MapReduce algorithm implementations from now on. We will however keep our widely used MapReduce algorithms in the codebase and maintain them.
We are building our future implementations on top of a DSL for linear algebraic operations which has been developed over the last months. Programs written in this DSL are automatically optimized and executed in parallel on Apache Spark.
Furthermore, there is an experimental contribution undergoing which aims to integrate the h20 platform into Mahout.
</code></pre>

<p>主要的意思是Mahout社区决定放弃MapReduce数据处理模型，而将代码库迁移到处理速度更快更丰富的数据处理模型，但是原来实现MapReduce算法还可用且继续维护。过去几个月Mahout团队正在开发<a href="https://mahout.apache.org/users/sparkbindings/home.html">DSL for linear algebraic operations</a>，使用DSL模型编写的代码可以在Spark平台中自动优化和并行执行，可以看出Mahout开始向Spark靠近，后面会将它的算法实现更加适应于Spark平台，来达到性能的大幅提升，因为我们知道Spark是基于内存，它本身处理数据的性能要远高于MapReduce，关于这个更多可以参考<a href="https://issues.apache.org/jira/browse/MAHOUT-1500">Mahout-Jira</a>，这里也有关于DSL的解释，我现在还没有太理解清楚：</p>

<pre><code>The second set of DSL is for (looking identically to in-core set of operators) is for distributed stuff. 
(on diagram those two are not visually separated other than there's just part of it over in-core 
and part of it over distributed optimizer).
</code></pre>

<h4>3、算法介绍</h4>

<p>Mahout实现了大多数常用的算法，详细列表可以看<a href="https://mahout.apache.org/users/basics/algorithms.html">Mahout-algorithms</a>，它共有3种实现，第一种是单机的（single machine），受限于单机的性能处理较小量级的数据，第二种分布式实现（MapReduce），算法可以运行在Hadoop分布式平台中，性能可以横向扩展，可以处理上PB的数据，第三种是Spark实现，算法可以跑在Spark平台上，这个也是上面提到的，Mahout开始转向Spark平台，后面更多算法会迁移到Spark平台。下面介绍几个常用的算法。</p>

<h4>3.1、Recommendation</h4>

<p>推荐，给用户找到他想要的物品是目前互联网平台用的最多的算法，包括电商网站的商品推荐，社交平台的好友推荐，内容平台类似豆瓣的电影、图书推荐等等，被用的最多的是协同过滤算法(Collaborative Filtering-based Recommendation)，就是物品或者用户的相似度找到同类物品或用户来进行推荐，协同过滤算法又有三个分支：</p>

<ul>
<li>基于用户的推荐（User-based Recommendation）：发现与当前用户口味偏好相近的用户进行推荐，就是电商网站的：购买了此物品的用户还购买了哪些物品推荐</li>
<li>基于项目的推荐（Item-based Recommendation）：发现与当前用户购买物品相似的物品进行推荐，就是电商网站的：组合购买或者是你还可以购买哪些物品</li>
<li>和基于模型的推荐（Model-based Recommendation）：这个通常是基于历史数据来训练一个模型来进行推荐</li>
</ul>


<h4>3.2、Clustering</h4>

<p>聚类，是把一个数据对象划分成子集的过程，每个子集是一个簇（cluster），簇内的对象彼此相似，但与其它簇中的对象不相似，它通常用于商务管理中的客户分类，为不同的客户群制定不同的营销策略，还有离群点监测，就是那些远离任何簇的值，常用在金融交易中的信用卡欺诈校验。聚类作为统计学的一个分支，已经被广泛研究多年，主要集中在基于距离的聚类分析，基于K-均值（K-means）、K-中性点（K-medoids）。</p>

<h3>二、Mahout 架构介绍</h3>

<p><img src="http://findhy.github.io/images/mahout1.png"><br/>
这个Mahout整个业务架构图，最下面是Mahout的底层数学模型库和数据存储，Mahout可以从HDFS中读取数据，中间是它的算法实现，上面是业务层。</p>

<h3>三、Mahout Startup</h3>

<p>这里构建基于Mahout的maven项目，和一个算法实例：User-based Recommendation。</p>

<p>1、POM文件加入依赖</p>

<pre><code>    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.mahout&lt;/groupId&gt;
        &lt;artifactId&gt;mahout-core&lt;/artifactId&gt;
        &lt;version&gt;0.7&lt;/version&gt;
    &lt;/dependency&gt;
</code></pre>

<p>2、准备源文件</p>

<pre><code>1,3
1,4
2,44
2,46
3,3
3,5
3,6
4,3
4,5
4,11
4,44
5,1
5,2
5,4
</code></pre>

<p>3、示例代码</p>

<pre><code>package com.find.mahout.recommend;

import java.io.File;
import java.io.FileNotFoundException;
import java.io.IOException;
import java.util.List;

import org.apache.commons.cli2.OptionException;
import org.apache.mahout.cf.taste.common.TasteException;
import org.apache.mahout.cf.taste.impl.common.LongPrimitiveIterator;
import org.apache.mahout.cf.taste.impl.model.file.FileDataModel;
import org.apache.mahout.cf.taste.impl.recommender.CachingRecommender;
import org.apache.mahout.cf.taste.impl.recommender.slopeone.SlopeOneRecommender;
import org.apache.mahout.cf.taste.model.DataModel;
import org.apache.mahout.cf.taste.recommender.RecommendedItem;

public class UserCFMahout {

    public static void main(String[] args) throws FileNotFoundException, TasteException, IOException, OptionException {

        // 创建DataModel  slopeOne
        File ratingsFile = new File("E:\\cy-bigdata-project\\mahout-example\\src\\main\\resources\\datafile\\slopeOne.csv");
        DataModel model = new FileDataModel(ratingsFile);

        // 创建Recommender，使用SlopeOneRecommender
        CachingRecommender cachingRecommender = new CachingRecommender(new SlopeOneRecommender(model));

        // for all users
        for (LongPrimitiveIterator it = model.getUserIDs(); it.hasNext();) {
            long userId = it.nextLong();

            // get the recommendations for the user
            List&lt;RecommendedItem&gt; recommendations = cachingRecommender.recommend(userId, 10);

            // if empty write something
            if (recommendations.size() == 0) {
                System.out.print("User ");
                System.out.print(userId);
                System.out.println(": no recommendations");
            }

            // print the list of recommendations for each
            for (RecommendedItem recommendedItem : recommendations) {
                System.out.print("User ");
                System.out.print(userId);
                System.out.print(": ");
                System.out.println(recommendedItem);
            }
        }
    }
}
</code></pre>

<p>4、测试结果</p>

<pre><code>User 1: RecommendedItem[item:5, value:1.0]
User 2: RecommendedItem[item:5, value:1.0]
User 2: RecommendedItem[item:3, value:1.0]
User 3: no recommendations
User 4: no recommendations
User 5: RecommendedItem[item:5, value:1.0]
User 5: RecommendedItem[item:3, value:1.0]
</code></pre>

<p>更多例子请参考这个项目：<a href="https://github.com/findhy/mahout-example">https://github.com/findhy/mahout-example</a></p>

<p class='post-footer'>
            original link:
            <a href='http://findhy.github.io/blog/2014/07/21/mahout-tutorial/'>http://findhy.github.io/blog/2014/07/21/mahout-tutorial/</a><br/>
            written by <a href='http://findhy.github.io'>Findhy</a>
            &nbsp;posted at <a href='http://findhy.github.io'>http://findhy.github.io</a>
            </p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[手机应用商店的商业形态以及个性化推荐]]></title>
    <link href="http://findhy.github.io/blog/2014/07/18/app-store-business-and-recommend/"/>
    <updated>2014-07-18T11:53:49+08:00</updated>
    <id>http://findhy.github.io/blog/2014/07/18/app-store-business-and-recommend</id>
    <content type="html"><![CDATA[<p>最近同时在参与两件事：手机应用商店的商业化之路以及应用的个性化推荐，一点想法记录下来，手机应用商店市场已经泛滥，既有官方出品（Google Play），又有巨头入场（应用宝、360、91），还有少年得志生机勃勃的豌豆荚，作为应用商店怎么在竞争如此激烈的市场中抢占一席之地呢？</p>

<!--more-->


<h4>生态环境</h4>

<p>在手机应用商店的生态环境中，有三个主要参与者：</p>

<ul>
<li>开发者或者叫商家</li>
<li>应用商店</li>
<li>用户</li>
</ul>


<h4>盈利方式</h4>

<p>我们可以从结果为导向来先看下，应用商店想盈利，必须从用户和开发者身上想办法，从用户：</p>

<ul>
<li>广告：用户点击收费</li>
<li>付费内容：应用或者其它资源</li>
</ul>


<p>从开发者：</p>

<ul>
<li>广告：为开发者推广应用收费</li>
<li>联运：合作运营，收入分成</li>
<li>支付：为开发者提供支付，通过账期和手续费来盈利</li>
</ul>


<p>后两个都需要其它资源辅助才能做，暂且不谈，所以还是又回到了互联网最基础的盈利方式：广告。</p>

<p>但是应用商店的广告会有一个平衡问题：</p>

<ul>
<li>应用商店最核心的内容就是要提供用户想要的应用，所以需要把用户最需要的应用放在最好的位置</li>
<li>应用商店想盈利，就必须把最好的位置留给广告主</li>
</ul>


<p>怎么将广告与用户的强需求完美的结合？这个是应用商店需要思考的问题，如果做得不好，用户找不到自己想要的应用，可能以后就不会再来了，但如果只是纯粹推热门的、用户感兴趣的，又无法盈利。这又回到本文的标题：个性化推荐。</p>

<h4>个性化推荐</h4>

<p>如果应用商店做到：推荐的广告就是用户想要的应用呢？首先我们需要非常了解我们的用户，积累用户行为数据非常重要：</p>

<ul>
<li>评论体系</li>
<li>评分体系</li>
<li>下载</li>
<li>搜索</li>
</ul>


<p>通过这些来分析用户的兴趣模型，再加上推荐算法(CF)达到给用户看的广告就是他感兴趣的，而且这个阀值也很重要，如果没有找到用户感兴趣的广告，宁愿不推荐也不要放一些不相关的广告来导致用户流失，所以整体算法需要同时考虑广告收益和用户体验，广告推荐要和应用推荐结合使用。</p>

<h4>总结</h4>

<ul>
<li>应用商店想盈利留住用户是关键，没有用户其它都是扯</li>
<li>所以在广告和用户体验上，广告必须要为用户体验让步</li>
<li>在应用商店产品层面，要做更多的附加内容来吸引用户(社交、线下活动、论坛、O2O)，形成一个圈子，单纯一个应用商店，产品形态还是太小了</li>
<li>有了用户，有了用户交互，积累了数据，盈利也会比较容易，也会衍生出其它产品形态(卖数据服务来指导游戏厂商哪些游戏受用户喜爱)</li>
</ul>


<p class='post-footer'>
            original link:
            <a href='http://findhy.github.io/blog/2014/07/18/app-store-business-and-recommend/'>http://findhy.github.io/blog/2014/07/18/app-store-business-and-recommend/</a><br/>
            written by <a href='http://findhy.github.io'>Findhy</a>
            &nbsp;posted at <a href='http://findhy.github.io'>http://findhy.github.io</a>
            </p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Zookeeper运维管理工具介绍]]></title>
    <link href="http://findhy.github.io/blog/2014/07/09/zookeeper-manager/"/>
    <updated>2014-07-09T13:39:26+08:00</updated>
    <id>http://findhy.github.io/blog/2014/07/09/zookeeper-manager</id>
    <content type="html"><![CDATA[<p><a href="http://zookeeper.apache.org/">Zookeeper</a>是一个提供维护配置信息、命名空间、分布式同步和组服务的集中式管理工具，广泛应用与分布式系统中，像Storm、HBase、Dubbo都依赖Zookeeper，所以怎么运维和管理Zookeeper就非常重要了，下面介绍几个Zookeeper的管理工具，有些我用过有些我没有用过，仅供参考。</p>

<!--more-->


<h3>1.Taokeeper</h3>

<p>阿里开源的Zookeeper监控工具，可以用来监控Zookeeper集群的工作状态、连接数、注册的Watcher数和磁盘/内存/CPU状态，还可以设置预警，当系统性能达到某个阀值的时候通知管理员，这块提供自定义开发接口。</p>

<p>项目地址：<a href="https://github.com/alibaba/taokeeper">https://github.com/alibaba/taokeeper</a></p>

<p>安装说明：<a href="http://jm-blog.aliapp.com/?p=1450">http://jm-blog.aliapp.com/?p=1450</a></p>

<p>注意如果启动完看不到机器的磁盘和CPU等状态，可能是服务器没有安装nc，执行 yum install -y nc</p>

<h3>2.ZooInspector</h3>

<p><a href="http://www.taobaotesting.com/blogs/qa?bid=15305">ZooInspector</a>可以用来查询Zookeeper里面的具体信息，还可以与Eclipse集成可视化，这样我们在开发过程中就能够实时看到Zookeeper中的数据。</p>

<h3>3.node-zk-browser</h3>

<p><a href="https://github.com/killme2008/node-zk-browser">node-zk-browser</a>和ZooInspector功能相似，但是基于浏览器访问的，底层是用Node.js实现。</p>

<p class='post-footer'>
            original link:
            <a href='http://findhy.github.io/blog/2014/07/09/zookeeper-manager/'>http://findhy.github.io/blog/2014/07/09/zookeeper-manager/</a><br/>
            written by <a href='http://findhy.github.io'>Findhy</a>
            &nbsp;posted at <a href='http://findhy.github.io'>http://findhy.github.io</a>
            </p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Spark1.0在YARN上部署过程]]></title>
    <link href="http://findhy.github.io/blog/2014/06/28/spark-on-yarn/"/>
    <updated>2014-06-28T14:13:34+08:00</updated>
    <id>http://findhy.github.io/blog/2014/06/28/spark-on-yarn</id>
    <content type="html"><![CDATA[<p>Spark支持独立部署和在YARN上部署，选择在YARN上就是作为一个APP跑在YARN平台上，这样的好处是可以和原来的数据平台无缝结合。</p>

<!--more-->


<h3>安装过程</h3>

<p>1、下载</p>

<pre><code>wget https://github.com/apache/spark/archive/v1.0.0.zip
</code></pre>

<p>2、解压</p>

<pre><code>unzip v1.0.0.zip
</code></pre>

<p>3、进入目录</p>

<pre><code>cd spark-1.0.0
</code></pre>

<p>4、设置maven内存参数：</p>

<pre><code>export MAVEN_OPTS="-Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m"
</code></pre>

<p>5、打包</p>

<pre><code>mvn -Pyarn-alpha -Dhadoop.version=2.0.0-cdh4.3.2 -DskipTests clean package
</code></pre>

<p>最后编译报错：</p>

<pre><code>Failed to execute goal org.apache.maven.plugins:maven-antrun-plugin:1.7:run (default) on project spark-core_2.10: An Ant BuildException has occured: Please set the SCALA_HOME (or SCALA_LIBRARY_PATH if scala is on the path) environment variables and retry.
[ERROR] around Ant part ...&lt;fail message="Please set the SCALA_HOME (or SCALA_LIBRARY_PATH if scala is on the path) environment variables and retry."&gt;... @ 6:126 in /home/hadoop/spark-cdh4.3.2/spark-1.0.0/core/target/antrun/build-main.xml
</code></pre>

<p>缺失scala的依赖</p>

<p>查看pom.xml</p>

<pre><code>&lt;scala.version&gt;2.10.4&lt;/scala.version&gt;
</code></pre>

<p>6、安装scala2.10.4</p>

<pre><code>wget http://www.scala-lang.org/files/archive/scala-2.10.4.tgz

tar -zxvf scala-2.10.4.tgz

export SCALA_HOME=/opt/scala/scala-2.10.4
</code></pre>

<p>7、再次打包</p>

<pre><code>mvn -Pyarn-alpha -Dhadoop.version=2.0.0-cdh4.3.2 -DskipTests clean package
</code></pre>

<p>这个打包过程需要很久，看到BUILD SUCCESS就成功了</p>

<p>8、打spark内核包</p>

<pre><code>SPARK_HADOOP_VERSION=2.0.0-cdh4.3.2 SPARK_YARN=true sbt/sbt assembly
</code></pre>

<p>9、执行完成之后，在YARN上发布APP</p>

<pre><code>./bin/spark-submit --class org.apache.spark.examples.SparkTC --master yarn-cluster --num-executors 3 --executor-memory 2g --driver-memory 4g --executor-cores 1 --jars /home/hadoop/spark-cdh4.3.2/spark-1.0.0/examples/target/spark-examples*.jar
</code></pre>

<p>看到命令行显示<br/>
<img src="http://findhy.github.io/images/spark-1.png"></p>

<p>YARN管理台显示<br/>
<img src="http://findhy.github.io/images/spark-2.png"></p>

<p>点击yarn app的tracking UI到达Spark的管理界面<br/>
<img src="http://findhy.github.io/images/spark-3.png"></p>

<p class='post-footer'>
            original link:
            <a href='http://findhy.github.io/blog/2014/06/28/spark-on-yarn/'>http://findhy.github.io/blog/2014/06/28/spark-on-yarn/</a><br/>
            written by <a href='http://findhy.github.io'>Findhy</a>
            &nbsp;posted at <a href='http://findhy.github.io'>http://findhy.github.io</a>
            </p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Titan Visualization With VivaGraphJS]]></title>
    <link href="http://findhy.github.io/blog/2014/06/23/titan-visualization-with-vivagraphjs/"/>
    <updated>2014-06-23T16:09:23+08:00</updated>
    <id>http://findhy.github.io/blog/2014/06/23/titan-visualization-with-vivagraphjs</id>
    <content type="html"><![CDATA[<p><a href="http://findhy.com/blog/2014/06/21/graph-database-visualization/">上一篇文章</a>介绍了Graph Database的可视化技术方案，本项目是Titan的数据可视化和在线更新，前端使用的是<a href="https://github.com/anvaka/VivaGraphJS">VivaGraphJS</a>，在线更新就是调用Rexster提供的REST API，具体可以参考github上面的README介绍：<a href="https://github.com/titan-cn/titan-vivagraph">titan-vivagraph</a>。</p>

<!--more-->




<p class='post-footer'>
            original link:
            <a href='http://findhy.github.io/blog/2014/06/23/titan-visualization-with-vivagraphjs/'>http://findhy.github.io/blog/2014/06/23/titan-visualization-with-vivagraphjs/</a><br/>
            written by <a href='http://findhy.github.io'>Findhy</a>
            &nbsp;posted at <a href='http://findhy.github.io'>http://findhy.github.io</a>
            </p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[存储性能瓶颈的成因、定位与排查]]></title>
    <link href="http://findhy.github.io/blog/2014/06/23/storage-performance-locate/"/>
    <updated>2014-06-23T10:41:46+08:00</updated>
    <id>http://findhy.github.io/blog/2014/06/23/storage-performance-locate</id>
    <content type="html"><![CDATA[<p>一个系统的性能瓶颈可能发生在很多地方，常见包括：CPU、内存、存储IO、网络、技术，其中存储IO的性能瓶颈最容易发生，往往我们刚开始搭建系统的时候都放在单台服务器上，当业务扩大后，单台已经无法支撑，常见的做法就是，前端加一个F5或者Nginx做负载均衡，后端应用做集群，数据库做集群或者读写分离、分表分库来缓解服务器的存储IO瓶颈，但是遇到存储瓶颈时我们如何定位问题和在架构设计的时候就去避免这样的问题呢，下面转载一篇EMC中文论坛的一篇文章，原文在这里<a href="https://community.emc.com/docs/DOC-34921">存储性能瓶颈的成因、定位与排查</a>。正文：</p>

<!--more-->


<h3>介绍</h3>

<p>企业数据存储性能瓶颈常常会发生在端口，控制器和磁盘，难点在于找出引起拥塞的单元，往往需要应用多重工具以及丰富的经验来查找并解决。
本文详细阐述存储瓶颈发生最常见的四种情况，可能发生的拥塞点，需要监控的参数指标，以及部署存储系统的最佳实践。</p>

<h3>更多信息</h3>

<h4>数据存储瓶颈的四个常见场景：</h4>

<p>以下是储瓶颈发生最常见的四种典型情况：</p>

<ol>
<li><p>当多个用户同时访问某一业务应用，无论是邮件服务器，企业资源规划（ERP）系统或数据库，数据请求会累积在队列中。单个I/O的响应时间开始增长，短暂延时开始转变成为漫长的等待。
这类响应时间敏感型应用的特征是，很多随机请求，读取比写入更多，I/O较小。最好的方法是：将负载分布在多块磁盘上，否则可能造成性能瓶颈。
如果应用增加了更多用户，或应用IOPS请求增加，则可能需要在RAID组中添加更多磁盘，或数据可能需要跨越更多磁盘，在更多层级做条带化。
存储在这样的情况下往往首先被怀疑，但大多数情况下并非存储引发，原因可能在于网络、应用或服务器。</p></li>
<li><p>带宽敏感型应用——如数据备份，视频流或安全登录，这类应用当多个用户同时访问大型文件或数据流时可能造成瓶颈。
定位这一问题存储管理员应当从备份服务器开始一路向下检查至磁盘，原因可能存在于这一通路的任何地方。
问题不一定发生在存储，可能是由于备份应用创建的方式或是磁带系统的工作方式引起的。如果瓶颈定位于存储，那么可能是由于服务I/O的磁盘数量不足，在控制器造成争用，或是阵列前端口带宽不足。
性能调优需要针对不同应用程序负载来完成。针对大型文件和流数据的调优并不适合于小型文件，反之亦然。这也就是为什么在大多数存储系统中往往做一个平衡，需要用户尝试并找出系统的折中。用户通常需要优化吞吐量或IOPS，但并不需要对两者同时优化。</p></li>
<li><p>RAID组中的磁盘故障。特别是在RAID 5中会造成性能的下降，因为系统需要重建校验数据。相比数据读写操作，重建会对性能造成更大影响。
即便坏盘是造成故障的根源，但控制器还是可能成为瓶颈，因为在重建过程中它需要不停地服务数据。当重建完成时，性能才会恢复正常。</p></li>
<li><p>部署了一种新的应用，而卷存在于处理繁忙邮件系统的同一磁盘。如果新的应用变得繁忙，邮件系统性能将会遭受影响。额外的流量最终会将磁盘完全覆盖。</p></li>
</ol>


<h4>存储瓶颈常发区域:</h4>

<h4>存储区域网络（Storage-area network, SAN）/阵列前端口</h4>

<p>存储部署于集中化SAN环境时，需考虑服务器和SAN之间的潜在网络瓶颈。例如，运行多部虚拟机的整合服务器可能不具备支持工作负载要求的足够网络端口。添加网络端口或转移网络密集型工作负载至其他服务器可解决这一问题。如前所述，对于带宽集中型应用，需考虑NFS有多少Fiber Channel 端口, or iSCSI 端口 or Ethernet 端口，需要用户站在带宽的角度来考量整个架构。</p>

<p>可能发生的问题包括：
如果阵列中端口数量不够，就会发生过饱和/过度使用。
虚拟服务器环境下的过量预定
端口间负载不均衡
交换机间链路争用/流量负荷过重
如某一HBA端口负载过重将导致HBA拥塞。使用虚拟机会导致问题更加严重。</p>

<h4>存储控制器</h4>

<p>一个标准的主动——被动或主动——主动控制器都有一个性能极限。接近这条上限取决于用户有多少块磁盘，因为每块磁盘的IOPS和吞吐量是固定的。</p>

<p>可能出现的问题包括：
控制器I/O过饱和，使得从缓存到阵列能够处理的IOPS受到限制
吞吐量“淹没“处理器
CPU过载/处理器功率不足
性能无法跟上SSD</p>

<h4>Cache</h4>

<p>由于服务器内存和CPU远比机械磁盘快得多，需为磁盘添加高速内存以缓存读写数据。例如，写入磁盘的数据存储在缓存中直到磁盘能够跟上，同时磁盘中的读数据放入缓存中直到能被主机读取。Cache比磁盘快1000倍，因此将数据写入和读出Cache对性能影响巨大。智能缓存算法能够预测你需要查找的数据，你是否会对此数据频繁访问，甚至是将访问频繁的随机数据放在缓存中。</p>

<p>可能发生的问题包括：
Cache memory不足
Cache写入过载，引起性能降低
频繁访问顺序性数据引起cache超负荷
Cache中需要持续不断地写入新数据，因此如果cache总是在refill，将无法从cache获益。</p>

<h4>磁盘</h4>

<p>磁盘瓶颈与磁盘转速有关, 慢速磁盘会引入较多延时。存储性能问题的排查首先考虑的因素就是磁盘速度，同时有多少块磁盘可进行并发读写。而另一因素是磁盘接口。采用更快的接口能够缓解磁盘瓶颈，但更重要的是在快速接口与相应更大的缓存大小以及转速之间取得平衡。同样，应避免将快速和慢速磁盘混入同一接口，因为慢速磁盘将会造成快速接口与快速磁盘的性能浪费。</p>

<p>可能引发的问题包括：
过多应用命中磁盘
磁盘数量不足以满足应用所需的IOPS或吞吐量
磁盘速度过慢无法满足性能需求及支持繁重工作负荷
Disk group往往是classic存储架构的潜在性能瓶颈，这种结构下RAID最多配置在16块磁盘。Thin结构通常每个LUN拥有更多磁盘，从而数据分布于更多spindle，因增加的并发性而减少了成为瓶颈的可能。</p>

<h4>需要监控的指标：</h4>

<p>曾经一度存储厂商们强调的是IOPS和吞吐量，但现在重点逐渐转变成为响应时间。也就是说，不是数据移动的速度有多快，而在于对请求的响应速度有多快。</p>

<p>正常情况下，15,000 rpm Fibre Channel磁盘响应时间为4ms，SAS磁盘响应时间约为5ms至6ms，SATA为10ms，而SSD少于1ms。如果发现Fibre Channel磁盘响应时间为12ms，或SSD响应时间变成5ms，那么就说明可能产生了争用，可能芯片发生了故障。</p>

<p>除了响应时间，其他需要监控的指标包括：
队列长度，队列中一次积累的请求数量，平均磁盘队列长度；
平均I/O大小千字节数；
IOPS （读和写，随机和顺序，整体平均IOPS）；
每秒百万字节吞吐量；
读写所占比例；
容量（空闲，使用和保留）。</p>

<h4>数据存储性能最佳实践：</h4>

<p>性能调优和改进的方式有很多种，用户当然可以通过添加磁盘，端口，多核处理器，内存来改善，但问题是：性价比，以及对业务是否实用。本文建议的方式是在预算范围内找寻性能最大化的解决方案。另外一个需要考虑的方面是环境并非一尘不变，系统部署方案要能够适应环境的改变需求。</p>

<p>首先需要考虑刷数据的性能特征，需要了解IO工作情况是怎样的。是否是cache友好型？是否是CPU集中型？业务数据很大数量很少，还是很小但数量很多？另外一方面就是构成存储环境的组件。包括应用，存储系统本身，网络。。。瓶颈可能在哪里，改善哪里最有效？</p>

<p>以下是一些常规建议：
不要仅仅根据空闲空间来分配存储，而需要结合考虑性能需求，确保为吞吐量或IOPS分配足够多的磁盘。
在磁盘间均衡分布应用负载，以减少热点地区的产生。
理解应用负载类型，并针对负载选择匹配的RAID类型。例如，写密集型应用建议使用RAID 1而不是RAID 5。因为当写入RAID 5时，需要计算校验位，需耗费较多时间。而RAID 1，写入两块磁盘速度快得多，无需计算。
磁盘类型（Fibre Channel, SAS, SATA）与期望性能相匹配。对于关键业务应用部署高性能磁盘，例如15,000 rpm Fibre Channel。
对于I/O密集型应用考虑采用SSD，但并不适用于写性能重要型应用。只要没有达到控制器瓶颈，SSD对读性能提升显著，但对写性能提升并没有明显效果。
采用端对端的监控工具，特别是虚拟服务器环境。虚拟端与物理端之间有一道防火墙，所以，需要穿透防火墙进行端到端的监控。
有些性能分析工具涵盖从应用到磁盘，有些仅局限于存储系统本身。由于性能是一个连锁反应包含很多变量，所以需要全面地分析数据。
以数据仅写入磁盘外部扇区的方式格式化磁盘。因减少数据定位时间而在高I/O环境下提升性能。负面作用是相当一部分磁盘容量未能得以使用。</p>

<h3>应用于</h3>

<p>存储性能分析、定位与排查</p>

<p class='post-footer'>
            original link:
            <a href='http://findhy.github.io/blog/2014/06/23/storage-performance-locate/'>http://findhy.github.io/blog/2014/06/23/storage-performance-locate/</a><br/>
            written by <a href='http://findhy.github.io'>Findhy</a>
            &nbsp;posted at <a href='http://findhy.github.io'>http://findhy.github.io</a>
            </p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[图数据库(graph Database)可视化]]></title>
    <link href="http://findhy.github.io/blog/2014/06/21/graph-database-visualization/"/>
    <updated>2014-06-21T11:04:58+08:00</updated>
    <id>http://findhy.github.io/blog/2014/06/21/graph-database-visualization</id>
    <content type="html"><![CDATA[<p>数据可视化是大数据的最后一环，重要性不可忽视，内行往往关心你的算法和架构，但是用户(客户或者领导)只会看最终展现在他们面前的东西，当然业务层面你需要先了解用户的核心需求，再去建模和设计指标，本文讨论关于数据可视化的技术方案，还有比较特别的图数据库的可视化。</p>

<!--more-->


<h3>1.可视化类型</h3>

<p>从用户响应时间的角度，有：</p>

<ul>
<li>静态可视化：当天日志导入到数据平台，晚上跑一些MR任务，再将执行结果存入HBase或者关系型数据库MySql，第二天运营看到结果</li>
<li>实时可视化：实时可视化前提是数据是实时变化的，通常用Storm或者Spark架构实时处理来自客户端或者Kafka的数据，再将处理完的数据导入到后台存储中，然后前端展现实时有两种方式，一种是被动的，就是你要再去刷新一次，这种比较好实现，一种是主动的，服务器端通知客户端，这种就用Websocket来做，如果服务器端用Node.js，更简单直接用socket.io</li>
</ul>


<p>从展现的图类型来看，可能列举不全：</p>

<ul>
<li>线性图：展现趋势</li>
<li>饼图/柱图：展现占比/对比</li>
<li>散点图：展现聚合关系</li>
<li>地理位置可视化：把数据在地图上展现，DataMap、GoogleMap都有接口，一般要求数据里面有地理位置属性（IP地址、城市、国家）</li>
<li>Graph：用Graph展现一个KnowledgeMap，像社交网站的关系图谱，通常用来做用户定位和推荐</li>
</ul>


<h3>2.前端可视化技术</h3>

<p>基于浏览器的开源的：</p>

<ul>
<li>highchart</li>
<li>d3.js</li>
<li>ECharts</li>
<li>google chart</li>
</ul>


<p>基于桌面的开源的：</p>

<ul>
<li><a href="https://gephi.org/">gephi</a></li>
</ul>


<p>上面这些比较常用，而且例子和社区都比较成熟，具体的可以直接去官方看，我们现在普通报表有用highchart和d3.js的，gephi也可以在浏览器端展现。</p>

<h3>3.Graph Database可视化</h3>

<p>关于Graph Database可视化单独拿出来，这块数据来源是像Titan、Neo4j和OrientDB这一类的图数据库，他们都提供基于REST的接口，以JSON数据返回Graph的信息，以Titan为例，在Titan上面可以搭建一个Rexster服务器，它提供很多针对Graph的REST接口，具体有哪些接口可以看这里<a href="https://github.com/tinkerpop/rexster/wiki/Basic-REST-API">Rexster-REST-API</a>，前端通过调用接口获取数据，在前端去构建Graph图，构建的方式可以是Canvas或SVG，关于这两的比较可以看这里<a href="http://msdn.microsoft.com/zh-cn/library/ie/gg193983(v=vs.85).aspx">SVG 与 Canvas：如何选择</a>。常用的技术解决方案有：</p>

<ul>
<li><a href="http://sigmajs.org/">Sigma.js</a>：开源，通用</li>
<li><a href="http://keylines.com/">Keylines</a>：商业方案，官网有针对Titan和Neo4j可视化的例子</li>
<li><a href="https://github.com/anvaka/VivaGraphJS">VivaGraph</a>：开源，通用，社区没有Sigma.js丰富</li>
<li><a href="https://github.com/anvaka/ngraph">ngraph</a>：VivaGraph的下一个版本，使用WebGL支持3d效果展现</li>
<li><a href="http://d3js.org/">D3.js</a>：开源，通用，上面提到了，它也提供Graph可视化的功能</li>
<li><a href="https://gephi.org/">Gephi</a>：开源，通用，很强大的基于桌面可视化解决方案，通过插件也可以在浏览器端展现</li>
<li><a href="http://linkurio.us/">Linkurious</a>：Neo4j专有的</li>
<li>Neo4J web-admin：Neo4j专有的</li>
</ul>


<p>参考：</p>

<p><a href="http://stackoverflow.com/questions/14867132/is-d3-js-the-right-choice-for-real-time-visualization-of-neo4j-graph-db-data/23522907#23522907">http://stackoverflow.com/questions/14867132/is-d3-js-the-right-choice-for-real-time-visualization-of-neo4j-graph-db-data/23522907#23522907</a></p>

<p><a href="http://stackoverflow.com/questions/18571685/neo4j-graph-visualizing-libraries?rq=1">http://stackoverflow.com/questions/18571685/neo4j-graph-visualizing-libraries?rq=1</a></p>

<p class='post-footer'>
            original link:
            <a href='http://findhy.github.io/blog/2014/06/21/graph-database-visualization/'>http://findhy.github.io/blog/2014/06/21/graph-database-visualization/</a><br/>
            written by <a href='http://findhy.github.io'>Findhy</a>
            &nbsp;posted at <a href='http://findhy.github.io'>http://findhy.github.io</a>
            </p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Titan Tutorial]]></title>
    <link href="http://findhy.github.io/blog/2014/06/19/titan-tutorial/"/>
    <updated>2014-06-19T13:43:40+08:00</updated>
    <id>http://findhy.github.io/blog/2014/06/19/titan-tutorial</id>
    <content type="html"><![CDATA[<p>Titan的<a href="https://github.com/thinkaurelius/titan/wiki">官方手册</a>内容更加丰富，但是太多，初学者不知如何下手，本文摘取重点部分，希望能快速上手Titan。</p>

<!--more-->


<h3>1.版本说明</h3>

<pre><code>Titan：titan-server-0.4.4
HBase：hbase-0.94.6-cdh4.3.2
Elasticsearch：elasticsearch-0.90.3
</code></pre>

<h3>2.环境说明</h3>

<p>服务器3台：</p>

<pre><code>master 10.0.1.252
slave1 10.0.1.253
slave2 10.0.1.254
</code></pre>

<p>HBase搭建的是集群，一个master，两个slave；Elasticsearch在master上部署的单机版本；Titan在master上部署的单机版本。本文不包括HBase集群搭建过程。</p>

<h3>3.Elasticsearch安装</h3>

<p>由于Titan0.4.4版本只能支持Elasticsearch的版本是0.90.3，看这里<a href="https://github.com/thinkaurelius/titan/wiki/Version-Compatibility">Version-Compatibility</a>。所以这里注意版本，Elasticsearch 0.90.3的文档可以看这里<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/0.90/index.html">Elasticsearch-doc</a>。下面开始安装。</p>

<pre><code>wget https://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-0.90.3.tar.gz
tar –zxvf elasticsearch-0.90.3.tar.gz
cd elasticsearch-0.90.3

启动elasticsearch：
./bin/elasticsearch
执行jps，会看到多了一个ElasticSearch的进程，说明成功
</code></pre>

<h3>4.Titan安装</h3>

<p>Titan有<a href="https://github.com/thinkaurelius/titan/wiki/Downloads">多种版本</a>提供下载，这里选择titan-server-0.4.4。</p>

<pre><code>mkdir /home/hadoop/titan-cdh4.3.2
进入
cd titan-cdh4.3.2
下载
wget http://s3.thinkaurelius.com/downloads/titan/titan-server-0.4.4.zip
解压
unzip titan-server-0.4.4.zip
进入目录
cd titan-server-0.4.4
</code></pre>

<p>修改配置文件</p>

<pre><code>vi ./conf/titan-hbase-es.properties

storage.hostname=master,slave1,slave2
storage.port=2181
cache.db-cache = true
cache.db-cache-clean-wait = 20
cache.db-cache-time = 180000
cache.db-cache-size = 0.5

storage.index.search.backend=elasticsearch
storage.index.search.hostname=master
storage.index.search.client-only=true
</code></pre>

<p>初始化Titan与HBase</p>

<pre><code>cd /home/hadoop/titan-cdh4.3.2/titan-server-0.4.4/
./bin/gremlin.sh

gremlin&gt;g = TitanFactory.open('conf/titan-hbase-es.properties')
</code></pre>

<p>这时候到hbase shell下面执行list命令，可以看到多了一张titan的表，执行describe &lsquo;titan&#8217;可以看到titan的表结构，加载数据：</p>

<pre><code>gremlin&gt; GraphOfTheGodsFactory.load(g)
</code></pre>

<p>到hbase shell下面执行scan &lsquo;titan&#8217;可以看到初始化了一些数据，下面用gremlin命令行验证一下这些数据</p>

<pre><code>gremlin&gt; saturn = g.V('name','saturn').next()
==&gt;v[4]
gremlin&gt; saturn.map()
==&gt;name=saturn
==&gt;age=10000
==&gt;type=titan
gremlin&gt; saturn.in('father').in('father').name
==&gt;hercules
</code></pre>

<p>如果输出一致则验证成功</p>

<h3>5.Rexster配置</h3>

<p>这部分文档参考：<a href="https://github.com/thinkaurelius/titan/wiki/Rexster-Graph-Server">https://github.com/thinkaurelius/titan/wiki/Rexster-Graph-Server</a></p>

<p>修改rexster配置文件</p>

<pre><code>cd /home/hadoop/titan-cdh4.3.2/titan-server-0.4.4/conf
cp rexster-cassandra-es.xml rexster-hbase-es.xml
vi rexster-hbase-es.xml
</code></pre>

<p>有两个地方要改，一个是http这个标签，一个是graphs这个标签，黄色是需要修改的内容，第一个修改如下：</p>

<pre><code>&lt;http&gt;
  &lt;server-port&gt;8182&lt;/rexster-server-port&gt;
  &lt;base-uri&gt;http://54.255.164.52&lt;/base-uri&gt;
  &lt;web-root&gt;public&lt;/web-root&gt;
  &lt;character-set&gt;UTF-8&lt;/character-set&gt;
  ...
&lt;/http&gt;
</code></pre>

<p>第二个修改如下：</p>

<pre><code>&lt;graphs&gt;
    &lt;graph&gt;
        &lt;graph-name&gt;graph&lt;/graph-name&gt;
       &lt;graph-type&gt;com.thinkaurelius.titan.tinkerpop.rexster.TitanGraphConfiguration&lt;/graph-type&gt;
        &lt;!-- &lt;graph-location&gt;/tmp/titan&lt;/graph-location&gt; --&gt;
        &lt;graph-read-only&gt;false&lt;/graph-read-only&gt;
        &lt;properties&gt;
            &lt;storage.backend&gt;hbase&lt;/storage.backend&gt;
            &lt;storage.hostname&gt;master,slave1,slave2&lt;/storage.hostname&gt;
            &lt;storage.index.search.backend&gt;elasticsearch&lt;/storage.index.search.backend&gt;
            &lt;storage.index.search.hostname&gt;master&lt;/storage.index.search.hostname&gt;
            &lt;!--&lt;storage.index.search.directory&gt;../db/es&lt;/storage.index.search.directory&gt;--&gt;
            &lt;storage.index.search.client-only&gt;false&lt;/storage.index.search.client-only&gt;
            &lt;storage.index.search.local-mode&gt;false&lt;/storage.index.search.local-mode&gt;
        &lt;/properties&gt;
        &lt;extensions&gt;
          &lt;allows&gt;
            &lt;allow&gt;tp:gremlin&lt;/allow&gt;
          &lt;/allows&gt;
        &lt;/extensions&gt;
    &lt;/graph&gt;
&lt;/graphs&gt;
</code></pre>

<p>启动Rexster</p>

<pre><code>cd /home/hadoop/titan-cdh4.3.2/titan-server-0.4.4
./bin/rexster.sh –s –c ../conf/rexster-hbase-es.xml
</code></pre>

<p>访问<a href="http://master-ip:8182/">http://master-ip:8182/</a></p>

<p>出现下面画面则启动成功</p>

<p><img src="http://findhy.github.io/images/titan-tul-1.png"></p>

<p><a href="https://github.com/tinkerpop/rexster/wiki">Rexster</a>是建立在任何实现了Blueprints的图数据库(Graph Database)之上的web server，它提供这三种功能：</p>

<ul>
<li>提供基于REST的接口方法：GET, POST, PUT, and DELETE，去操作Graph Database

<ul>
<li>基于上面的例子，在浏览器输入：<a href="http://master-ip:8182/graphs/graph/edges">http://master-ip:8182/graphs/graph/edges</a>  会返回graph的edge信息</li>
</ul>
</li>
<li><a href="https://github.com/tinkerpop/rexster/wiki/The-Dog-House">The Dog House</a>提供基于浏览器去操作Graph，还有可视化Graph，界面如下：

<ul>
<li><img src="http://findhy.github.io/images/titan-tul-2.png"></li>
<li><img src="http://findhy.github.io/images/titan-tul-3.png"></li>
</ul>
</li>
<li>提供<a href="https://github.com/tinkerpop/rexster/wiki/RexPro-Java">RexsterClient</a>客户端去访问Rexster server，包括执行一些Graph的操作</li>
</ul>


<p class='post-footer'>
            original link:
            <a href='http://findhy.github.io/blog/2014/06/19/titan-tutorial/'>http://findhy.github.io/blog/2014/06/19/titan-tutorial/</a><br/>
            written by <a href='http://findhy.github.io'>Findhy</a>
            &nbsp;posted at <a href='http://findhy.github.io'>http://findhy.github.io</a>
            </p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Titan经典入门PPT]]></title>
    <link href="http://findhy.github.io/blog/2014/06/19/titan-classic-get-started/"/>
    <updated>2014-06-19T11:00:50+08:00</updated>
    <id>http://findhy.github.io/blog/2014/06/19/titan-classic-get-started</id>
    <content type="html"><![CDATA[<p>下面是Marko Rodriguez分享的一个PPT，原文在这里<a href="http://www.slideshare.net/slidarko/titan-the-rise-of-big-graph-data">slideshare.</a>，该PPT深入浅出，从Graph基础知识到Graph Database到Titan的优势，还包括基本的入门操作，我觉得非常经典，很适合初学者，在此分享，因为原文PPT表达很简单而且加上动画展示，所以即使英文不好的人看也没有障碍。</p>

<!--more-->




<iframe src="http://www.slideshare.net/slideshow/embed_code/13328271 " width="595" height="446" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC;border-width:1px 1px 0;margin-bottom:5px" allowfullscreen></iframe>


<p></p>

<p class='post-footer'>
            original link:
            <a href='http://findhy.github.io/blog/2014/06/19/titan-classic-get-started/'>http://findhy.github.io/blog/2014/06/19/titan-classic-get-started/</a><br/>
            written by <a href='http://findhy.github.io'>Findhy</a>
            &nbsp;posted at <a href='http://findhy.github.io'>http://findhy.github.io</a>
            </p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[怎么在Octopress Blog中嵌入slideshare]]></title>
    <link href="http://findhy.github.io/blog/2014/06/19/how-to-put-slideshare-on-octopress-blog/"/>
    <updated>2014-06-19T10:56:50+08:00</updated>
    <id>http://findhy.github.io/blog/2014/06/19/how-to-put-slideshare-on-octopress-blog</id>
    <content type="html"><![CDATA[<p>怎么在Octopress blog中嵌入slideshar，使用该插件<a href="https://github.com/petehamilton/Octopress-Slideshare-Plugin">Octopress-Slideshare-Plugin</a>，下面是操作过程。</p>

<!--more-->


<h3>1.下载插件</h3>

<pre><code>git clone https://github.com/petehamilton/Octopress-Slideshare-Plugin.git
</code></pre>

<h3>2.安装插件</h3>

<p>进入上面下载的插件目录，拷贝Octopress-Slideshare-Plugin/slideshare.rb到octopress/plugins目录下面</p>

<h3>3.生成对应的Slideshare Embed ID</h3>

<p>到<a href="http://www.slideshare.net/">slideshare</a>网站找到你需要嵌入的ppt。</p>

<p><img src="http://findhy.github.io/images/slideshare-1.png"></p>

<p>点击Embed，生成类似下面的链接，最后的291600就是我们要的Slideshare Embed ID</p>

<pre><code>http://www.slideshare.net/slideshow/embed_code/291600
</code></pre>

<h3>4.嵌入</h3>

<p>在你的bolg MD文件中添加下面的代码：</p>

<p><img src="http://findhy.github.io/images/slideshare-2.png"></p>

<p>执行</p>

<pre><code>rake generate
rake preview
</code></pre>

<p>看到</p>

<iframe src="http://www.slideshare.net/slideshow/embed_code/291600 " width="595" height="446" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC;border-width:1px 1px 0;margin-bottom:5px" allowfullscreen></iframe>


<p></p>

<p class='post-footer'>
            original link:
            <a href='http://findhy.github.io/blog/2014/06/19/how-to-put-slideshare-on-octopress-blog/'>http://findhy.github.io/blog/2014/06/19/how-to-put-slideshare-on-octopress-blog/</a><br/>
            written by <a href='http://findhy.github.io'>Findhy</a>
            &nbsp;posted at <a href='http://findhy.github.io'>http://findhy.github.io</a>
            </p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[TinkerPop]]></title>
    <link href="http://findhy.github.io/blog/2014/06/18/tinkerpop/"/>
    <updated>2014-06-18T15:09:59+08:00</updated>
    <id>http://findhy.github.io/blog/2014/06/18/tinkerpop</id>
    <content type="html"><![CDATA[<p><a href="http://www.tinkerpop.com/">TinkerPop</a>是Graph领域的一系列开源工具包的集合。下面分别介绍：</p>

<!--more-->


<h3>Blueprints</h3>

<p>上一篇文章我们讲<a href="http://findhy.com/blog/2014/06/17/graph-database-data-structure/">Graph Database理论知识</a>的时候，提到了图论关于图(Graph)的定义：顶点和边组成的图形，也在后面提到了关于Graph的一系列操作，包括：插入顶点、插入边、获取路径等等。Blueprints是对图这种抽象模型的具体实现，官方定义：<a href="https://github.com/tinkerpop/blueprints/wiki">Blueprints</a>是一系列<a href="https://github.com/tinkerpop/gremlin/wiki/Defining-a-Property-Graph">属性图模型接口(property graph model interface)</a>，那么接下来，什么是属性图模型(property graph model)？满足下面三个条件的图(Graph)被称为属性图(property graphs)：</p>

<ul>
<li>顶点(vertices)和边(edges)可以包含任意多的key/value的属性</li>
<li>方向性，边(edges)具有方向性，可以从一个顶点(vertices)指向另外一个顶点(vertices)</li>
<li>多样性，顶点(vertices)之间的关系边(edges)可以是不同的类型，就是说两个顶点(vertices)可以拥有多种不同类型的边(edges)</li>
</ul>


<p>满足上述三个条件的graph被称为property graphs，下面展现一个property graphs的例子，数据格式可以是<a href="http://graphml.graphdrawing.org/index.html">GraphML</a>或者<a href="https://github.com/tinkerpop/blueprints/wiki/GraphSON-Reader-and-Writer-Library">GraphSON</a>，前者是<a href="https://github.com/tinkerpop/gremlin/blob/master/data/graph-example-1.xml">XML</a>，后者<a href="https://github.com/tinkerpop/gremlin/blob/master/data/graph-example-1.json">JSON</a>，当然JSON会更轻量级。</p>

<p><img src="http://findhy.github.io/images/tinkpop-1.png"></p>

<p>一个property graphs包含下面这些元素</p>

<ul>
<li>一系列顶点(vertices)

<ul>
<li>每一个顶点(vertex)有一个唯一标识</li>
<li>每一个顶点(vertex)有一个或者多个指向其它顶点的边(edge)</li>
<li>每一个顶点(vertex)有一个或者多个指向自己的边(edge)</li>
<li>每一个顶点(vertex)包含了一个或多个由map定义的key/value属性</li>
</ul>
</li>
<li>一系列边(edges)

<ul>
<li>每一个边(edge)有一个唯一标识</li>
<li>每一个边(edge)具有方向性指向一个顶点(vertex)</li>
<li>每一个边(edge)有一个label来标识两个顶点(vertex)之间的关系</li>
<li>每一个边(edge)包含了一个或多个由map定义的key/value属性</li>
</ul>
</li>
</ul>


<p>什么是property graphs搞明白之后，我们再来看Blueprints，Blueprints为属性图模型(property graph data model)提供了一套接口、实现还有测试用例，你可以把它想象成JDBC，JDBC对数据库的操作原语进行了封装和实现，只不过JDBC是用来操作关系型数据库，而Blueprints用来操作Graph Database。现在主流的Graph Database都支持Blueprints，而且在TinkerPop整个软件栈中，Blueprints是最底层的基础，就是其它的工具包都是基于它之上的封装和扩展。怎么使用Blueprints？</p>

<p>maven引入：</p>

<pre><code>&lt;dependency&gt;
   &lt;groupId&gt;com.tinkerpop.blueprints&lt;/groupId&gt;
   &lt;artifactId&gt;blueprints-core&lt;/artifactId&gt;
   &lt;version&gt;2.5.0&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>

<p>样例代码：</p>

<pre><code>Graph graph = new Neo4jGraph("/tmp/my_graph");
Vertex a = graph.addVertex(null);
Vertex b = graph.addVertex(null);
a.setProperty("name","marko");
b.setProperty("name","peter");
Edge e = graph.addEdge(null, a, b, "knows");
e.setProperty("since", 2006);
graph.shutdown();
</code></pre>

<h3>Pipes</h3>

<p><a href="https://github.com/tinkerpop/pipes/wiki">Pipes</a>是一个图数据处理的框架，可以将它理解为管道(Pipe),它最大的好处是管道(Pipe)的输出可以作为其它管道(Pipe)的输入，这样我们就可以实现类似于mapreducer的复杂运算。</p>

<h3>Gremlin</h3>

<p><a href="https://github.com/tinkerpop/gremlin/wiki">Gremlin</a>是一个图遍历语言，可以用Gremlin来实现图的查询、分析和操作，Gremlin只能适用于支持Blueprints的图数据库，支持多种JVM语言：Java 和 Groovy，文档：<a href="http://gremlindocs.com/">GremlinDocs</a>、<a href="http://sql2gremlin.com/">SQL2Gremlin</a>。</p>

<h3>Frames</h3>

<p><a href="https://github.com/tinkerpop/frames/wiki">Frames</a>是一个object-to-graph映射框架</p>

<h3>Furnace</h3>

<p><a href="https://github.com/tinkerpop/furnace/wiki">Furnace</a>是一个Graph算法包</p>

<h3>Rexster</h3>

<p><a href="https://github.com/tinkerpop/rexster/wiki">Rexster</a>是一个Graph Server</p>

<p>TinkerPop的维护人员来自不同的Graph Database产品厂商，像Neo4j、Titan、OrientDB、Bitsy，它在Graph Database领域的地位我理解就像JavaEE里面的Apache。在最新的<a href="https://github.com/tinkerpop/tinkerpop3">TinkerPop3.0</a>版本的时候，TinkerPop将原本分散的各个工具包合并成了一个项目，并且增加了很多特性，Titan0.5版本还不支持TP3，将会在Titan1.0版本时支持，更多的可以看<a href="http://www.tinkerpop.com/docs/current/">TinkerPop3 Story/doc</a>。</p>

<p class='post-footer'>
            original link:
            <a href='http://findhy.github.io/blog/2014/06/18/tinkerpop/'>http://findhy.github.io/blog/2014/06/18/tinkerpop/</a><br/>
            written by <a href='http://findhy.github.io'>Findhy</a>
            &nbsp;posted at <a href='http://findhy.github.io'>http://findhy.github.io</a>
            </p>

]]></content>
  </entry>
  
</feed>
