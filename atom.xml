<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Findhy's Blog]]></title>
  <link href="http://findhy.com/atom.xml" rel="self"/>
  <link href="http://findhy.com/"/>
  <updated>2014-07-25T14:25:59+08:00</updated>
  <id>http://findhy.com/</id>
  <author>
    <name><![CDATA[Findhy]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[基于云的Hadoop架构]]></title>
    <link href="http://findhy.com/blog/2014/07/25/hadoop-cloud/"/>
    <updated>2014-07-25T12:18:55+08:00</updated>
    <id>http://findhy.com/blog/2014/07/25/hadoop-cloud</id>
    <content type="html"><![CDATA[<p>今天看了Netflix分享的基于AWS的大数据平台Hadoop架构，感觉这是未来大数据平台的发展模式，将整个数据仓库部署在云端，开发者不需要考虑集群的资源扩展，也不需要接触复杂的底层命令，通过一个简单RESTFul接口就可以提交我们的MapReduce任务，还可以实时看到任务和运行状态。</p>

<!--more-->


<h3>Netflix基于AWS的Hadoop架构介绍</h3>

<p><img src="http://findhy.com/images/hadoopcloud1.png"></p>

<p>Netflix的Hadoop云架构是完全基于AWS来搭建，底层存储采用S3，Hadoop组件采用AWS上原生的EMR，PAAS层是自己开发的Genie(目前已经开源)，开发者或者数据分析师通过Genie提供的RESTFul接口来提交和管理任务，由于AWS云带来的弹性扩展的好处，整个集群可以任意横向和纵向扩展，而开发者只需要专注在算法和业务上即可。</p>

<p>Netflix官方博客介绍：<br/>
<a href="http://techblog.netflix.com/2013/01/hadoop-platform-as-service-in-cloud.html">http://techblog.netflix.com/2013/01/hadoop-platform-as-service-in-cloud.html</a></p>

<p>Infoq中文介绍：<br/>
<a href="http://www.infoq.com/cn/news/2013/02/netflix-hadoop-PaaS">http://www.infoq.com/cn/news/2013/02/netflix-hadoop-PaaS</a></p>

<p>Genie介绍PPT：</p>

<iframe src="http://www.slideshare.net/slideshow/embed_code/24063535 " width="595" height="446" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC;border-width:1px 1px 0;margin-bottom:5px" allowfullscreen></iframe>


<p></p>

<p>Genie已经开源：<br/>
<a href="https://github.com/Netflix/genie">https://github.com/Netflix/genie</a></p>

<p>Hadoop on OpenStack介绍PPT：</p>

<iframe src="http://www.slideshare.net/slideshow/embed_code/18948566 " width="595" height="446" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC;border-width:1px 1px 0;margin-bottom:5px" allowfullscreen></iframe>


<p></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[S3cmd 多账户配置]]></title>
    <link href="http://findhy.com/blog/2014/07/24/s3cmd-multiple-accounts/"/>
    <updated>2014-07-24T15:16:17+08:00</updated>
    <id>http://findhy.com/blog/2014/07/24/s3cmd-multiple-accounts</id>
    <content type="html"><![CDATA[<p>关于s3cmd的安装配置参考这篇文章：<a href="http://findhy.com/blog/2014/06/05/s3cmd-config/">s3cmd Configure</a>，本文介绍s3cmd多账户配置。</p>

<!--more-->


<h4>复制配置文件</h4>

<p>s3cmd安装完成之后，在home/user/目录下会生成一个 .s3cfg 配置文件，在复制一个出来</p>

<pre><code>cp .s3cfg .s3ad
</code></pre>

<h4>修改配置文件</h4>

<pre><code>vi .s3ad
将access_key和secret_key修改成对应账户的值
</code></pre>

<h4>测试</h4>

<p>使用 -c 指令指定配置文件</p>

<pre><code>s3cmd -c ~/.s3ad ls s3://log-ad/adserver/ 
</code></pre>

<h4>改进</h4>

<p>会不会觉得每次用 -c 指定配置文件太麻烦了，用 Bash 别名，我们用一个别名来替代 <em>s3cmd -c ~/.s3ad</em></p>

<pre><code>vi ~/.bashrc  
添加
alias s3ad='s3cmd -c ~/.s3ad'  

执行下面命令，让配置文件生效
source .bashrc  

测试
s3ad ls s3://log-ad/adserver/

OK!这样就不用每次指定配置文件了
</code></pre>

<h4>参考</h4>

<p><a href="https://blog.techopsguru.com/2011/12/s3-bucket-copying-with-multiple-accounts.html  ">https://blog.techopsguru.com/2011/12/s3-bucket-copying-with-multiple-accounts.html  </a>
<a href="http://mikesisk.tumblr.com/post/8703449578/s3cmd-and-multiple-accounts  ">http://mikesisk.tumblr.com/post/8703449578/s3cmd-and-multiple-accounts  </a>
<a href="http://mikesisk.com/2011/08/09/s3cmd-with-multiple-aws-accounts/">http://mikesisk.com/2011/08/09/s3cmd-with-multiple-aws-accounts/</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Introduction to Item-Based Recommendations With Hadoop]]></title>
    <link href="http://findhy.com/blog/2014/07/22/introduction-to-item-based-recommendation-with-hadoop/"/>
    <updated>2014-07-22T14:30:07+08:00</updated>
    <id>http://findhy.com/blog/2014/07/22/introduction-to-item-based-recommendation-with-hadoop</id>
    <content type="html"><![CDATA[<p>本文是Mahout官网上的Item-Based CF on Hadoop的例子，原文在这里<a href="https://mahout.apache.org/users/recommender/intro-itembased-hadoop.html">Introduction to Item-Based Recommendations with Hadoop</a>。</p>

<!--more-->


<h3>前言</h3>

<p>Item-Based CF是协调过滤推荐算法的一个分支，是基于物品相似推荐，基于物品的 CF 的原理和基于用户的 CF 类似，只是在计算邻居时采用物品本身，而不是从用户的角度，即基于用户对物品的偏好找到相似的物品，然后根据用户的历史偏好，推荐相似的物品给他。从计算的角度看，就是将所有用户对某个物品的偏好作为一个向量来计算物品之间的相似度，得到物品的相似物品后，根据用户历史的偏好预测当前用户还没有表示偏好的 物品，计算得到一个排序的物品列表作为推荐。下图给出了一个例子，对于物品 A，根据所有用户的历史偏好，喜欢物品 A 的用户都喜欢物品 C，得出物品 A 和物品 C 比较相似，而用户 C 喜欢物品 A，那么可以推断出用户 C 可能也喜欢物品 C。  <br/>
<img src="http://findhy.com/images/mahout3.png"></p>

<h3>示例</h3>

<h4>1、准备数据</h4>

<p>数据要求有三个字段：<em>userID, itemID and preference</em>，<em>userID</em>是用户ID，<em>itemID</em>是商品ID或者其它可推荐的物品ID，<em>preference</em>是用户对物品的偏好，可以是购买记录也可以是用户对物品的评分，如果<em>preference</em>没有值，Mahout会默认填1.0，比如我们用户购买商品的数据来进行推荐，这个偏好大家都是一样的，下面是准备的数据，没有填偏好字段：</p>

<pre><code>1,101
1,102
1,103
2,101
2,102
2,103
2,104
3,101
3,104
3,105
3,107
4,101
4,103
4,104
4,106
5,101
5,102
5,103
5,104
5,105
5,106
</code></pre>

<p>生成<em>input.txt</em>，将文件上传到HDFS中</p>

<pre><code>hadoop fs -put input.txt /user/hadoop/mahout/
</code></pre>

<h4>2、执行</h4>

<pre><code>mahout recommenditembased -s SIMILARITY_LOGLIKELIHOOD -i /user/hadoop/mahout/input.txt -o /user/hadoop/mahout/output --numRecommendations 25
</code></pre>

<h4>3、查看结果</h4>

<p><img src="http://findhy.com/images/mahout2.png"></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Mahout Tutorial]]></title>
    <link href="http://findhy.com/blog/2014/07/21/mahout-tutorial/"/>
    <updated>2014-07-21T17:47:04+08:00</updated>
    <id>http://findhy.com/blog/2014/07/21/mahout-tutorial</id>
    <content type="html"><![CDATA[<p><a href="http://mahout.apache.org/">Mahout</a> 是一个开源的Java实现的可伸缩的机器学习库，它实现了常见的聚类和推荐等机器学习算法，并且可运行在 Hadoop 分布式平台处理上PB的数据，Mahout 社区和文档非常丰富，是目前搭建推荐系统或者其它算法平台的首选方案。</p>

<!--more-->


<h3>目录</h3>

<ul>
<li>一、前言

<ul>
<li>1、Mahout 介绍</li>
<li>2、Mahout 最新动态 &ndash; Goodbye MapReduce</li>
<li>3、算法介绍</li>
</ul>
</li>
<li>二、Mahout 架构介绍</li>
<li>三、Mahout Startup</li>
</ul>


<h3>一、前言</h3>

<h4>1、Mahout介绍</h4>

<p>Mahout 是 Apache Software Foundation (ASF) 开发的一个全新的开源项目，其主要目标是创建一些可伸缩的机器学习算法，供开发人员在 Apache 在许可下免费使用，Mahout项目是由 Apache Lucene社区中对机器学习感兴趣的一些成员发起的，他们希望建立一个可靠、文档翔实、可伸缩的项目，在其中实现一些常见的用于聚类和分类的机器学习算法，该社区最初基于 Ngetal. 的文章 “Map-Reduce for Machine Learning on Multicore”，但此后在发展中又并入了更多广泛的机器学习方法，包括Collaborative Filtering（CF），Dimensionality Reduction，Topic Models等。此外，通过使用 Apache Hadoop 库，Mahout 可以有效地扩展到云中。在Mahout的Recommendation类算法中，主要有User-Based CF，Item-Based CF，ALS，ALS on Implicit Feedback，Weighted MF，SVD++，Parallel SGD等。</p>

<h4>2、Mahout 最新动态 &ndash; Goodbye MapReduce</h4>

<p>在今年4月25日，Mahout官方发布了一则新闻：</p>

<pre><code>25 April 2014 - Goodbye MapReduce
The Mahout community decided to move its codebase onto modern data processing systems that offer a richer programming model and more efficient execution than Hadoop MapReduce. Mahout will therefore reject new MapReduce algorithm implementations from now on. We will however keep our widely used MapReduce algorithms in the codebase and maintain them.
We are building our future implementations on top of a DSL for linear algebraic operations which has been developed over the last months. Programs written in this DSL are automatically optimized and executed in parallel on Apache Spark.
Furthermore, there is an experimental contribution undergoing which aims to integrate the h20 platform into Mahout.
</code></pre>

<p>主要的意思是Mahout社区决定放弃MapReduce数据处理模型，而将代码库迁移到处理速度更快更丰富的数据处理模型，但是原来实现MapReduce算法还可用且继续维护。过去几个月Mahout团队正在开发<a href="https://mahout.apache.org/users/sparkbindings/home.html">DSL for linear algebraic operations</a>，使用DSL模型编写的代码可以在Spark平台中自动优化和并行执行，可以看出Mahout开始向Spark靠近，后面会将它的算法实现更加适应于Spark平台，来达到性能的大幅提升，因为我们知道Spark是基于内存，它本身处理数据的性能要远高于MapReduce，关于这个更多可以参考<a href="https://issues.apache.org/jira/browse/MAHOUT-1500">Mahout-Jira</a>，这里也有关于DSL的解释，我现在还没有太理解清楚：</p>

<pre><code>The second set of DSL is for (looking identically to in-core set of operators) is for distributed stuff. 
(on diagram those two are not visually separated other than there's just part of it over in-core 
and part of it over distributed optimizer).
</code></pre>

<h4>3、算法介绍</h4>

<p>Mahout实现了大多数常用的算法，详细列表可以看<a href="https://mahout.apache.org/users/basics/algorithms.html">Mahout-algorithms</a>，它共有3种实现，第一种是单机的（single machine），受限于单机的性能处理较小量级的数据，第二种分布式实现（MapReduce），算法可以运行在Hadoop分布式平台中，性能可以横向扩展，可以处理上PB的数据，第三种是Spark实现，算法可以跑在Spark平台上，这个也是上面提到的，Mahout开始转向Spark平台，后面更多算法会迁移到Spark平台。下面介绍几个常用的算法。</p>

<h4>3.1、Recommendation</h4>

<p>推荐，给用户找到他想要的物品是目前互联网平台用的最多的算法，包括电商网站的商品推荐，社交平台的好友推荐，内容平台类似豆瓣的电影、图书推荐等等，被用的最多的是协同过滤算法(Collaborative Filtering-based Recommendation)，就是物品或者用户的相似度找到同类物品或用户来进行推荐，协同过滤算法又有三个分支：</p>

<ul>
<li>基于用户的推荐（User-based Recommendation）：发现与当前用户口味偏好相近的用户进行推荐，就是电商网站的：购买了此物品的用户还购买了哪些物品推荐</li>
<li>基于项目的推荐（Item-based Recommendation）：发现与当前用户购买物品相似的物品进行推荐，就是电商网站的：组合购买或者是你还可以购买哪些物品</li>
<li>和基于模型的推荐（Model-based Recommendation）：这个通常是基于历史数据来训练一个模型来进行推荐</li>
</ul>


<h4>3.2、Clustering</h4>

<p>聚类，是把一个数据对象划分成子集的过程，每个子集是一个簇（cluster），簇内的对象彼此相似，但与其它簇中的对象不相似，它通常用于商务管理中的客户分类，为不同的客户群制定不同的营销策略，还有离群点监测，就是那些远离任何簇的值，常用在金融交易中的信用卡欺诈校验。聚类作为统计学的一个分支，已经被广泛研究多年，主要集中在基于距离的聚类分析，基于K-均值（K-means）、K-中性点（K-medoids）。</p>

<h3>二、Mahout 架构介绍</h3>

<p><img src="http://findhy.com/images/mahout1.png"><br/>
这个Mahout整个业务架构图，最下面是Mahout的底层数学模型库和数据存储，Mahout可以从HDFS中读取数据，中间是它的算法实现，上面是业务层。</p>

<h3>三、Mahout Startup</h3>

<p>这里构建基于Mahout的maven项目，和一个算法实例：User-based Recommendation。</p>

<p>1、POM文件加入依赖</p>

<pre><code>    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.mahout&lt;/groupId&gt;
        &lt;artifactId&gt;mahout-core&lt;/artifactId&gt;
        &lt;version&gt;0.7&lt;/version&gt;
    &lt;/dependency&gt;
</code></pre>

<p>2、准备源文件</p>

<pre><code>1,3
1,4
2,44
2,46
3,3
3,5
3,6
4,3
4,5
4,11
4,44
5,1
5,2
5,4
</code></pre>

<p>3、示例代码</p>

<pre><code>package com.find.mahout.recommend;

import java.io.File;
import java.io.FileNotFoundException;
import java.io.IOException;
import java.util.List;

import org.apache.commons.cli2.OptionException;
import org.apache.mahout.cf.taste.common.TasteException;
import org.apache.mahout.cf.taste.impl.common.LongPrimitiveIterator;
import org.apache.mahout.cf.taste.impl.model.file.FileDataModel;
import org.apache.mahout.cf.taste.impl.recommender.CachingRecommender;
import org.apache.mahout.cf.taste.impl.recommender.slopeone.SlopeOneRecommender;
import org.apache.mahout.cf.taste.model.DataModel;
import org.apache.mahout.cf.taste.recommender.RecommendedItem;

public class UserCFMahout {

    public static void main(String[] args) throws FileNotFoundException, TasteException, IOException, OptionException {

        // 创建DataModel  slopeOne
        File ratingsFile = new File("E:\\cy-bigdata-project\\mahout-example\\src\\main\\resources\\datafile\\slopeOne.csv");
        DataModel model = new FileDataModel(ratingsFile);

        // 创建Recommender，使用SlopeOneRecommender
        CachingRecommender cachingRecommender = new CachingRecommender(new SlopeOneRecommender(model));

        // for all users
        for (LongPrimitiveIterator it = model.getUserIDs(); it.hasNext();) {
            long userId = it.nextLong();

            // get the recommendations for the user
            List&lt;RecommendedItem&gt; recommendations = cachingRecommender.recommend(userId, 10);

            // if empty write something
            if (recommendations.size() == 0) {
                System.out.print("User ");
                System.out.print(userId);
                System.out.println(": no recommendations");
            }

            // print the list of recommendations for each
            for (RecommendedItem recommendedItem : recommendations) {
                System.out.print("User ");
                System.out.print(userId);
                System.out.print(": ");
                System.out.println(recommendedItem);
            }
        }
    }
}
</code></pre>

<p>4、测试结果</p>

<pre><code>User 1: RecommendedItem[item:5, value:1.0]
User 2: RecommendedItem[item:5, value:1.0]
User 2: RecommendedItem[item:3, value:1.0]
User 3: no recommendations
User 4: no recommendations
User 5: RecommendedItem[item:5, value:1.0]
User 5: RecommendedItem[item:3, value:1.0]
</code></pre>

<p>更多例子请参考这个项目：<a href="https://github.com/findhy/mahout-example">https://github.com/findhy/mahout-example</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[手机应用商店的商业形态以及个性化推荐]]></title>
    <link href="http://findhy.com/blog/2014/07/18/app-store-business-and-recommend/"/>
    <updated>2014-07-18T11:53:49+08:00</updated>
    <id>http://findhy.com/blog/2014/07/18/app-store-business-and-recommend</id>
    <content type="html"><![CDATA[<p>最近同时在参与两件事：手机应用商店的商业化之路以及应用的个性化推荐，一点想法记录下来，手机应用商店市场已经泛滥，既有官方出品（Google Play），又有巨头入场（应用宝、360、91），还有少年得志生机勃勃的豌豆荚，作为应用商店怎么在竞争如此激烈的市场中抢占一席之地呢？</p>

<!--more-->


<h4>生态环境</h4>

<p>在手机应用商店的生态环境中，有三个主要参与者：</p>

<ul>
<li>开发者或者叫商家</li>
<li>应用商店</li>
<li>用户</li>
</ul>


<h4>盈利方式</h4>

<p>我们可以从结果为导向来先看下，应用商店想盈利，必须从用户和开发者身上想办法，从用户：</p>

<ul>
<li>广告：用户点击收费</li>
<li>付费内容：应用或者其它资源</li>
</ul>


<p>从开发者：</p>

<ul>
<li>广告：为开发者推广应用收费</li>
<li>联运：合作运营，收入分成</li>
<li>支付：为开发者提供支付，通过账期和手续费来盈利</li>
</ul>


<p>后两个都需要其它资源辅助才能做，暂且不谈，所以还是又回到了互联网最基础的盈利方式：广告。</p>

<p>但是应用商店的广告会有一个平衡问题：</p>

<ul>
<li>应用商店最核心的内容就是要提供用户想要的应用，所以需要把用户最需要的应用放在最好的位置</li>
<li>应用商店想盈利，就必须把最好的位置留给广告主</li>
</ul>


<p>怎么将广告与用户的强需求完美的结合？这个是应用商店需要思考的问题，如果做得不好，用户找不到自己想要的应用，可能以后就不会再来了，但如果只是纯粹推热门的、用户感兴趣的，又无法盈利。这又回到本文的标题：个性化推荐。</p>

<h4>个性化推荐</h4>

<p>如果应用商店做到：推荐的广告就是用户想要的应用呢？首先我们需要非常了解我们的用户，积累用户行为数据非常重要：</p>

<ul>
<li>评论体系</li>
<li>评分体系</li>
<li>下载</li>
<li>搜索</li>
</ul>


<p>通过这些来分析用户的兴趣模型，再加上推荐算法(CF)达到给用户看的广告就是他感兴趣的，而且这个阀值也很重要，如果没有找到用户感兴趣的广告，宁愿不推荐也不要放一些不相关的广告来导致用户流失，所以整体算法需要同时考虑广告收益和用户体验，广告推荐要和应用推荐结合使用。</p>

<h4>总结</h4>

<ul>
<li>应用商店想盈利留住用户是关键，没有用户其它都是扯</li>
<li>所以在广告和用户体验上，广告必须要为用户体验让步</li>
<li>在应用商店产品层面，要做更多的附加内容来吸引用户(社交、线下活动、论坛、O2O)，形成一个圈子，单纯一个应用商店，产品形态还是太小了</li>
<li>有了用户，有了用户交互，积累了数据，盈利也会比较容易，也会衍生出其它产品形态(卖数据服务来指导游戏厂商哪些游戏受用户喜爱)</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Zookeeper运维管理工具介绍]]></title>
    <link href="http://findhy.com/blog/2014/07/09/zookeeper-manager/"/>
    <updated>2014-07-09T13:39:26+08:00</updated>
    <id>http://findhy.com/blog/2014/07/09/zookeeper-manager</id>
    <content type="html"><![CDATA[<p><a href="http://zookeeper.apache.org/">Zookeeper</a>是一个提供维护配置信息、命名空间、分布式同步和组服务的集中式管理工具，广泛应用与分布式系统中，像Storm、HBase、Dubbo都依赖Zookeeper，所以怎么运维和管理Zookeeper就非常重要了，下面介绍几个Zookeeper的管理工具，有些我用过有些我没有用过，仅供参考。</p>

<!--more-->


<h3>1.Taokeeper</h3>

<p>阿里开源的Zookeeper监控工具，可以用来监控Zookeeper集群的工作状态、连接数、注册的Watcher数和磁盘/内存/CPU状态，还可以设置预警，当系统性能达到某个阀值的时候通知管理员，这块提供自定义开发接口。</p>

<p>项目地址：<a href="https://github.com/alibaba/taokeeper">https://github.com/alibaba/taokeeper</a></p>

<p>安装说明：<a href="http://jm-blog.aliapp.com/?p=1450">http://jm-blog.aliapp.com/?p=1450</a></p>

<p>注意如果启动完看不到机器的磁盘和CPU等状态，可能是服务器没有安装nc，执行 yum install -y nc</p>

<h3>2.ZooInspector</h3>

<p><a href="http://www.taobaotesting.com/blogs/qa?bid=15305">ZooInspector</a>可以用来查询Zookeeper里面的具体信息，还可以与Eclipse集成可视化，这样我们在开发过程中就能够实时看到Zookeeper中的数据。</p>

<h3>3.node-zk-browser</h3>

<p><a href="https://github.com/killme2008/node-zk-browser">node-zk-browser</a>和ZooInspector功能相似，但是基于浏览器访问的，底层是用Node.js实现。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Spark1.0在YARN上部署过程]]></title>
    <link href="http://findhy.com/blog/2014/06/28/spark-on-yarn/"/>
    <updated>2014-06-28T14:13:34+08:00</updated>
    <id>http://findhy.com/blog/2014/06/28/spark-on-yarn</id>
    <content type="html"><![CDATA[<p>Spark支持独立部署和在YARN上部署，选择在YARN上就是作为一个APP跑在YARN平台上，这样的好处是可以和原来的数据平台无缝结合。</p>

<!--more-->


<h3>安装过程</h3>

<p>1、下载</p>

<pre><code>wget https://github.com/apache/spark/archive/v1.0.0.zip
</code></pre>

<p>2、解压</p>

<pre><code>unzip v1.0.0.zip
</code></pre>

<p>3、进入目录</p>

<pre><code>cd spark-1.0.0
</code></pre>

<p>4、设置maven内存参数：</p>

<pre><code>export MAVEN_OPTS="-Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m"
</code></pre>

<p>5、打包</p>

<pre><code>mvn -Pyarn-alpha -Dhadoop.version=2.0.0-cdh4.3.2 -DskipTests clean package
</code></pre>

<p>最后编译报错：</p>

<pre><code>Failed to execute goal org.apache.maven.plugins:maven-antrun-plugin:1.7:run (default) on project spark-core_2.10: An Ant BuildException has occured: Please set the SCALA_HOME (or SCALA_LIBRARY_PATH if scala is on the path) environment variables and retry.
[ERROR] around Ant part ...&lt;fail message="Please set the SCALA_HOME (or SCALA_LIBRARY_PATH if scala is on the path) environment variables and retry."&gt;... @ 6:126 in /home/hadoop/spark-cdh4.3.2/spark-1.0.0/core/target/antrun/build-main.xml
</code></pre>

<p>缺失scala的依赖</p>

<p>查看pom.xml</p>

<pre><code>&lt;scala.version&gt;2.10.4&lt;/scala.version&gt;
</code></pre>

<p>6、安装scala2.10.4</p>

<pre><code>wget http://www.scala-lang.org/files/archive/scala-2.10.4.tgz

tar -zxvf scala-2.10.4.tgz

export SCALA_HOME=/opt/scala/scala-2.10.4
</code></pre>

<p>7、再次打包</p>

<pre><code>mvn -Pyarn-alpha -Dhadoop.version=2.0.0-cdh4.3.2 -DskipTests clean package
</code></pre>

<p>这个打包过程需要很久，看到BUILD SUCCESS就成功了</p>

<p>8、打spark内核包</p>

<pre><code>SPARK_HADOOP_VERSION=2.0.0-cdh4.3.2 SPARK_YARN=true sbt/sbt assembly
</code></pre>

<p>9、执行完成之后，在YARN上发布APP</p>

<pre><code>./bin/spark-submit --class org.apache.spark.examples.SparkTC --master yarn-cluster --num-executors 3 --executor-memory 2g --driver-memory 4g --executor-cores 1 --jars /home/hadoop/spark-cdh4.3.2/spark-1.0.0/examples/target/spark-examples*.jar
</code></pre>

<p>看到命令行显示<br/>
<img src="http://findhy.com/images/spark-1.png"></p>

<p>YARN管理台显示<br/>
<img src="http://findhy.com/images/spark-2.png"></p>

<p>点击yarn app的tracking UI到达Spark的管理界面<br/>
<img src="http://findhy.com/images/spark-3.png"></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Titan Visualization With VivaGraphJS]]></title>
    <link href="http://findhy.com/blog/2014/06/23/titan-visualization-with-vivagraphjs/"/>
    <updated>2014-06-23T16:09:23+08:00</updated>
    <id>http://findhy.com/blog/2014/06/23/titan-visualization-with-vivagraphjs</id>
    <content type="html"><![CDATA[<p><a href="http://findhy.com/blog/2014/06/21/graph-database-visualization/">上一篇文章</a>介绍了Graph Database的可视化技术方案，本项目是Titan的数据可视化和在线更新，前端使用的是<a href="https://github.com/anvaka/VivaGraphJS">VivaGraphJS</a>，在线更新就是调用Rexster提供的REST API，具体可以参考github上面的README介绍：<a href="https://github.com/titan-cn/titan-vivagraph">titan-vivagraph</a>。</p>

<!--more-->



]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[存储性能瓶颈的成因、定位与排查]]></title>
    <link href="http://findhy.com/blog/2014/06/23/storage-performance-locate/"/>
    <updated>2014-06-23T10:41:46+08:00</updated>
    <id>http://findhy.com/blog/2014/06/23/storage-performance-locate</id>
    <content type="html"><![CDATA[<p>一个系统的性能瓶颈可能发生在很多地方，常见包括：CPU、内存、存储IO、网络、技术，其中存储IO的性能瓶颈最容易发生，往往我们刚开始搭建系统的时候都放在单台服务器上，当业务扩大后，单台已经无法支撑，常见的做法就是，前端加一个F5或者Nginx做负载均衡，后端应用做集群，数据库做集群或者读写分离、分表分库来缓解服务器的存储IO瓶颈，但是遇到存储瓶颈时我们如何定位问题和在架构设计的时候就去避免这样的问题呢，下面转载一篇EMC中文论坛的一篇文章，原文在这里<a href="https://community.emc.com/docs/DOC-34921">存储性能瓶颈的成因、定位与排查</a>。正文：</p>

<!--more-->


<h3>介绍</h3>

<p>企业数据存储性能瓶颈常常会发生在端口，控制器和磁盘，难点在于找出引起拥塞的单元，往往需要应用多重工具以及丰富的经验来查找并解决。
本文详细阐述存储瓶颈发生最常见的四种情况，可能发生的拥塞点，需要监控的参数指标，以及部署存储系统的最佳实践。</p>

<h3>更多信息</h3>

<h4>数据存储瓶颈的四个常见场景：</h4>

<p>以下是储瓶颈发生最常见的四种典型情况：</p>

<ol>
<li><p>当多个用户同时访问某一业务应用，无论是邮件服务器，企业资源规划（ERP）系统或数据库，数据请求会累积在队列中。单个I/O的响应时间开始增长，短暂延时开始转变成为漫长的等待。
这类响应时间敏感型应用的特征是，很多随机请求，读取比写入更多，I/O较小。最好的方法是：将负载分布在多块磁盘上，否则可能造成性能瓶颈。
如果应用增加了更多用户，或应用IOPS请求增加，则可能需要在RAID组中添加更多磁盘，或数据可能需要跨越更多磁盘，在更多层级做条带化。
存储在这样的情况下往往首先被怀疑，但大多数情况下并非存储引发，原因可能在于网络、应用或服务器。</p></li>
<li><p>带宽敏感型应用——如数据备份，视频流或安全登录，这类应用当多个用户同时访问大型文件或数据流时可能造成瓶颈。
定位这一问题存储管理员应当从备份服务器开始一路向下检查至磁盘，原因可能存在于这一通路的任何地方。
问题不一定发生在存储，可能是由于备份应用创建的方式或是磁带系统的工作方式引起的。如果瓶颈定位于存储，那么可能是由于服务I/O的磁盘数量不足，在控制器造成争用，或是阵列前端口带宽不足。
性能调优需要针对不同应用程序负载来完成。针对大型文件和流数据的调优并不适合于小型文件，反之亦然。这也就是为什么在大多数存储系统中往往做一个平衡，需要用户尝试并找出系统的折中。用户通常需要优化吞吐量或IOPS，但并不需要对两者同时优化。</p></li>
<li><p>RAID组中的磁盘故障。特别是在RAID 5中会造成性能的下降，因为系统需要重建校验数据。相比数据读写操作，重建会对性能造成更大影响。
即便坏盘是造成故障的根源，但控制器还是可能成为瓶颈，因为在重建过程中它需要不停地服务数据。当重建完成时，性能才会恢复正常。</p></li>
<li><p>部署了一种新的应用，而卷存在于处理繁忙邮件系统的同一磁盘。如果新的应用变得繁忙，邮件系统性能将会遭受影响。额外的流量最终会将磁盘完全覆盖。</p></li>
</ol>


<h4>存储瓶颈常发区域:</h4>

<h4>存储区域网络（Storage-area network, SAN）/阵列前端口</h4>

<p>存储部署于集中化SAN环境时，需考虑服务器和SAN之间的潜在网络瓶颈。例如，运行多部虚拟机的整合服务器可能不具备支持工作负载要求的足够网络端口。添加网络端口或转移网络密集型工作负载至其他服务器可解决这一问题。如前所述，对于带宽集中型应用，需考虑NFS有多少Fiber Channel 端口, or iSCSI 端口 or Ethernet 端口，需要用户站在带宽的角度来考量整个架构。</p>

<p>可能发生的问题包括：
如果阵列中端口数量不够，就会发生过饱和/过度使用。
虚拟服务器环境下的过量预定
端口间负载不均衡
交换机间链路争用/流量负荷过重
如某一HBA端口负载过重将导致HBA拥塞。使用虚拟机会导致问题更加严重。</p>

<h4>存储控制器</h4>

<p>一个标准的主动——被动或主动——主动控制器都有一个性能极限。接近这条上限取决于用户有多少块磁盘，因为每块磁盘的IOPS和吞吐量是固定的。</p>

<p>可能出现的问题包括：
控制器I/O过饱和，使得从缓存到阵列能够处理的IOPS受到限制
吞吐量“淹没“处理器
CPU过载/处理器功率不足
性能无法跟上SSD</p>

<h4>Cache</h4>

<p>由于服务器内存和CPU远比机械磁盘快得多，需为磁盘添加高速内存以缓存读写数据。例如，写入磁盘的数据存储在缓存中直到磁盘能够跟上，同时磁盘中的读数据放入缓存中直到能被主机读取。Cache比磁盘快1000倍，因此将数据写入和读出Cache对性能影响巨大。智能缓存算法能够预测你需要查找的数据，你是否会对此数据频繁访问，甚至是将访问频繁的随机数据放在缓存中。</p>

<p>可能发生的问题包括：
Cache memory不足
Cache写入过载，引起性能降低
频繁访问顺序性数据引起cache超负荷
Cache中需要持续不断地写入新数据，因此如果cache总是在refill，将无法从cache获益。</p>

<h4>磁盘</h4>

<p>磁盘瓶颈与磁盘转速有关, 慢速磁盘会引入较多延时。存储性能问题的排查首先考虑的因素就是磁盘速度，同时有多少块磁盘可进行并发读写。而另一因素是磁盘接口。采用更快的接口能够缓解磁盘瓶颈，但更重要的是在快速接口与相应更大的缓存大小以及转速之间取得平衡。同样，应避免将快速和慢速磁盘混入同一接口，因为慢速磁盘将会造成快速接口与快速磁盘的性能浪费。</p>

<p>可能引发的问题包括：
过多应用命中磁盘
磁盘数量不足以满足应用所需的IOPS或吞吐量
磁盘速度过慢无法满足性能需求及支持繁重工作负荷
Disk group往往是classic存储架构的潜在性能瓶颈，这种结构下RAID最多配置在16块磁盘。Thin结构通常每个LUN拥有更多磁盘，从而数据分布于更多spindle，因增加的并发性而减少了成为瓶颈的可能。</p>

<h4>需要监控的指标：</h4>

<p>曾经一度存储厂商们强调的是IOPS和吞吐量，但现在重点逐渐转变成为响应时间。也就是说，不是数据移动的速度有多快，而在于对请求的响应速度有多快。</p>

<p>正常情况下，15,000 rpm Fibre Channel磁盘响应时间为4ms，SAS磁盘响应时间约为5ms至6ms，SATA为10ms，而SSD少于1ms。如果发现Fibre Channel磁盘响应时间为12ms，或SSD响应时间变成5ms，那么就说明可能产生了争用，可能芯片发生了故障。</p>

<p>除了响应时间，其他需要监控的指标包括：
队列长度，队列中一次积累的请求数量，平均磁盘队列长度；
平均I/O大小千字节数；
IOPS （读和写，随机和顺序，整体平均IOPS）；
每秒百万字节吞吐量；
读写所占比例；
容量（空闲，使用和保留）。</p>

<h4>数据存储性能最佳实践：</h4>

<p>性能调优和改进的方式有很多种，用户当然可以通过添加磁盘，端口，多核处理器，内存来改善，但问题是：性价比，以及对业务是否实用。本文建议的方式是在预算范围内找寻性能最大化的解决方案。另外一个需要考虑的方面是环境并非一尘不变，系统部署方案要能够适应环境的改变需求。</p>

<p>首先需要考虑刷数据的性能特征，需要了解IO工作情况是怎样的。是否是cache友好型？是否是CPU集中型？业务数据很大数量很少，还是很小但数量很多？另外一方面就是构成存储环境的组件。包括应用，存储系统本身，网络。。。瓶颈可能在哪里，改善哪里最有效？</p>

<p>以下是一些常规建议：
不要仅仅根据空闲空间来分配存储，而需要结合考虑性能需求，确保为吞吐量或IOPS分配足够多的磁盘。
在磁盘间均衡分布应用负载，以减少热点地区的产生。
理解应用负载类型，并针对负载选择匹配的RAID类型。例如，写密集型应用建议使用RAID 1而不是RAID 5。因为当写入RAID 5时，需要计算校验位，需耗费较多时间。而RAID 1，写入两块磁盘速度快得多，无需计算。
磁盘类型（Fibre Channel, SAS, SATA）与期望性能相匹配。对于关键业务应用部署高性能磁盘，例如15,000 rpm Fibre Channel。
对于I/O密集型应用考虑采用SSD，但并不适用于写性能重要型应用。只要没有达到控制器瓶颈，SSD对读性能提升显著，但对写性能提升并没有明显效果。
采用端对端的监控工具，特别是虚拟服务器环境。虚拟端与物理端之间有一道防火墙，所以，需要穿透防火墙进行端到端的监控。
有些性能分析工具涵盖从应用到磁盘，有些仅局限于存储系统本身。由于性能是一个连锁反应包含很多变量，所以需要全面地分析数据。
以数据仅写入磁盘外部扇区的方式格式化磁盘。因减少数据定位时间而在高I/O环境下提升性能。负面作用是相当一部分磁盘容量未能得以使用。</p>

<h3>应用于</h3>

<p>存储性能分析、定位与排查</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[图数据库(graph Database)可视化]]></title>
    <link href="http://findhy.com/blog/2014/06/21/graph-database-visualization/"/>
    <updated>2014-06-21T11:04:58+08:00</updated>
    <id>http://findhy.com/blog/2014/06/21/graph-database-visualization</id>
    <content type="html"><![CDATA[<p>数据可视化是大数据的最后一环，重要性不可忽视，内行往往关心你的算法和架构，但是用户(客户或者领导)只会看最终展现在他们面前的东西，当然业务层面你需要先了解用户的核心需求，再去建模和设计指标，本文讨论关于数据可视化的技术方案，还有比较特别的图数据库的可视化。</p>

<!--more-->


<h3>1.可视化类型</h3>

<p>从用户响应时间的角度，有：</p>

<ul>
<li>静态可视化：当天日志导入到数据平台，晚上跑一些MR任务，再将执行结果存入HBase或者关系型数据库MySql，第二天运营看到结果</li>
<li>实时可视化：实时可视化前提是数据是实时变化的，通常用Storm或者Spark架构实时处理来自客户端或者Kafka的数据，再将处理完的数据导入到后台存储中，然后前端展现实时有两种方式，一种是被动的，就是你要再去刷新一次，这种比较好实现，一种是主动的，服务器端通知客户端，这种就用Websocket来做，如果服务器端用Node.js，更简单直接用socket.io</li>
</ul>


<p>从展现的图类型来看，可能列举不全：</p>

<ul>
<li>线性图：展现趋势</li>
<li>饼图/柱图：展现占比/对比</li>
<li>散点图：展现聚合关系</li>
<li>地理位置可视化：把数据在地图上展现，DataMap、GoogleMap都有接口，一般要求数据里面有地理位置属性（IP地址、城市、国家）</li>
<li>Graph：用Graph展现一个KnowledgeMap，像社交网站的关系图谱，通常用来做用户定位和推荐</li>
</ul>


<h3>2.前端可视化技术</h3>

<p>基于浏览器的开源的：</p>

<ul>
<li>highchart</li>
<li>d3.js</li>
<li>ECharts</li>
<li>google chart</li>
</ul>


<p>基于桌面的开源的：</p>

<ul>
<li><a href="https://gephi.org/">gephi</a></li>
</ul>


<p>上面这些比较常用，而且例子和社区都比较成熟，具体的可以直接去官方看，我们现在普通报表有用highchart和d3.js的，gephi也可以在浏览器端展现。</p>

<h3>3.Graph Database可视化</h3>

<p>关于Graph Database可视化单独拿出来，这块数据来源是像Titan、Neo4j和OrientDB这一类的图数据库，他们都提供基于REST的接口，以JSON数据返回Graph的信息，以Titan为例，在Titan上面可以搭建一个Rexster服务器，它提供很多针对Graph的REST接口，具体有哪些接口可以看这里<a href="https://github.com/tinkerpop/rexster/wiki/Basic-REST-API">Rexster-REST-API</a>，前端通过调用接口获取数据，在前端去构建Graph图，构建的方式可以是Canvas或SVG，关于这两的比较可以看这里<a href="http://msdn.microsoft.com/zh-cn/library/ie/gg193983(v=vs.85).aspx">SVG 与 Canvas：如何选择</a>。常用的技术解决方案有：</p>

<ul>
<li><a href="http://sigmajs.org/">Sigma.js</a>：开源，通用</li>
<li><a href="http://keylines.com/">Keylines</a>：商业方案，官网有针对Titan和Neo4j可视化的例子</li>
<li><a href="https://github.com/anvaka/VivaGraphJS">VivaGraph</a>：开源，通用，社区没有Sigma.js丰富</li>
<li><a href="https://github.com/anvaka/ngraph">ngraph</a>：VivaGraph的下一个版本，使用WebGL支持3d效果展现</li>
<li><a href="http://d3js.org/">D3.js</a>：开源，通用，上面提到了，它也提供Graph可视化的功能</li>
<li><a href="https://gephi.org/">Gephi</a>：开源，通用，很强大的基于桌面可视化解决方案，通过插件也可以在浏览器端展现</li>
<li><a href="http://linkurio.us/">Linkurious</a>：Neo4j专有的</li>
<li>Neo4J web-admin：Neo4j专有的</li>
</ul>


<p>参考：</p>

<p><a href="http://stackoverflow.com/questions/14867132/is-d3-js-the-right-choice-for-real-time-visualization-of-neo4j-graph-db-data/23522907#23522907">http://stackoverflow.com/questions/14867132/is-d3-js-the-right-choice-for-real-time-visualization-of-neo4j-graph-db-data/23522907#23522907</a></p>

<p><a href="http://stackoverflow.com/questions/18571685/neo4j-graph-visualizing-libraries?rq=1">http://stackoverflow.com/questions/18571685/neo4j-graph-visualizing-libraries?rq=1</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Titan Tutorial]]></title>
    <link href="http://findhy.com/blog/2014/06/19/titan-tutorial/"/>
    <updated>2014-06-19T13:43:40+08:00</updated>
    <id>http://findhy.com/blog/2014/06/19/titan-tutorial</id>
    <content type="html"><![CDATA[<p>Titan的<a href="https://github.com/thinkaurelius/titan/wiki">官方手册</a>内容更加丰富，但是太多，初学者不知如何下手，本文摘取重点部分，希望能快速上手Titan。</p>

<!--more-->


<h3>1.版本说明</h3>

<pre><code>Titan：titan-server-0.4.4
HBase：hbase-0.94.6-cdh4.3.2
Elasticsearch：elasticsearch-0.90.3
</code></pre>

<h3>2.环境说明</h3>

<p>服务器3台：</p>

<pre><code>master 10.0.1.252
slave1 10.0.1.253
slave2 10.0.1.254
</code></pre>

<p>HBase搭建的是集群，一个master，两个slave；Elasticsearch在master上部署的单机版本；Titan在master上部署的单机版本。本文不包括HBase集群搭建过程。</p>

<h3>3.Elasticsearch安装</h3>

<p>由于Titan0.4.4版本只能支持Elasticsearch的版本是0.90.3，看这里<a href="https://github.com/thinkaurelius/titan/wiki/Version-Compatibility">Version-Compatibility</a>。所以这里注意版本，Elasticsearch 0.90.3的文档可以看这里<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/0.90/index.html">Elasticsearch-doc</a>。下面开始安装。</p>

<pre><code>wget https://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-0.90.3.tar.gz
tar –zxvf elasticsearch-0.90.3.tar.gz
cd elasticsearch-0.90.3

启动elasticsearch：
./bin/elasticsearch
执行jps，会看到多了一个ElasticSearch的进程，说明成功
</code></pre>

<h3>4.Titan安装</h3>

<p>Titan有<a href="https://github.com/thinkaurelius/titan/wiki/Downloads">多种版本</a>提供下载，这里选择titan-server-0.4.4。</p>

<pre><code>mkdir /home/hadoop/titan-cdh4.3.2
进入
cd titan-cdh4.3.2
下载
wget http://s3.thinkaurelius.com/downloads/titan/titan-server-0.4.4.zip
解压
unzip titan-server-0.4.4.zip
进入目录
cd titan-server-0.4.4
</code></pre>

<p>修改配置文件</p>

<pre><code>vi ./conf/titan-hbase-es.properties

storage.hostname=master,slave1,slave2
storage.port=2181
cache.db-cache = true
cache.db-cache-clean-wait = 20
cache.db-cache-time = 180000
cache.db-cache-size = 0.5

storage.index.search.backend=elasticsearch
storage.index.search.hostname=master
storage.index.search.client-only=true
</code></pre>

<p>初始化Titan与HBase</p>

<pre><code>cd /home/hadoop/titan-cdh4.3.2/titan-server-0.4.4/
./bin/gremlin.sh

gremlin&gt;g = TitanFactory.open('conf/titan-hbase-es.properties')
</code></pre>

<p>这时候到hbase shell下面执行list命令，可以看到多了一张titan的表，执行describe &lsquo;titan&#8217;可以看到titan的表结构，加载数据：</p>

<pre><code>gremlin&gt; GraphOfTheGodsFactory.load(g)
</code></pre>

<p>到hbase shell下面执行scan &lsquo;titan&#8217;可以看到初始化了一些数据，下面用gremlin命令行验证一下这些数据</p>

<pre><code>gremlin&gt; saturn = g.V('name','saturn').next()
==&gt;v[4]
gremlin&gt; saturn.map()
==&gt;name=saturn
==&gt;age=10000
==&gt;type=titan
gremlin&gt; saturn.in('father').in('father').name
==&gt;hercules
</code></pre>

<p>如果输出一致则验证成功</p>

<h3>5.Rexster配置</h3>

<p>这部分文档参考：<a href="https://github.com/thinkaurelius/titan/wiki/Rexster-Graph-Server">https://github.com/thinkaurelius/titan/wiki/Rexster-Graph-Server</a></p>

<p>修改rexster配置文件</p>

<pre><code>cd /home/hadoop/titan-cdh4.3.2/titan-server-0.4.4/conf
cp rexster-cassandra-es.xml rexster-hbase-es.xml
vi rexster-hbase-es.xml
</code></pre>

<p>有两个地方要改，一个是http这个标签，一个是graphs这个标签，黄色是需要修改的内容，第一个修改如下：</p>

<pre><code>&lt;http&gt;
  &lt;server-port&gt;8182&lt;/rexster-server-port&gt;
  &lt;base-uri&gt;http://54.255.164.52&lt;/base-uri&gt;
  &lt;web-root&gt;public&lt;/web-root&gt;
  &lt;character-set&gt;UTF-8&lt;/character-set&gt;
  ...
&lt;/http&gt;
</code></pre>

<p>第二个修改如下：</p>

<pre><code>&lt;graphs&gt;
    &lt;graph&gt;
        &lt;graph-name&gt;graph&lt;/graph-name&gt;
       &lt;graph-type&gt;com.thinkaurelius.titan.tinkerpop.rexster.TitanGraphConfiguration&lt;/graph-type&gt;
        &lt;!-- &lt;graph-location&gt;/tmp/titan&lt;/graph-location&gt; --&gt;
        &lt;graph-read-only&gt;false&lt;/graph-read-only&gt;
        &lt;properties&gt;
            &lt;storage.backend&gt;hbase&lt;/storage.backend&gt;
            &lt;storage.hostname&gt;master,slave1,slave2&lt;/storage.hostname&gt;
            &lt;storage.index.search.backend&gt;elasticsearch&lt;/storage.index.search.backend&gt;
            &lt;storage.index.search.hostname&gt;master&lt;/storage.index.search.hostname&gt;
            &lt;!--&lt;storage.index.search.directory&gt;../db/es&lt;/storage.index.search.directory&gt;--&gt;
            &lt;storage.index.search.client-only&gt;false&lt;/storage.index.search.client-only&gt;
            &lt;storage.index.search.local-mode&gt;false&lt;/storage.index.search.local-mode&gt;
        &lt;/properties&gt;
        &lt;extensions&gt;
          &lt;allows&gt;
            &lt;allow&gt;tp:gremlin&lt;/allow&gt;
          &lt;/allows&gt;
        &lt;/extensions&gt;
    &lt;/graph&gt;
&lt;/graphs&gt;
</code></pre>

<p>启动Rexster</p>

<pre><code>cd /home/hadoop/titan-cdh4.3.2/titan-server-0.4.4
./bin/rexster.sh –s –c ../conf/rexster-hbase-es.xml
</code></pre>

<p>访问<a href="http://master-ip:8182/">http://master-ip:8182/</a></p>

<p>出现下面画面则启动成功</p>

<p><img src="http://findhy.com/images/titan-tul-1.png"></p>

<p><a href="https://github.com/tinkerpop/rexster/wiki">Rexster</a>是建立在任何实现了Blueprints的图数据库(Graph Database)之上的web server，它提供这三种功能：</p>

<ul>
<li>提供基于REST的接口方法：GET, POST, PUT, and DELETE，去操作Graph Database

<ul>
<li>基于上面的例子，在浏览器输入：<a href="http://master-ip:8182/graphs/graph/edges">http://master-ip:8182/graphs/graph/edges</a>  会返回graph的edge信息</li>
</ul>
</li>
<li><a href="https://github.com/tinkerpop/rexster/wiki/The-Dog-House">The Dog House</a>提供基于浏览器去操作Graph，还有可视化Graph，界面如下：

<ul>
<li><img src="http://findhy.com/images/titan-tul-2.png"></li>
<li><img src="http://findhy.com/images/titan-tul-3.png"></li>
</ul>
</li>
<li>提供<a href="https://github.com/tinkerpop/rexster/wiki/RexPro-Java">RexsterClient</a>客户端去访问Rexster server，包括执行一些Graph的操作</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Titan经典入门PPT]]></title>
    <link href="http://findhy.com/blog/2014/06/19/titan-classic-get-started/"/>
    <updated>2014-06-19T11:00:50+08:00</updated>
    <id>http://findhy.com/blog/2014/06/19/titan-classic-get-started</id>
    <content type="html"><![CDATA[<p>下面是Marko Rodriguez分享的一个PPT，原文在这里<a href="http://www.slideshare.net/slidarko/titan-the-rise-of-big-graph-data">slideshare.</a>，该PPT深入浅出，从Graph基础知识到Graph Database到Titan的优势，还包括基本的入门操作，我觉得非常经典，很适合初学者，在此分享，因为原文PPT表达很简单而且加上动画展示，所以即使英文不好的人看也没有障碍。</p>

<!--more-->




<iframe src="http://www.slideshare.net/slideshow/embed_code/13328271 " width="595" height="446" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC;border-width:1px 1px 0;margin-bottom:5px" allowfullscreen></iframe>


<p></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[怎么在Octopress Blog中嵌入slideshare]]></title>
    <link href="http://findhy.com/blog/2014/06/19/how-to-put-slideshare-on-octopress-blog/"/>
    <updated>2014-06-19T10:56:50+08:00</updated>
    <id>http://findhy.com/blog/2014/06/19/how-to-put-slideshare-on-octopress-blog</id>
    <content type="html"><![CDATA[<p>怎么在Octopress blog中嵌入slideshar，使用该插件<a href="https://github.com/petehamilton/Octopress-Slideshare-Plugin">Octopress-Slideshare-Plugin</a>，下面是操作过程。</p>

<!--more-->


<h3>1.下载插件</h3>

<pre><code>git clone https://github.com/petehamilton/Octopress-Slideshare-Plugin.git
</code></pre>

<h3>2.安装插件</h3>

<p>进入上面下载的插件目录，拷贝Octopress-Slideshare-Plugin/slideshare.rb到octopress/plugins目录下面</p>

<h3>3.生成对应的Slideshare Embed ID</h3>

<p>到<a href="http://www.slideshare.net/">slideshare</a>网站找到你需要嵌入的ppt。</p>

<p><img src="http://findhy.com/images/slideshare-1.png"></p>

<p>点击Embed，生成类似下面的链接，最后的291600就是我们要的Slideshare Embed ID</p>

<pre><code>http://www.slideshare.net/slideshow/embed_code/291600
</code></pre>

<h3>4.嵌入</h3>

<p>在你的bolg MD文件中添加下面的代码：</p>

<p><img src="http://findhy.com/images/slideshare-2.png"></p>

<p>执行</p>

<pre><code>rake generate
rake preview
</code></pre>

<p>看到</p>

<iframe src="http://www.slideshare.net/slideshow/embed_code/291600 " width="595" height="446" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC;border-width:1px 1px 0;margin-bottom:5px" allowfullscreen></iframe>


<p></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[TinkerPop]]></title>
    <link href="http://findhy.com/blog/2014/06/18/tinkerpop/"/>
    <updated>2014-06-18T15:09:59+08:00</updated>
    <id>http://findhy.com/blog/2014/06/18/tinkerpop</id>
    <content type="html"><![CDATA[<p><a href="http://www.tinkerpop.com/">TinkerPop</a>是Graph领域的一系列开源工具包的集合。下面分别介绍：</p>

<!--more-->


<h3>Blueprints</h3>

<p>上一篇文章我们讲<a href="http://findhy.com/blog/2014/06/17/graph-database-data-structure/">Graph Database理论知识</a>的时候，提到了图论关于图(Graph)的定义：顶点和边组成的图形，也在后面提到了关于Graph的一系列操作，包括：插入顶点、插入边、获取路径等等。Blueprints是对图这种抽象模型的具体实现，官方定义：<a href="https://github.com/tinkerpop/blueprints/wiki">Blueprints</a>是一系列<a href="https://github.com/tinkerpop/gremlin/wiki/Defining-a-Property-Graph">属性图模型接口(property graph model interface)</a>，那么接下来，什么是属性图模型(property graph model)？满足下面三个条件的图(Graph)被称为属性图(property graphs)：</p>

<ul>
<li>顶点(vertices)和边(edges)可以包含任意多的key/value的属性</li>
<li>方向性，边(edges)具有方向性，可以从一个顶点(vertices)指向另外一个顶点(vertices)</li>
<li>多样性，顶点(vertices)之间的关系边(edges)可以是不同的类型，就是说两个顶点(vertices)可以拥有多种不同类型的边(edges)</li>
</ul>


<p>满足上述三个条件的graph被称为property graphs，下面展现一个property graphs的例子，数据格式可以是<a href="http://graphml.graphdrawing.org/index.html">GraphML</a>或者<a href="https://github.com/tinkerpop/blueprints/wiki/GraphSON-Reader-and-Writer-Library">GraphSON</a>，前者是<a href="https://github.com/tinkerpop/gremlin/blob/master/data/graph-example-1.xml">XML</a>，后者<a href="https://github.com/tinkerpop/gremlin/blob/master/data/graph-example-1.json">JSON</a>，当然JSON会更轻量级。</p>

<p><img src="http://findhy.com/images/tinkpop-1.png"></p>

<p>一个property graphs包含下面这些元素</p>

<ul>
<li>一系列顶点(vertices)

<ul>
<li>每一个顶点(vertex)有一个唯一标识</li>
<li>每一个顶点(vertex)有一个或者多个指向其它顶点的边(edge)</li>
<li>每一个顶点(vertex)有一个或者多个指向自己的边(edge)</li>
<li>每一个顶点(vertex)包含了一个或多个由map定义的key/value属性</li>
</ul>
</li>
<li>一系列边(edges)

<ul>
<li>每一个边(edge)有一个唯一标识</li>
<li>每一个边(edge)具有方向性指向一个顶点(vertex)</li>
<li>每一个边(edge)有一个label来标识两个顶点(vertex)之间的关系</li>
<li>每一个边(edge)包含了一个或多个由map定义的key/value属性</li>
</ul>
</li>
</ul>


<p>什么是property graphs搞明白之后，我们再来看Blueprints，Blueprints为属性图模型(property graph data model)提供了一套接口、实现还有测试用例，你可以把它想象成JDBC，JDBC对数据库的操作原语进行了封装和实现，只不过JDBC是用来操作关系型数据库，而Blueprints用来操作Graph Database。现在主流的Graph Database都支持Blueprints，而且在TinkerPop整个软件栈中，Blueprints是最底层的基础，就是其它的工具包都是基于它之上的封装和扩展。怎么使用Blueprints？</p>

<p>maven引入：</p>

<pre><code>&lt;dependency&gt;
   &lt;groupId&gt;com.tinkerpop.blueprints&lt;/groupId&gt;
   &lt;artifactId&gt;blueprints-core&lt;/artifactId&gt;
   &lt;version&gt;2.5.0&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>

<p>样例代码：</p>

<pre><code>Graph graph = new Neo4jGraph("/tmp/my_graph");
Vertex a = graph.addVertex(null);
Vertex b = graph.addVertex(null);
a.setProperty("name","marko");
b.setProperty("name","peter");
Edge e = graph.addEdge(null, a, b, "knows");
e.setProperty("since", 2006);
graph.shutdown();
</code></pre>

<h3>Pipes</h3>

<p><a href="https://github.com/tinkerpop/pipes/wiki">Pipes</a>是一个图数据处理的框架，可以将它理解为管道(Pipe),它最大的好处是管道(Pipe)的输出可以作为其它管道(Pipe)的输入，这样我们就可以实现类似于mapreducer的复杂运算。</p>

<h3>Gremlin</h3>

<p><a href="https://github.com/tinkerpop/gremlin/wiki">Gremlin</a>是一个图遍历语言，可以用Gremlin来实现图的查询、分析和操作，Gremlin只能适用于支持Blueprints的图数据库，支持多种JVM语言：Java 和 Groovy，文档：<a href="http://gremlindocs.com/">GremlinDocs</a>、<a href="http://sql2gremlin.com/">SQL2Gremlin</a>。</p>

<h3>Frames</h3>

<p><a href="https://github.com/tinkerpop/frames/wiki">Frames</a>是一个object-to-graph映射框架</p>

<h3>Furnace</h3>

<p><a href="https://github.com/tinkerpop/furnace/wiki">Furnace</a>是一个Graph算法包</p>

<h3>Rexster</h3>

<p><a href="https://github.com/tinkerpop/rexster/wiki">Rexster</a>是一个Graph Server</p>

<p>TinkerPop的维护人员来自不同的Graph Database产品厂商，像Neo4j、Titan、OrientDB、Bitsy，它在Graph Database领域的地位我理解就像JavaEE里面的Apache。在最新的<a href="https://github.com/tinkerpop/tinkerpop3">TinkerPop3.0</a>版本的时候，TinkerPop将原本分散的各个工具包合并成了一个项目，并且增加了很多特性，Titan0.5版本还不支持TP3，将会在Titan1.0版本时支持，更多的可以看<a href="http://www.tinkerpop.com/docs/current/">TinkerPop3 Story/doc</a>。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Graph Database]]></title>
    <link href="http://findhy.com/blog/2014/06/17/graph-database-data-structure/"/>
    <updated>2014-06-17T14:50:02+08:00</updated>
    <id>http://findhy.com/blog/2014/06/17/graph-database-data-structure</id>
    <content type="html"><![CDATA[<p><a href="http://en.wikipedia.org/wiki/Graph_database">Graph database</a>是近年兴起的NoSQL存储模型(graph, key-value, column, and document)中的一种实现数据库，代表产品是Neo4j、Titan，它的理论基础是<a href="http://zh.wikipedia.org/wiki/%E5%9B%BE%E8%AE%BA">图论/Graph theory</a>,一个数学分支，主要研究顶点和边组成的图形的数学理论和方法，它的起源也比较有意思，这部分本文不谈，感兴趣的同学可以去Google研究一下。</p>

<!--more-->


<p>想了解Graph Database，我们还是要把图论(Graph theory)简单说一下，略过中间复杂的数学问题，我们以结果为导向，图论可以解决哪些问题？这里有一个<a href="http://www.global-sci.org/mc/issues/3/no2/freepdf/67s.pdf">图论经典问题简介</a>,诸如<a href="http://zh.wikipedia.org/wiki/%E6%9C%80%E7%9F%AD%E8%B7%AF%E9%97%AE%E9%A2%98">最短路径问题</a>、<a href="http://zh.wikipedia.org/wiki/%E6%97%85%E8%A1%8C%E6%8E%A8%E9%94%80%E5%91%98%E9%97%AE%E9%A2%98">旅行售货商问题</a>、<a href="http://zh.wikipedia.org/wiki/%E9%82%AE%E9%80%92%E5%91%98%E9%97%AE%E9%A2%98">中国邮递员问题</a>，这些是图论中研究的经典问题，具体实现会涉及到不同的算法问题。</p>

<p>举个例子，在社交网络中，你搜索了一个人，怎么最快结识他，这其实就是一个最短路径问题，LinkedIn已经有相关的推荐产品，可以去试一下。而且未来随着智能设备的普及个人数据会呈现爆炸性增长，就是人的属性和关联会更多更复杂，其实这个问题在阿里现在已经存在了，他收购了那么多产品，重要一点就是为了获取数据，现在阿里拥有一个人的网购数据(淘宝、天猫)、社交数据(陌陌、新浪微博)、浏览搜索数据(UC、优酷)等等，这么庞大的一个数据结构该怎么来描述，更关键的是怎么快速的去做定位和推荐，这也是Graph Database兴起的一个原因，未来Graph这块必然会大放异彩。</p>

<h3>数据结构</h3>

<p>Graph Database的存储单元是：节点(nodes)、关系/边(edges)、属性(properties)<br/>
<img src="http://findhy.com/images/GraphDatabase_1.png"></p>

<ul>
<li>节点(nodes)：通常是一个实体，类似于社交网络中的人或者电商网络中的商品，节点不一定都是同一个类型的，比如我们构建一个Graph，节点是人和商品，之间的连接是谁买了哪个商品，这样我们可以很容易找到买相同的产品的人，去做关联推荐</li>
<li>属性(properties)：是与节点(nodes)相关的信息，通常是节点的描述，比如人的性别、年龄、地点、电话等信息</li>
<li>关系/边(edges)：用来连接节点(nodes)的，代表节点(nodes)与节点(nodes)之间具有某种关系，比如用户A和用户B是好友，用户A和用户C来自同一个地方，关系可以是有方向和无方向的，而且节点(nodes)之间可以有多条关系</li>
</ul>


<p>Graph Database的数据结构使得它在处理数据关联问题更具优势，因为它的节点(nodes)原生的就是通过某种关系连接在一起，这样就减少了join这样耗费资源的操作。</p>

<h3>算法</h3>

<p>Graph Database中很多问题都涉及到一些重要算法，下面列举一些，数据来源<a href="http://zh.wikipedia.org/wiki/%E6%9C%80%E7%9F%AD%E8%B7%AF%E9%97%AE%E9%A2%98">Wikipedia-Graph-Algorithms</a>：</p>

<ul>
<li>基本遍历：深度优先搜索、广度优先搜索、A*、Flood fill</li>
<li>最短路径：Dijkstra、Bellman-Ford、Floyd-Warshall 、Kneser图</li>
<li>最小生成树：Prim、Kruskal</li>
<li>强连通分量：Kosaraju算法、Gabow算法、Tarjan算法</li>
<li>图匹配：匈牙利算法、Hopcroft–Karp、Edmonds&rsquo;s matching</li>
<li>网络流：Ford-Fulkerson、Edmonds-Karp、Dinic 、Push-relabel maximum flow</li>
</ul>


<h3>常规操作</h3>

<p>类似于关系型数据库提供的CRUD操作，Graph Database也提供一系列指令来操作Graph，下面G代表一个Graph data structure，你可以把它想象为关系型数据库中的一张表。</p>

<ul>
<li>adjacent(G, x, y)：检查节点x和节点y之间是否有一个边(edge)</li>
<li>neighbors(G, x)：列出所有与节点x有连接/边(edge)的节点</li>
<li>add(G, x, y)：插入一个边(edge)，从x指向y</li>
<li>delete(G, x, y)：删除一个从x指向y的边(edge)</li>
<li>get_node_value(G, x)：返回节点x的相关属性</li>
<li>set_node_value(G, x, a)：设置节点x的属性为a</li>
<li>get_edge_value(G, x, y)：返回节点x和y之间的边(edge)的属性，边(edge)也是有属性的</li>
<li>set_edge_value(G, x, y, v)：设置节点x和y之间的边(edge)的属性为v</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Titan:下一代分布式图数据库]]></title>
    <link href="http://findhy.com/blog/2014/06/17/titan-next-generation-graph-database/"/>
    <updated>2014-06-17T10:23:30+08:00</updated>
    <id>http://findhy.com/blog/2014/06/17/titan-next-generation-graph-database</id>
    <content type="html"><![CDATA[<p><a href="http://thinkaurelius.github.io/titan/">Titan</a>是一个由<a href="http://thinkaurelius.com/">Aurelius</a>维护的开源协议为<a href="http://www.apache.org/licenses/LICENSE-2.0.html">Apache 2.0</a>的分布式图形数据库。</p>

<!--more-->


<p>Graph Database作为NoSQL数据库四种存储模式(graph, key-value, column, and document)的其中一种，近年来发展迅猛，因为随着人工智能和社交网络不断发展和融合，数据结构越来越复杂，举个例子，以用户为中心的模型，用户的相关数据可能来源他的社交网络，也可能来源他的网购记录，也可能来源他的个人可穿戴设备等等，这个数据会呈现爆炸性增长，如果用户基数为千万级，再去做关联和流行度分析会非常复杂，Graph Database处理这样的需求具体天生的优势。目前市面上的<a href="http://en.wikipedia.org/wiki/Graph_database#Graph_database_projects">Graph Database</a>有很多，<a href="http://www.neo4j.org/">Neo4j</a>是最为成熟和知名度最高的产品，但因为Neo4j<a href="http://stackoverflow.com/questions/21558589/neo4j-sharding-aspect/21566766#21566766">不支持分片</a>导致其存在可伸缩性的问题，但是貌似Neo4j已经推出相应的解决方案架构，参考这里<a href="http://info.neotechnology.com/rs/neotechnology/images/Understanding%20Neo4j%20Scalability(2).pdf">Neo4j HA-1</a>、<a href="http://neo4j.com/blog/2013-whats-coming-next-in-neo4j/">Neo4j HA-2</a>。</p>

<p>Titan作为新一代的Graph Database，还比较年轻，但非常有前途，它的优势有几方面：</p>

<ul>
<li>天生支持分布式：横向扩展很容易，并且性能可以线性增长</li>
<li>性能：Titan官方在Titan-0.1-alpha做过一个<a href="http://thinkaurelius.com/2012/08/06/titan-provides-real-time-big-graph-data/">测试</a>，性能表现非常强劲</li>
<li>后端存储无关：它可以将数据存储在不同的数据库，目前支持HBase、Cassandra和BerkeleyDB，而且Titan 0.5.0将会集成另外一个模块：Titan/Hadoop，这样会让Titan与现有的数据平台结合更加容易</li>
<li>后端索引无关：目前支持<a href="http://www.elasticsearch.org/">ElasticSearch</a>和<a href="http://lucene.apache.org/">Apache Lucene</a>两种索引</li>
<li>支持多数据中心的高可用和热备份</li>
<li>原生支持<a href="http://www.tinkerpop.com/">tinkerpop</a>：<a href="http://www.tinkerpop.com/">tinkerpop</a>是一系列Graph领域的开源软件栈</li>
</ul>


<p>Titan的架构图：<br/>
<img src="http://findhy.com/images/titan-next-1.png"></p>

<h4>总结</h4>

<p>目前Titan刚刚发布<a href="https://groups.google.com/forum/#!topic/aureliusgraphs/cNb4fKoe95M">Titan 0.5.0-M1</a>版本，增加了很多特性，而且文档更加完善了，Titan 0.5.0 GA会在七月底发布，这会是一个非常接近1.0版本的产品，对于有需求的公司可以进行预研，对它完全掌握了再投入生产，毕竟Titan在实际生产环境的案例和技术文档都比较欠缺。但我相信Titan会成为下一代非常出色的Graph Database，我也会继续研究Titan和发布相关Titan相关的文章，希望能为Titan在中国推广做一些贡献，有感兴趣的同学欢迎一起讨论。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Storm与Kafka集成开发]]></title>
    <link href="http://findhy.com/blog/2014/06/16/storm-kafka-dev/"/>
    <updated>2014-06-16T16:44:35+08:00</updated>
    <id>http://findhy.com/blog/2014/06/16/storm-kafka-dev</id>
    <content type="html"><![CDATA[<p>项目源码在这里<a href="https://github.com/findhy/storm-kafka">storm-kafka</a>，里面包括了简单说明和测试过程，照着做就可以了，下面简单介绍一下。</p>

<p><img src="http://findhy.com/images/storm-kafka-dev1.png"></p>

<!--more-->


<ul>
<li>数据源：该架构主要用来处理Streaming data，例子使用Wikipedia提供的Websocket接口，实时发送当前在Wikipedia网站编辑内容的用户相关信息</li>
<li>Websocket：可以参考这篇文章<a href="http://findhy.com/blog/2014/06/12/java-websocket/">Java-Websocket</a>，本框架使用Java语言作为kafka的客户端实现，所以也用的Java来实现Websocket，Kafka也支持其它语言作为client，这里是支持的客户端列表<a href="https://cwiki.apache.org/confluence/display/KAFKA/Clients">kafka-client</a>，用Node.js也是一个不错的选择</li>
<li>Storm：Storm与Kafka集成的重点在于Storm的Spout部分，这部分直接依赖这个库<a href="https://github.com/wurstmeister/storm-kafka-0.8-plus">storm-kafka-0.8-plus</a>，实现订阅Kafka的Topic</li>
<li>数据出口：Storm对Streaming data处理完了之后，一般会有两种出口，一是将数据持久化到HBase/Cassandra/Redis这样的NoSql Database中，二是通知前端在可视化界面上实时变动，该框架实现的是Storm将处理完的数据再次发送到Kafka中，前端通过Node.js和socket.io去订阅这个数据就可以(这部分暂未实现)</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Caused by: java.io.NotSerializableException: kafka.javaapi.producer.Producer]]></title>
    <link href="http://findhy.com/blog/2014/06/14/kafka-storm-notserializableexception/"/>
    <updated>2014-06-14T20:24:15+08:00</updated>
    <id>http://findhy.com/blog/2014/06/14/kafka-storm-notserializableexception</id>
    <content type="html"><![CDATA[<p>我们现在的架构使用Kafka作为消息的入口，数据全部发送到Kafka中，然后用Storm的Topology写一个spout去订阅Kafka的消息，执行提交Topology：</p>

<!--more-->


<pre><code>storm jar storm-kafka-0.8-plus-test-0.1.0-SNAPSHOT-jar-with-dependencies.jar storm.kafka.topology.CyouStormTopology -c nimbus.host=10.0.1.254
</code></pre>

<p>但是报错：</p>

<pre><code>Exception in thread "main" java.lang.RuntimeException: java.io.NotSerializableException: kafka.javaapi.producer.Producer
    at backtype.storm.utils.Utils.serialize(Utils.java:56)
    at backtype.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:89)
    at storm.kafka.topology.CyouStormTopology.main(CyouStormTopology.java:32)
Caused by: java.io.NotSerializableException: kafka.javaapi.producer.Producer
    at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1183)
    at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
    at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
    at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
    at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
    at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
    at backtype.storm.utils.Utils.serialize(Utils.java:52)
    ... 2 more
</code></pre>

<p>后来参考这里的提示：<a href="https://groups.google.com/forum/#!msg/storm-user/heQSeawhC5I/y40-FnQ4hiUJ  ">https://groups.google.com/forum/#!msg/storm-user/heQSeawhC5I/y40-FnQ4hiUJ  </a>
修改代码如下：</p>

<pre><code>    @Override
    public void prepare(Map stormConf, TopologyContext context,
            OutputCollector collector) {
        LOG.info("begin to prepare in bolt from CyouSendToKafkaBolt");
        this._collerctor = collector;

        Properties props = new Properties();
        props.put("metadata.broker.list", "master:9092");
        props.put("serializer.class", "kafka.serializer.StringEncoder");
        props.put("partitioner.class", "storm.kafka.producer.CyouPartitioner");
        props.put("request.required.acks", "1");
        ProducerConfig config = new ProducerConfig(props);

        producer = new Producer&lt;String, String&gt;(config);
    }
</code></pre>

<p>将原来在Bolt构造函数里面初始化Producer，改为在prepare(&hellip;)中去初始化，最后再次执行就没有问题了。</p>

<p>总结：在bolt中做一些初始化的代码，要放到prepare(&hellip;)方法中，而不要放到构造函数中，因为prepare(&hellip;)方法是在Storm worker JVM中被调用，而构造函数是在Nimbus JVM中被调用而造成不会被serialized。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Java-WebSocket]]></title>
    <link href="http://findhy.com/blog/2014/06/12/java-websocket/"/>
    <updated>2014-06-12T22:51:51+08:00</updated>
    <id>http://findhy.com/blog/2014/06/12/java-websocket</id>
    <content type="html"><![CDATA[<p>本文涉及到几个概念后面细说，Websocket协议、socket.io、Java-WebSocket，核心都是围绕Websocket，最后重点讲解在Java端实现Websocket的访问。</p>

<!--more-->


<h3>1.Websocket协议</h3>

<p>首先解释一下Websocket协议，<a href="http://zh.wikipedia.org/wiki/WebSocket">Wikipedia</a>的定义是：</p>

<blockquote><p>WebSocket是HTML5开始提供的一种浏览器与服务器间进行全双工通讯的网络技术</p></blockquote>

<p>什么是【全双工】？可以理解为双方经过对方确认可以互传数据了。客户端和服务器端建立<a href="http://zh.wikipedia.org/wiki/%E4%BC%A0%E8%BE%93%E6%8E%A7%E5%88%B6%E5%8D%8F%E8%AE%AE">TCP</a>连接的时候需要经过三次握手过程才能达到全双工的连接状态。在WebSocket API中，浏览器和服务器只需要做一个握手的动作，然后，浏览器和服务器之间就形成了一条快速通道。两者之间就直接可以数据互相传送。</p>

<p>在Websocket协议之前，浏览器和服务器端最常用的是HTTP协议，而HTTP协议是半双工，服务器端无法实时向客户端发送消息，但是很多场景下我们又有这样的需求，以前的解决方案是浏览器端做轮询或者用Comet做长连接，两种方案都比较消耗资源。</p>

<p>Websocket协议的出现完全解决了这个问题，服务器端可实现主动推送数据到客户端。现在主流的浏览器Chrome、Firefox等都已经支持Websocket协议了。</p>

<p>而在服务器端，主流的web服务器和编程语言都对Websocket进行了支持，下面摘自Wikipedia：</p>

<ul>
<li>php &ndash; <a href="http://code.google.com/p/phpwebsocket/">http://code.google.com/p/phpwebsocket/</a></li>
<li>jetty &ndash; <a href="http://jetty.codehaus.org/jetty/">http://jetty.codehaus.org/jetty/</a> (版本7开始支持websocket)</li>
<li>netty &ndash; <a href="http://www.jboss.org/netty">http://www.jboss.org/netty</a></li>
<li>ruby &ndash; <a href="http://github.com/gimite/web-socket-ruby">http://github.com/gimite/web-socket-ruby</a></li>
<li>Kaazing &ndash; <a href="http://www.kaazing.org/confluence/display/KAAZING/Home">http://www.kaazing.org/confluence/display/KAAZING/Home</a></li>
<li>Tomcat &ndash; <a href="http://tomcat.apache.org/">http://tomcat.apache.org/</a> (7.0.26支持websocket)</li>
<li>WebLogic &ndash; <a href="http://www.oracle.com/us/products/middleware/cloud-app-foundation/-">http://www.oracle.com/us/products/middleware/cloud-app-foundation/-</a> weblogic/overview/index.html (12.1.2 开始支持)</li>
<li>node.js &ndash; <a href="https://github.com/Worlize/WebSocket-Node">https://github.com/Worlize/WebSocket-Node</a></li>
<li>node.js &ndash; <a href="http://socket.io">http://socket.io</a></li>
<li>nginx &ndash; <a href="http://nginx.com/">http://nginx.com/</a></li>
<li>mojolicious &ndash; <a href="http://mojolicio.us/">http://mojolicio.us/</a></li>
<li>python &ndash; <a href="https://github.com/abourget/gevent-socketio">https://github.com/abourget/gevent-socketio</a></li>
<li>Django &ndash; <a href="https://github.com/stephenmcd/django-socketio">https://github.com/stephenmcd/django-socketio</a></li>
<li>Java &ndash; <a href="http://java-websocket.org/">http://java-websocket.org/</a></li>
</ul>


<h3>2.socket.io</h3>

<p><a href="http://socket.io/">socket.io</a>一个是基于Node.js架构体系的，支持Websocket的协议用于时时通信的一个软件包。socket.io给跨浏览器构建实时应用提供了完整的封装，socket.io完全由javascript实现。Node.js实现Websocket访问的方式有很多种，socket.io是最常用的的。</p>

<p>和Node.js一样，socket.io是基于事件驱动模型，它使用Websocket协议，但同时对Adobe Flash sockets、JSONP polling、AJAX long polling也提供支持，它的源代码在这里<a href="https://github.com/Automattic/socket.io">socket.io-github</a>。</p>

<p>所以socket.io是一个对Websocket协议封装的javascript库，我们可以用它来完成客户端和服务器端的代码。</p>

<h3>3.Java-WebSocket</h3>

<p>我们有时会有这样的需求，就是在后台Java端去调用一个Websocket接口的服务，比如在Android端去请求一个公共Websocket服务，<a href="http://java-websocket.org/">Java-WebSocket</a>就是为了这样的需求而产生的，你可以完全用Java代码来实现Websocket的服务器端和客户端，而且它支持WSS，WSS是安全的Websocket连接，类似于HTTPS，下面我们来看一个客户端的例子，用Java实现用读取Wikipedia提供的Websocket接口：当前谁在Wikipedia上修改了文章。</p>

<p>POM文件添加依赖</p>

<pre><code>&lt;dependency&gt;
    &lt;groupId&gt;org.java-websocket&lt;/groupId&gt;
    &lt;artifactId&gt;Java-WebSocket&lt;/artifactId&gt;
    &lt;version&gt;1.3.0&lt;/version&gt;
&lt;/dependency&gt; 
</code></pre>

<p>客户端Java类</p>

<pre><code>package storm.kafka.websocket;

import java.net.URI;
import java.net.URISyntaxException;

import org.java_websocket.client.WebSocketClient;
import org.java_websocket.drafts.Draft;
import org.java_websocket.drafts.Draft_10;
import org.java_websocket.framing.Framedata;
import org.java_websocket.handshake.ServerHandshake;

/**
 * This example demonstrates how to create a websocket connection to a server.
 * Only the most important callbacks are overloaded.
 */
public class TestWebsocket extends WebSocketClient {

    public TestWebsocket(URI serverUri, Draft draft) {
        super(serverUri, draft);
    }

    public TestWebsocket(URI serverURI) {
        super(serverURI);
    }

    @Override
    public void onOpen(ServerHandshake handshakedata) {
        System.out.println("opened connection");
        // if you plan to refuse connection based on ip or httpfields overload:
        // onWebsocketHandshakeReceivedAsClient
    }

    @Override
    public void onMessage(String message) {
        System.out.println("received: " + message);
    }

    public void onFragment(Framedata fragment) {
        System.out.println("received fragment: "
                + new String(fragment.getPayloadData().array()));
    }

    @Override
    public void onClose(int code, String reason, boolean remote) {
        // The codecodes are documented in class
        // org.java_websocket.framing.CloseFrame
        System.out.println("Connection closed by "
                + (remote ? "remote peer" : "us"));
    }

    @Override
    public void onError(Exception ex) {
        ex.printStackTrace();
        // if the error is fatal then onClose will be called additionally
    }

    public static void main(String[] args) throws URISyntaxException {
        TestWebsocket c = new TestWebsocket(new URI("ws://wikimon.hatnote.com:9000"),
                new Draft_10()); // more about drafts here:
                                    // http://github.com/TooTallNate/Java-WebSocket/wiki/Drafts
        c.connect();
    }

}
</code></pre>

<p>执行结果如下，可以接受到Wikipedia返回的当前修改的具体信息：</p>

<pre><code>opened connection
received: {"action": "edit", "change_size": 7, "flags": "M", "is_anon": false, "is_bot": false, "is_minor": true, "is_new": false, "is_unpatrolled": false, "ns": "Main", "page_title": "Siege of Amida (502\u2013503)", "parent_rev_id": "612641484", "rev_id": "575212259", "summary": "categorized for missing coordinate data", "url": "http://en.wikipedia.org/w/index.php?diff=612641484&amp;oldid=575212259", "user": "Kevinsam"}
received: {"action": "edit", "change_size": 38, "flags": null, "geo_ip": {"areacode": "", "city": "", "country_code": "US", "country_name": "United States", "ip": "71.167.104.60", "latitude": 38, "longitude": -97, "metro_code": "", "region_code": "", "region_name": "", "zipcode": ""}, "is_anon": true, "is_bot": false, "is_minor": false, "is_new": false, "is_unpatrolled": false, "ns": "Main", "page_title": "KMOV", "parent_rev_id": "612641485", "rev_id": "612641166", "summary": "/* History */", "url": "http://en.wikipedia.org/w/index.php?diff=612641485&amp;oldid=612641166", "user": "71.167.104.60"}
</code></pre>

<h3>4.总结</h3>

<p>Websocket协议是为了解决服务器推送数据到客户端而出现的由于HTTP协议的解决方案，基于Websocket协议又有很多实现，实现包括客户端和服务器端，客户端体现在主流的浏览器现在基本上都支持Websocket协议，服务器端体现在各个web服务器都对Websocket提供支持，各个编程语言也提供相应的实现包，让我们针对Websocket的开发更容易。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Storm Spouts Lifecycle]]></title>
    <link href="http://findhy.com/blog/2014/06/10/storm-spouts-lifecycle/"/>
    <updated>2014-06-10T15:16:53+08:00</updated>
    <id>http://findhy.com/blog/2014/06/10/storm-spouts-lifecycle</id>
    <content type="html"><![CDATA[<p>Spouts是Storm中的Topology对应的消息生产者，消息将从Spouts发出，消息的单位是tuple，本文讲解Spouts核心方法以及Spouts方法的生命周期。相关接口方法看这里：<a href="http://storm.incubator.apache.org/apidocs/backtype/storm/spout/ISpout.html">ISpout</a>。Spouts在Storm中的位置可以参考下图：<br/>
<img src="http://findhy.com/images/storm-concepts.png"></p>

<!--more-->


<h3>1.初始化</h3>

<p>当我们自定义一个Spouts的时候，继承BaseRichSpout类，代码是这样的：</p>

<pre><code>public class TestWordSpout extends BaseRichSpout{

    public static Logger LOG = LoggerFactory.getLogger(TestWordSpout.class);
    SpoutOutputCollector _collector;

    @Override
    public void open(Map conf, TopologyContext context,SpoutOutputCollector collector) {
        _collector = collector;
    }

    @Override
    public void nextTuple() {

    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {

    }

}
</code></pre>

<p>BaseRichSpout是Storm提供的一个抽象类，代码如下：</p>

<pre><code>/*
 * To change this template, choose Tools | Templates
 * and open the template in the editor.
 */
package backtype.storm.topology.base;

import backtype.storm.topology.IRichSpout;

/**
 *
 * @author nathan
 */
public abstract class BaseRichSpout extends BaseComponent implements IRichSpout {
    @Override
    public void close() {
    }

    @Override
    public void activate() {
    }

    @Override
    public void deactivate() {
    }

    @Override
    public void ack(Object msgId) {
    }

    @Override
    public void fail(Object msgId) {
    }
}
</code></pre>

<p>可以看到BaseRichSpout没有做任何事情就是实现了接口的方法，没有任何具体实现，这样子类不用显式实现方法了，它继承了BaseComponent，实现了接口IRichSpout，而IRichSpout接口是继承自ISpout, IComponent这两个接口，BaseComponent类如下：</p>

<pre><code>package backtype.storm.topology.base;

import backtype.storm.topology.IComponent;
import java.util.Map;

public abstract class BaseComponent implements IComponent {
    @Override
    public Map&lt;String, Object&gt; getComponentConfiguration() {
        return null;
    }
}
</code></pre>

<p>可以看到BaseComponent只是实现了接口的方法getComponentConfiguration()返回null。   <br/>
下面来看下ISpout, IComponent这两个接口</p>

<pre><code>package backtype.storm.topology;

import java.io.Serializable;
import java.util.Map;

public interface IComponent extends Serializable {

    void declareOutputFields(OutputFieldsDeclarer declarer);

    Map&lt;String, Object&gt; getComponentConfiguration();

}

package backtype.storm.spout;

import backtype.storm.task.TopologyContext;
import java.util.Map;
import java.io.Serializable;

public interface ISpout extends Serializable {

    void open(Map conf, TopologyContext context, SpoutOutputCollector collector);

    void close();

    void activate();

    void deactivate();

    void nextTuple();

    void ack(Object msgId);

    void fail(Object msgId);
}
</code></pre>

<p>上面把javadoc都去掉，如果要看的话直接看源码，或者到这里看：<a href="http://storm.incubator.apache.org/apidocs  ">http://storm.incubator.apache.org/apidocs  </a>
到这里基本可以看到核心的接口方法主要在IComponent和ISpou这两个接口中定义，下面详细讲解。</p>

<h3>2.IComponent接口方法详解</h3>

<p>IComponent接口是topology中所有组件的顶层接口，包括Spouts、Bolts，它定义了两个核心的公共方法：</p>

<pre><code>    /**
     * 该方法是用来定义topology中的spout或者bolt的ID，而且该ID在同是spouts或者bolts内部不能重复，但是spouts和bolts之间的ID可以重复
     * 如果没有显示指定该值，则默认使用Utils.DEFAULT_STREAM_ID这个值
     * 这里可以参考Storm源码TopologyBuilder类里面的validateUnusedId这个方法
     * 
     * 该方法在客户端调用createTopology()方法时被执行，同样参见TopologyBuilder类
     * 
     * 源码：https://github.com/nathanmarz/storm/blob/moved-to-apache/storm-core/src/jvm/backtype/storm/topology/TopologyBuilder.java#L226
     */
    void declareOutputFields(OutputFieldsDeclarer declarer);

    /**
     * 该方法可以用来配置组件
     * 
     * Declare configuration specific to this component. Only a subset of the "topology.*" configs can
     * be overridden. The component configuration can be further overridden when constructing the 
     * topology using {@link TopologyBuilder}
     *
     */
    Map&lt;String, Object&gt; getComponentConfiguration();
</code></pre>

<h3>3.ISpout接口方法详解</h3>

<pre><code>    /**
     * 该方法在task任务组件在worker里初始化的时候被调用，它提供了spout的执行环境
     * Called when a task for this component is initialized within a worker on the cluster.
     * It provides the spout with the environment in which the spout executes.
     *
     * &lt;p&gt;This includes the:&lt;/p&gt;
     *
     * @param conf The Storm configuration for this spout. This is the configuration 
     * provided to the topology merged in with cluster configuration on this machine.
     * @param context This object can be used to get information about this task's place 
     * within the topology, including the task id and component id of this task, input 
     * and output information, etc.
     * 
     * @param collector The collector is used to emit tuples from this spout. 
     * Tuples can be emitted at any time, including the open and close methods. 
     * The collector is thread-safe and should be saved as an instance variable of this spout object.
     */
    void open(Map conf, TopologyContext context, SpoutOutputCollector collector);

    /**
     * Spout停掉的时候会调用此方法
     * 
     * Called when an ISpout is going to be shutdown. There is no guarentee that close
     * will be called, because the supervisor kill -9's worker processes on the cluster.
     *
     * &lt;p&gt;The one context where close is guaranteed to be called is a topology is
     * killed when running Storm in local mode.&lt;/p&gt;
     */
    void close();

    /**
     * spout由deactivated模式转为activated模式时被调用
     * 
     * Called when a spout has been activated out of a deactivated mode.
     * nextTuple will be called on this spout soon. A spout can become activated
     * after having been deactivated when the topology is manipulated using the 
     * `storm` client. 
     */
    void activate();

    /**
     * spout被deactivated的时候调用，并且当spout被deactivated时，nextTuple就不会被调用了
     * 
     * Called when a spout has been deactivated. nextTuple will not be called while
     * a spout is deactivated. The spout may or may not be reactivated in the future.
     */
    void deactivate();

    /**
     * spout发射Tuple时被调用，该方法必须是非阻塞的（non-blocking），这样如果Spout没有tuples可以发射了，
     * 这个方法也会返回，nextTuple, ack, and fail这几个方法在同一个spout task同一个线程里面是会被循环调用的，
     * 当没有tuples可以发射了，应该让线程sleep一段时间
     * 这样就不会占用太多的CPU资源了
     * 
     * When this method is called, Storm is requesting that the Spout emit tuples to the 
     * output collector. This method should be non-blocking, so if the Spout has no tuples
     * to emit, this method should return. nextTuple, ack, and fail are all called in a tight
     * loop in a single thread in the spout task. When there are no tuples to emit, it is courteous
     * to have nextTuple sleep for a short amount of time (like a single millisecond)
     * so as not to waste too much CPU.
     */
    void nextTuple();

    /**
     * Storm可以保证spout发射出去的tuples必须被处理了，msgId就是就是发射的消息的ID
     * 当tuples被处理完了之后，该方法会被调用，并且将它从队列中去掉防止被反复处理
     * 
     * Storm has determined that the tuple emitted by this spout with the msgId identifier
     * has been fully processed. Typically, an implementation of this method will take that
     * message off the queue and prevent it from being replayed.
     */
    void ack(Object msgId);

    /**
     * 如果msgId对应的tuples处理失败，该方法会被调用，通常的实现会将tuples重新放回队列，让它多一段时间可以被处理
     * 上面两个方法ack和fail是Storm保证数据一定被处理和避免重复处理的机制，
     * 参数msgId就是每次发射tuples的时候spout提供的一个
     * message-id，后面可以通过这个message-id来追踪这个tuple
     * 
     * The tuple emitted by this spout with the msgId identifier has failed to be
     * fully processed. Typically, an implementation of this method will put that
     * message back on the queue to be replayed at a later time.
     */
    void fail(Object msgId);
</code></pre>

<h3>4.总结</h3>

<ul>
<li>客户端提交Topology的时候，首先调用declareOutputFields(&hellip;)方法，指定spout和bolt的ID，如果没有实现该方法则默认为Utils.DEFAULT_STREAM_ID</li>
<li>然后Storm会在worker进程内初始化task的运行环境，再调用open(&hellip;)方法，传回SpoutOutputCollector对象，后面我们后可以SpoutOutputCollector来发射tuples</li>
<li>然后Storm就会反复调用spout的nextTuple方法获取下一个tuple，如果任务处理成功了就调用ack方法，如果任务处理失败就调用fail方法</li>
<li>并且Spout发射tuple会提供一个message-id，后面我们通过这个message-id来追踪这个tuple，ack和fail接受的参数就是该message-id</li>
<li>一个tuple可能会产生多个tuple，最终形成一个tuple树，Storm会跟踪整个tuple树，如果其中一个叶子tuple失败，那么整个tuple树会重新被处理</li>
<li>值得注意的一点是，storm调用ack或者fail的task始终是产生这个tuple的那个task。所以如果一个spout被分成很多个task来执行， 消息执行的成功失败与否始终会通知最开始发出tuple的那个task</li>
<li>每个你处理的tuple， 必须被ack或者fail。因为storm追踪每个tuple要占用内存。所以如果你不ack/fail每一个tuple， 那么最终你会看到OutOfMemory错误。</li>
</ul>


<p>本文参考：<br/>
<a href="http://xumingming.sinaapp.com/127/twitter-storm%E5%A6%82%E4%BD%95%E4%BF%9D%E8%AF%81%E6%B6%88%E6%81%AF%E4%B8%8D%E4%B8%A2%E5%A4%B1/">Storm如何保证消息不丢失</a><br/>
<a href="http://stackoverflow.com/questions/21689059/with-storm-spouts-when-is-declareoutputfields-called">storm-spouts</a><br/>
<a href="http://storm.incubator.apache.org/apidocs">storm-javadocs</a><br/>
<a href="http://xumingming.sinaapp.com/117/twitter-storm%E7%9A%84%E4%B8%80%E4%BA%9B%E5%85%B3%E9%94%AE%E6%A6%82%E5%BF%B5/">storm的一些关键概念</a><br/>
<a href="http://xumingming.sinaapp.com/138/twitter-storm%E5%85%A5%E9%97%A8/">storm入门</a></p>
]]></content>
  </entry>
  
</feed>
